# Programming Methods {#Programming Principles for Hardware Accelerators}

The first two sections of this chapter primarily discuss the
significance, ideas, and basic principles behind the design of hardware
accelerators. Co-optimization of software and hardware, as an important
guiding principle for building efficient AI systems, requires mutual
influence and close coupling between software algorithms/stacks and
hardware architectures in neural network applications. In order to fully
leverage the advantages of accelerators, it is necessary to design a set
of programming methods based on the hardware system architecture.

## Method Classification

Programming methods for hardware accelerators are categorized into three
approaches: using high-level computation operators, harnessing
primitives for specialized hardware units, and employing low-level
assembly languages:

1.  **High-level computation operators**: Hardware accelerators often
    come equipped with high-level, hardware-accelerated implementations
    of operators extensively used in numerical computing and deep
    learning. For instance, NVIDIA provides cuBLAS (CUDA Basic Linear
    Algebra Subprograms) and cuDNN (CUDA Deep Neural Network library).
    These libraries offer developers an accessible way to harness the
    power of NVIDIA GPUs without delving into low-level code. These
    operators are optimized for efficiency and automatically exploit
    specific GPU features, such as Tensor Cores.

2.  **Primitives for task-specific hardware units:**: Hardware
    accelerators typically feature task-specific hardware units (like
    the Tensor Cores in NVIDIA GPUs) engineered to execute
    mixed-precision matrix multiplication operations at high speed.
    These units have associated programming primitives, such as CUDA's
    Warp Matrix Multiply Accumulate (WMMA) and primitives for
    loading/unloading tensors on the units.

3.  **Low-level assembly languages**: Hardware accelerators also have
    low-level assembly language interfaces. For instance, NVIDIA GPUs
    offer the PTX ISA (Parallel Thread Execution Instruction Set
    Architecture). It provides explicit control over all aspects of GPU
    behavior, but it requires a deep understanding of the GPU
    architecture and is more challenging to use correctly and
    effectively than the high-level interfaces provided by cuBLAS and
    cuDNN. PTX code is typically generated by a compiler from a
    high-level language like CUDA C++.

In essence, the above three methods operate at different levels of
abstraction. High-level operators like cuBLAS and cuDNN provide
easy-to-use interfaces to powerful hardware-accelerated operations,
while the primitives provided by task-specific hardware units provide a
more detailed interface to hardware operations, and low-level assembly
languages like PTX ISA provide the most detailed, low-level control over
accelerator behavior.

## Programming Examples

We exemplify different programming methods by implementing the General
Matrix Multiplication (GEMM) with each approach. The implementation
targets an NVIDIA Volta GPU. GEMM follows the equation
$\bf{C} = \alpha \bf{A}\times \bf{B} + \beta \bf{C}$, where
$\bf{A}\in\mathbb{R}^{M\times K}, \bf{B}\in\mathbb{R}^{K\times N}, \bf{C}\in\mathbb{R}^{M\times N}$,
and $\alpha$ and $\beta$ are parameters provided by users.

### High-level Computation Operators {#sec-accelerator-use-cublas}

Using an operator acceleration library directly is the most
straightforward method. NVIDIA offers two types of operator libraries:
cuBLAS and cuDNN. cuBLAS provides an interface for leveraging Tensor
Cores to accelerate GEMM operations, while cuDNN offers an interface to
hasten neural network operations. To utilize Tensor Cores via cuBLAS
doing GEMM, we can use function `cublasGemmEx`, its signature is shown
in CodeÂ [\[lst:cublasGemmEx\]](#lst:cublasGemmEx){reference-type="ref"
reference="lst:cublasGemmEx"}.

     [caption={Fragment types}, label={lst:cublasGemmEx}]
    cublasStatus_t cublasGemmEx(cublasHandle_t handle, cublasOperation_t transa, cublasOperation_t transb, int m, int n, int k, const void *alpha, const void *A, cudaDataType_t Atype, int lda, const void *B, cudaDataType_t Btype, int ldb, const void *beta, void *C, cudaDataType_t Ctype, int ldc, cublasComputeType_t computeType, cublasGemmAlgo_t algo)


[^1]: available at
    <https://docs.nvidia.com/cuda/inline-ptx-assembly/index.html>
