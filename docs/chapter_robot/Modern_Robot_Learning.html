<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
  <meta content="ie=edge" http-equiv="x-ua-compatible"/>
  <title>
   13.4. Modern Robot Learning — Machine Learning Systems: Design and Implementation 1.0.0 documentation
  </title>
  <link href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/sphinx_materialdesign_theme.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/fontawesome/all.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/fonts.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/basic.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/d2l.css" rel="stylesheet" type="text/css"/>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js">
  </script>
  <script src="../_static/jquery.js">
  </script>
  <script src="../_static/underscore.js">
  </script>
  <script src="../_static/_sphinx_javascript_frameworks_compat.js">
  </script>
  <script src="../_static/doctools.js">
  </script>
  <script src="../_static/sphinx_highlight.js">
  </script>
  <script src="../_static/d2l.js">
  </script>
  <link href="../_static/favicon.png" rel="shortcut icon"/>
  <link href="../genindex.html" rel="index" title="Index"/>
  <link href="../search.html" rel="search" title="Search"/>
  <link href="Chapter_Summary.html" rel="next" title="13.5. Chapter Summary"/>
  <link href="Case_Study.html" rel="prev" title="13.3. Case Study: Using ROS"/>
 </head>
 <body>
  <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer">
   <header class="mdl-layout__header mdl-layout__header--waterfall">
    <div class="mdl-layout__header-row">
     <nav class="mdl-navigation breadcrumb">
      <a class="mdl-navigation__link" href="index.html">
       <span class="section-number">
        13.
       </span>
       Robotic System
      </a>
      <i class="material-icons">
       navigate_next
      </i>
      <a class="mdl-navigation__link is-active">
       <span class="section-number">
        13.4.
       </span>
       Modern Robot Learning
      </a>
     </nav>
     <div class="mdl-layout-spacer">
     </div>
     <nav class="mdl-navigation">
      <form action="../search.html" class="form-inline pull-sm-right" method="get">
       <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label class="mdl-button mdl-js-button mdl-button--icon" for="waterfall-exp" id="quick-search-icon">
         <i class="material-icons">
          search
         </i>
        </label>
        <div class="mdl-textfield__expandable-holder">
         <input class="mdl-textfield__input" id="waterfall-exp" name="q" placeholder="Search" type="text"/>
         <input name="check_keywords" type="hidden" value="yes"/>
         <input name="area" type="hidden" value="default"/>
        </div>
       </div>
       <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
        Quick search
       </div>
      </form>
      <a class="mdl-button mdl-js-button mdl-button--icon" href="../_sources/chapter_robot/Modern_Robot_Learning.rst.txt" id="button-show-source" rel="nofollow">
       <i class="material-icons">
        code
       </i>
      </a>
      <div class="mdl-tooltip" data-mdl-for="button-show-source">
       Show Source
      </div>
     </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
     <div class="mdl-layout-spacer">
     </div>
     <nav class="mdl-navigation">
      <a class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-en">
       <i class="fab fa-github">
       </i>
       GitHub
      </a>
      <a class="mdl-navigation__link" href="https://openmlsys.github.io/">
       <i class="fas fa-external-link-alt">
       </i>
       中文版
      </a>
     </nav>
    </div>
   </header>
   <header class="mdl-layout__drawer">
    <!-- Title -->
    <span class="mdl-layout-title">
     <a class="title" href="../index.html">
      <img alt="Machine Learning Systems: Design and Implementation" class="logo" src="../_static/logo-with-text.png"/>
     </a>
    </span>
    <div class="globaltoc">
     <span class="mdl-layout-title toc">
      Table Of Contents
     </span>
     <nav class="mdl-navigation">
      <ul class="current">
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_preface/index.html">
         1. Preface
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_introduction/index.html">
         2. Introduction
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/Machine_Learning_Applications.html">
           2.1. Machine Learning Applications
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/Design_Objectives_of_Machine_Learning_Frameworks.html">
           2.2. Design Objectives of Machine Learning Frameworks
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/Machine_Learning_Framework_Architecture.html">
           2.3. Machine Learning Framework Architecture
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/Application_Scenarios_of_Machine_Learning_Systems.html">
           2.4. Application Scenarios of Machine Learning Systems
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/Book_Organization_and_Intended_Audience.html">
           2.5. Book Organization and Intended Audience
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_preface_basic/index.html">
         3. Part I Framework Design
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_programming_model/index.html">
         4. Programming Model
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_model/Overview.html">
           4.1. Overview
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_model/Machine_Learning_Workflow.html">
           4.2. Machine Learning Workflow
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_model/Neural_Network_Programming.html">
           4.3. Neural Network Programming
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_model/Functional_Programming.html">
           4.4. Functional Programming
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_model/Bridging_Python_and_C_C%2B%2B_Functions.html">
           4.5. Bridging Python and C/C++ Functions
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_model/Chapter_Summary.html">
           4.6. Chapter Summary
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_compiler_frontend/index.html">
         5. AI Compiler Frontend
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_frontend/Overview_of_AI_Compilers.html">
           5.1. Overview of AI Compilers
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_frontend/Overview_of_AI_Compiler_Frontends.html">
           5.2. Overview of AI Compiler Frontends
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_frontend/Intermediate_Representation.html">
           5.3. Intermediate Representation
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_frontend/Automatic_Differentiation.html">
           5.4. Automatic Differentiation
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_frontend/Type_Systems_and_Static_Analysis.html">
           5.5. Type Systems and Static Analysis
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_frontend/Frontend_Compilation_Optimization.html">
           5.6. Frontend Compilation Optimization
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_frontend/Chapter_Summary.html">
           5.7. Chapter Summary
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_frontend/Further_Reading.html">
           5.8. Further Reading
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_compiler_backend/index.html">
         6. AI Compiler Backend
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_backend/Overview.html">
           6.1. Overview
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_backend/Graph_Optimization.html">
           6.2. Graph Optimization
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_backend/Operator_Selection.html">
           6.3. Operator Selection
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_backend/Memory_Allocation.html">
           6.4. Memory Allocation
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_backend/Operator_Compiler.html">
           6.5. Operator Compiler
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_backend/Chapter_Summary.html">
           6.6. Chapter Summary
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_compiler_backend/Further_Reading.html">
           6.7. Further Reading
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_accelerator/index.html">
         7. Hardware Accelerator
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/Overview.html">
           7.1. Overview
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/Components_of_Hardware_Accelerators.html">
           7.2. Components of Hardware Accelerators
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/Programming_Methods.html">
           7.3. Programming Methods
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/Performance_Optimization_Methods.html">
           7.4. Performance Optimization Methods
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/Chapter_Summary.html">
           7.5. Chapter Summary
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_distributed/index.html">
         8. Distributed Training
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed/Overview.html">
           8.1. Overview
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed/Parallelism_Methods.html">
           8.2. Parallelism Methods
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed/Pipeline_Parallelism_with_Micro-Batching.html">
           8.3. Pipeline Parallelism with Micro-Batching
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed/Architecture_of_Machine_Learning_Clusters.html">
           8.4. Architecture of Machine Learning Clusters
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed/Collective_Communication.html">
           8.5. Collective Communication
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed/Parameter_Server.html">
           8.6. Parameter Server
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed/Federated_Learning.html">
           8.7. Federated Learning
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed/Training_Large_Language_Models.html">
           8.8. Training Large Language Models
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed/Chapter_Summary.html">
           8.9. Chapter Summary
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed/Further_Reading.html">
           8.10. Further Reading
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_model_deployment/index.html">
         9. Model Deployment
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/Overview.html">
           9.1. Overview
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/Conversion_to_Inference_Model_and_Model_Optimization.html">
           9.2. Conversion to Inference Model and Model Optimization
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/Model_Compression.html">
           9.3. Model Compression
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/Advanced_Efficient_Techniques.html">
           9.4. Advanced Efficient Techniques
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/Model_Inference.html">
           9.5. Model Inference
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/Security_Protection_of_Models.html">
           9.6. Security Protection of Models
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/Chapter_Summary.html">
           9.7. Chapter Summary
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/Further_Reading.html">
           9.8. Further Reading
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_preface_extension/index.html">
         10. Part II Application Scenarios
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_recommender_system/index.html">
         11. Recommender System
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/Overview.html">
           11.1. Overview
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/System_Components.html">
           11.2. System Components
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/Recommendation_Pipeline.html">
           11.3. Recommendation Pipeline
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/Model_Update.html">
           11.4. Model Update
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/Supporting_Real-time_Machine_Learning.html">
           11.5. Supporting Real-time Machine Learning
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/Chapter_Summary.html">
           11.6. Chapter Summary
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/Further_Reading.html">
           11.7. Further Reading
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_reinforcement_learning/index.html">
         12. Reinforcement Learning System
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/Overview.html">
           12.1. Overview
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/Introduction_to_Reinforcement_Learning.html">
           12.2. Introduction to Reinforcement Learning
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/Single-Node_Reinforcement_Learning_System.html">
           12.3. Single-Node Reinforcement Learning System
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/Distributed_Reinforcement_Learning_System.html">
           12.4. Distributed Reinforcement Learning System
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/Multi-agent_Reinforcement_Learning.html">
           12.5. Multi-agent Reinforcement Learning
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/Multi-agent_Reinforcement_Learning_System.html">
           12.6. Multi-agent Reinforcement Learning System
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/Chapter_Summary.html">
           12.7. Chapter Summary
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1 current">
        <a class="reference internal" href="index.html">
         13. Robotic System
        </a>
        <ul class="current">
         <li class="toctree-l2">
          <a class="reference internal" href="Overview_of_Robotic_Systems.html">
           13.1. Overview of Robotic Systems
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="Robot_Operating_System.html">
           13.2. Robot Operating System
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="Case_Study.html">
           13.3. Case Study: Using ROS
          </a>
         </li>
         <li class="toctree-l2 current">
          <a class="current reference internal" href="#">
           13.4. Modern Robot Learning
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="Chapter_Summary.html">
           13.5. Chapter Summary
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </nav>
    </div>
   </header>
   <main class="mdl-layout__content" tabindex="0">
    <script src="../_static/sphinx_materialdesign_theme.js " type="text/javascript">
    </script>
    <header class="mdl-layout__drawer">
     <!-- Title -->
     <span class="mdl-layout-title">
      <a class="title" href="../index.html">
       <img alt="Machine Learning Systems: Design and Implementation" class="logo" src="../_static/logo-with-text.png"/>
      </a>
     </span>
     <div class="globaltoc">
      <span class="mdl-layout-title toc">
       Table Of Contents
      </span>
      <nav class="mdl-navigation">
       <ul class="current">
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_preface/index.html">
          1. Preface
         </a>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_introduction/index.html">
          2. Introduction
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/Machine_Learning_Applications.html">
            2.1. Machine Learning Applications
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/Design_Objectives_of_Machine_Learning_Frameworks.html">
            2.2. Design Objectives of Machine Learning Frameworks
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/Machine_Learning_Framework_Architecture.html">
            2.3. Machine Learning Framework Architecture
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/Application_Scenarios_of_Machine_Learning_Systems.html">
            2.4. Application Scenarios of Machine Learning Systems
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/Book_Organization_and_Intended_Audience.html">
            2.5. Book Organization and Intended Audience
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_preface_basic/index.html">
          3. Part I Framework Design
         </a>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_programming_model/index.html">
          4. Programming Model
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_model/Overview.html">
            4.1. Overview
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_model/Machine_Learning_Workflow.html">
            4.2. Machine Learning Workflow
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_model/Neural_Network_Programming.html">
            4.3. Neural Network Programming
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_model/Functional_Programming.html">
            4.4. Functional Programming
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_model/Bridging_Python_and_C_C%2B%2B_Functions.html">
            4.5. Bridging Python and C/C++ Functions
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_model/Chapter_Summary.html">
            4.6. Chapter Summary
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_compiler_frontend/index.html">
          5. AI Compiler Frontend
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_frontend/Overview_of_AI_Compilers.html">
            5.1. Overview of AI Compilers
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_frontend/Overview_of_AI_Compiler_Frontends.html">
            5.2. Overview of AI Compiler Frontends
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_frontend/Intermediate_Representation.html">
            5.3. Intermediate Representation
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_frontend/Automatic_Differentiation.html">
            5.4. Automatic Differentiation
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_frontend/Type_Systems_and_Static_Analysis.html">
            5.5. Type Systems and Static Analysis
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_frontend/Frontend_Compilation_Optimization.html">
            5.6. Frontend Compilation Optimization
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_frontend/Chapter_Summary.html">
            5.7. Chapter Summary
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_frontend/Further_Reading.html">
            5.8. Further Reading
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_compiler_backend/index.html">
          6. AI Compiler Backend
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_backend/Overview.html">
            6.1. Overview
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_backend/Graph_Optimization.html">
            6.2. Graph Optimization
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_backend/Operator_Selection.html">
            6.3. Operator Selection
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_backend/Memory_Allocation.html">
            6.4. Memory Allocation
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_backend/Operator_Compiler.html">
            6.5. Operator Compiler
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_backend/Chapter_Summary.html">
            6.6. Chapter Summary
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_compiler_backend/Further_Reading.html">
            6.7. Further Reading
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_accelerator/index.html">
          7. Hardware Accelerator
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/Overview.html">
            7.1. Overview
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/Components_of_Hardware_Accelerators.html">
            7.2. Components of Hardware Accelerators
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/Programming_Methods.html">
            7.3. Programming Methods
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/Performance_Optimization_Methods.html">
            7.4. Performance Optimization Methods
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/Chapter_Summary.html">
            7.5. Chapter Summary
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_distributed/index.html">
          8. Distributed Training
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed/Overview.html">
            8.1. Overview
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed/Parallelism_Methods.html">
            8.2. Parallelism Methods
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed/Pipeline_Parallelism_with_Micro-Batching.html">
            8.3. Pipeline Parallelism with Micro-Batching
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed/Architecture_of_Machine_Learning_Clusters.html">
            8.4. Architecture of Machine Learning Clusters
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed/Collective_Communication.html">
            8.5. Collective Communication
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed/Parameter_Server.html">
            8.6. Parameter Server
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed/Federated_Learning.html">
            8.7. Federated Learning
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed/Training_Large_Language_Models.html">
            8.8. Training Large Language Models
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed/Chapter_Summary.html">
            8.9. Chapter Summary
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed/Further_Reading.html">
            8.10. Further Reading
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_model_deployment/index.html">
          9. Model Deployment
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/Overview.html">
            9.1. Overview
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/Conversion_to_Inference_Model_and_Model_Optimization.html">
            9.2. Conversion to Inference Model and Model Optimization
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/Model_Compression.html">
            9.3. Model Compression
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/Advanced_Efficient_Techniques.html">
            9.4. Advanced Efficient Techniques
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/Model_Inference.html">
            9.5. Model Inference
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/Security_Protection_of_Models.html">
            9.6. Security Protection of Models
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/Chapter_Summary.html">
            9.7. Chapter Summary
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/Further_Reading.html">
            9.8. Further Reading
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_preface_extension/index.html">
          10. Part II Application Scenarios
         </a>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_recommender_system/index.html">
          11. Recommender System
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/Overview.html">
            11.1. Overview
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/System_Components.html">
            11.2. System Components
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/Recommendation_Pipeline.html">
            11.3. Recommendation Pipeline
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/Model_Update.html">
            11.4. Model Update
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/Supporting_Real-time_Machine_Learning.html">
            11.5. Supporting Real-time Machine Learning
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/Chapter_Summary.html">
            11.6. Chapter Summary
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/Further_Reading.html">
            11.7. Further Reading
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_reinforcement_learning/index.html">
          12. Reinforcement Learning System
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/Overview.html">
            12.1. Overview
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/Introduction_to_Reinforcement_Learning.html">
            12.2. Introduction to Reinforcement Learning
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/Single-Node_Reinforcement_Learning_System.html">
            12.3. Single-Node Reinforcement Learning System
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/Distributed_Reinforcement_Learning_System.html">
            12.4. Distributed Reinforcement Learning System
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/Multi-agent_Reinforcement_Learning.html">
            12.5. Multi-agent Reinforcement Learning
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/Multi-agent_Reinforcement_Learning_System.html">
            12.6. Multi-agent Reinforcement Learning System
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/Chapter_Summary.html">
            12.7. Chapter Summary
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1 current">
         <a class="reference internal" href="index.html">
          13. Robotic System
         </a>
         <ul class="current">
          <li class="toctree-l2">
           <a class="reference internal" href="Overview_of_Robotic_Systems.html">
            13.1. Overview of Robotic Systems
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="Robot_Operating_System.html">
            13.2. Robot Operating System
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="Case_Study.html">
            13.3. Case Study: Using ROS
           </a>
          </li>
          <li class="toctree-l2 current">
           <a class="current reference internal" href="#">
            13.4. Modern Robot Learning
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="Chapter_Summary.html">
            13.5. Chapter Summary
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </nav>
     </div>
    </header>
    <div class="document">
     <div class="page-content" role="main">
      <div class="section" id="modern-robot-learning">
       <h1>
        <span class="section-number">
         13.4.
        </span>
        Modern Robot Learning
        <a class="headerlink" href="#modern-robot-learning" title="Permalink to this heading">
         ¶
        </a>
       </h1>
       <div class="section" id="overview">
        <h2>
         <span class="section-number">
          13.4.1.
         </span>
         Overview
         <a class="headerlink" href="#overview" title="Permalink to this heading">
          ¶
         </a>
        </h2>
        <p>
         We human can interact with objects with various shapes, sizes, materials
and physics properties. And we have a fantanstic ability to do it under
different environment settings: we can avoid potentially dangerous
collisions, we can finish a task with a long horizon; and we can learn
extremely hard task after practice. However, it still a challenge to
teach a machine to have above ability. A key challenge in intelligent
robotics is creating robots that are capable of directly interacting
with the world around them to achieve their goals. The last decade has
seen substantial growth in research on the problem of robot
manipulation, which aims to exploit the increasing availability of
affordable robot arms and grippers to create robots capable of directly
interacting with the world to achieve their goals. Learning will be
central to such autonomous systems, as the real world contains too much
variation for a robot to expect to have an accurate model of its
environment, the objects in it, or the skills required to manipulate
them.
        </p>
        <div class="figure align-default" id="id10">
         <span id="robot-framework">
         </span>
         <img alt="../_images/framework.pdf" src="../_images/framework.pdf"/>
         <p class="caption">
          <span class="caption-number">
           Fig. 13.4.1
          </span>
          <span class="caption-text">
           An Overview of Common Frameworks for RobotLearning.
          </span>
          <a class="headerlink" href="#id10" title="Permalink to this image">
           ¶
          </a>
         </p>
        </div>
       </div>
       <div class="section" id="robot-interaction-environments">
        <h2>
         <span class="section-number">
          13.4.2.
         </span>
         Robot Interaction Environments
         <a class="headerlink" href="#robot-interaction-environments" title="Permalink to this heading">
          ¶
         </a>
        </h2>
        <p>
         In the realm of robot learning, the process of acquiring the ability to
manipulate objects in real-world settings can be a time-consuming and
resource-intensive endeavor. Not only does it demand considerable
effort, but it also necessitates substantial hardware investments. To
tackle these challenges and expedite the learning process, researchers
have turned to data-driven methods that make use of simulation
environments. By simulating various scenarios, these environments enable
robots to learn and make informed decisions without the need for
physically interacting with the real world. In this chapter, we will
explore the concept of using simulation environments for robot decision
making, highlighting some simulator examples and their key features,
while drawing comparisons to the real world and the underlying hardware
considerations.
        </p>
        <div class="section" id="learning-in-simulation-environments">
         <h3>
          <span class="section-number">
           13.4.2.1.
          </span>
          Learning in Simulation Environments:
          <a class="headerlink" href="#learning-in-simulation-environments" title="Permalink to this heading">
           ¶
          </a>
         </h3>
         <p>
          Simulators have emerged as indispensable tools for training robots in a
controlled and efficient manner. Instead of relying solely on real-world
experiences, simulation environments provide a cost-effective means of
generating large amounts of training data. This approach not only saves
time but also minimizes the risks associated with operating physical
robots in potentially hazardous or expensive settings. Furthermore,
simulators offer the flexibility to create diverse scenarios and
manipulate various environmental parameters, allowing researchers to
thoroughly explore different learning strategies and optimize robot
decision-making algorithms.
         </p>
         <span id="simulations">
         </span>
         <table class="docutils align-default" id="id11" style="margin-left:auto;margin-right:auto;margin-top:10px;margin-bottom:20px;">
          <caption>
           <span class="caption-number">
            Table 13.4.1
           </span>
           <span class="caption-text">
            Physics Simulation Libraries and Environments
           </span>
           <a class="headerlink" href="#id11" title="Permalink to this table">
            ¶
           </a>
          </caption>
          <colgroup>
           <col style="width: 4%"/>
           <col style="width: 16%"/>
           <col style="width: 80%"/>
          </colgroup>
          <thead>
           <tr class="row-odd">
            <th class="head">
             <ul class="simple">
              <li>
              </li>
              <li>
              </li>
             </ul>
             <p>
              Y
e
a
r
*
*
             </p>
            </th>
            <th class="head">
             <p>
              <strong>
               Simulat
or
              </strong>
             </p>
            </th>
            <th class="head">
             <p>
              <strong>
               Description
              </strong>
             </p>
            </th>
           </tr>
          </thead>
          <tbody>
           <tr class="row-even">
            <td>
             <p>
              2
0
1
2
             </p>
            </td>
            <td>
             <p>
              MuJoCo [@
todorov20
12mujoco]
             </p>
            </td>
            <td>
             <p>
              MuJoCo offers a unique combination of speed, accuracy
and modeling power, designed from the ground up for
model-based optimization and optimization through
contacts.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              2
0
1
6
             </p>
            </td>
            <td>
             <p>
              PyBullet
[@coumans
2016pybul
let]
             </p>
            </td>
            <td>
             <p>
              PyBullet is an easy to use Python module for physics
simulation, robotics and deep reinforcement learning
based on the Bullet Physics SDK.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              2
0
1
6
             </p>
            </td>
            <td>
             <p>
              Unreal
CV [@qiu2
016unreal
cv]
             </p>
            </td>
            <td>
             <p>
              Unreal CV provides functions for perception,
navigation, physical simulations, and learning and
evaluation of algorithms.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              2
0
1
6
             </p>
            </td>
            <td>
             <p>
              DeepMind
Lab [@bea
ttie2016d
eepmind]
             </p>
            </td>
            <td>
             <p>
              DeepMind Lab is a first-person 3D game platform
designed for research and development of general
artificial intelligence and machine learning systems.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              2
0
1
6
             </p>
            </td>
            <td>
             <p>
              Habitat [
@beattie2
016deepmi
nd]
             </p>
            </td>
            <td>
             <p>
              Habitat-Sim is a flexible, high-performance 3D
simulator with configurable agents, sensors, and
generic 3D dataset handling.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              2
0
1
7
             </p>
            </td>
            <td>
             <p>
              AI2-THOR
[@kolve20
17ai2]
             </p>
            </td>
            <td>
             <p>
              AI2-THOR consists of near photo-realistic 3D indoor
scenes, enabling research in various domains
including deep reinforcement learning and visual
question answering.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              2
0
1
8
             </p>
            </td>
            <td>
             <p>
              CHALET [@
yan2018ch
alet]
             </p>
            </td>
            <td>
             <p>
              CHALET includes rooms and house configurations for
training and evaluating autonomous agents in a
challenging dynamic environment.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              2
0
1
8
             </p>
            </td>
            <td>
             <p>
              VirtualHo
me [@puig
2018virtu
alhome]
             </p>
            </td>
            <td>
             <p>
              VirtualHome simulator allows the creation of a large
activity video dataset with rich ground-truth,
enabling training and testing of video understanding
models.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              2
0
1
9
             </p>
            </td>
            <td>
             <p>
              VRKitchen
 [@gao201
9vrkitche
n]
             </p>
            </td>
            <td>
             <p>
              VRKitchen enables embodied agents to perform complex
tasks involving fine-grained object manipulations in
a realistic environment, and supports learning from
demonstration.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              2
0
2
0
             </p>
            </td>
            <td>
             <p>
              SAPIEN [@
xiang2020
sapien]
             </p>
            </td>
            <td>
             <p>
              SAPIEN is a realistic and physics-rich simulated
environment that hosts a large-scale set of
articulated objects, enabling various robotic vision
and interaction tasks.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              2
0
2
0
             </p>
            </td>
            <td>
             <p>
              ThreeDWor
ld [@gan2
020threed
world]
             </p>
            </td>
            <td>
             <p>
              ThreeDWorld is a platform for interactive multi-modal
physical simulation, supporting real-time
near-photo-realistic image rendering, high-fidelity
audio rendering, and more.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              2
0
2
1
             </p>
            </td>
            <td>
             <p>
              Brax [@fr
eeman2021
brax]
             </p>
            </td>
            <td>
             <p>
              Brax is an open-source library for rigid body
simulation with a focus on performance and
parallelism on accelerators, written in JAX.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              2
0
2
1
             </p>
            </td>
            <td>
             <p>
              iGibson2.
0 [@li202
1igibson]
             </p>
            </td>
            <td>
             <p>
              iGibson 2.0 supports object states, predicate logic
functions, and can sample valid physical states based
on logic states.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              2
0
2
1
             </p>
            </td>
            <td>
             <p>
              Issac
Gym [@mak
oviychuk2
021isaac]
             </p>
            </td>
            <td>
             <p>
              Isaac Gym offers a high performance learning platform
to train policies for a wide variety of robotics
tasks directly on GPU.
             </p>
            </td>
           </tr>
          </tbody>
         </table>
         <p>
          As shown in Table :numref:
          <code class="docutils literal notranslate">
           <span class="pre">
            simulations
           </span>
          </code>
          , there are several simulator
examples used in the field.
         </p>
        </div>
        <div class="section" id="comparing-simulators-to-the-real-world">
         <h3>
          <span class="section-number">
           13.4.2.2.
          </span>
          Comparing Simulators to the Real World:
          <a class="headerlink" href="#comparing-simulators-to-the-real-world" title="Permalink to this heading">
           ¶
          </a>
         </h3>
         <p>
          While simulation environments offer numerous advantages, it is essential
to recognize the differences between these virtual worlds and the real
world. One crucial distinction is the potential discrepancy in physics
and dynamics modeling. Simulators often approximate physical
interactions, which may deviate from the intricacies and complexities of
the actual physical systems. Researchers must be cognizant of these
limitations and carefully validate their learned policies in real-world
scenarios to ensure robustness and generalizability.
         </p>
         <p>
          Another consideration is the sensory input. In simulation, sensors can
provide perfect, noise-free data, whereas real-world sensors are prone
to noise, calibration issues, and limited field of view. Accounting for
these discrepancies during the learning process and applying appropriate
techniques, such as sensor noise injection or domain adaptation, is
crucial for achieving successful transfer of learned policies from
simulation to reality.
         </p>
         <div class="figure align-default" id="id12">
          <img alt="../_images/realworld.pdf" src="../_images/realworld.pdf"/>
          <p class="caption">
           <span class="caption-number">
            Fig. 13.4.2
           </span>
           <span class="caption-text">
            Robot manipulation environments in real world [@geng2022end].
           </span>
           <a class="headerlink" href="#id12" title="Permalink to this image">
            ¶
           </a>
          </p>
         </div>
        </div>
        <div class="section" id="hardware-considerations">
         <span id="id1">
         </span>
         <h3>
          <span class="section-number">
           13.4.2.3.
          </span>
          Hardware Considerations:
          <a class="headerlink" href="#hardware-considerations" title="Permalink to this heading">
           ¶
          </a>
         </h3>
         <p>
          While simulation environments alleviate the need for expensive hardware
setups, certain hardware-related aspects should not be overlooked. To
accurately simulate robot behaviors, it is necessary to employ hardware
models and kinematic representations that closely resemble the physical
robots being studied. Additionally, to ensure efficient and real-time
simulation, high-performance computing resources may be required,
especially when dealing with complex scenarios or large-scale
simulations.
         </p>
        </div>
        <div class="section" id="conclusion">
         <span id="id2">
         </span>
         <h3>
          <span class="section-number">
           13.4.2.4.
          </span>
          Conclusion:
          <a class="headerlink" href="#conclusion" title="Permalink to this heading">
           ¶
          </a>
         </h3>
         <p>
          Simulation environments have become invaluable tools for training robots
and enabling efficient decision making. By leveraging these virtual
worlds, researchers can expedite the learning process while minimizing
the costs and risks associated with real-world experimentation. However,
it is important to acknowledge the limitations of simulations and the
challenges in transferring learned policies to physical systems. Through
a combination of careful validation, sensor calibration, and hardware
modeling, the gap between simulation and reality can be bridged, paving
the way for robust and reliable robot decision-making capabilities in
real-world settings.
         </p>
        </div>
       </div>
       <div class="section" id="robot-skill-learning">
        <h2>
         <span class="section-number">
          13.4.3.
         </span>
         Robot Skill Learning
         <a class="headerlink" href="#robot-skill-learning" title="Permalink to this heading">
          ¶
         </a>
        </h2>
        <div class="section" id="perception">
         <span id="id3">
         </span>
         <h3>
          <span class="section-number">
           13.4.3.1.
          </span>
          Perception
          <a class="headerlink" href="#perception" title="Permalink to this heading">
           ¶
          </a>
         </h3>
         <p>
          In the fascinating realm of robotics, perception plays a crucial role in
enabling robots to interact with and understand the world around them.
Just like humans rely on their senses to gather information, robots
employ various sensing modalities to perceive and make sense of their
environment. This chapter delves into the realm of robot perception,
focusing on the integration of multiple sensing modalities and their
significance in enhancing a robot’s understanding of its surroundings.
We will explore the diverse range of sensing modalities available to
robots and their unique capabilities in perception.
         </p>
         <p>
          <strong>
           Visual Perception:
          </strong>
          Visual perception is one of the primary sensing
modalities for robots, mimicking human vision. Cameras and image sensors
capture visual data, allowing robots to perceive objects, scenes, and
spatial information. This section discusses the role of computer vision
techniques, such as image processing, object recognition, and depth
estimation, in enabling robots to interpret visual data and extract
meaningful information.
         </p>
         <p>
          <strong>
           Tactile Perception:
          </strong>
          Tactile perception focuses on a robot’s ability
to sense and interpret physical contact with objects and surfaces.
Tactile sensors embedded in robotic fingers or hands provide information
about texture, shape, hardness, and temperature. This section explores
the integration of tactile sensors and their application in object
manipulation, grasping, and fine motor control, enabling robots to
interact with the physical world in a more human-like manner.
         </p>
         <p>
          <strong>
           Auditory Perception:
          </strong>
          Sound and auditory cues are valuable sources of
information for robots. By integrating microphones or specialized
auditory sensors, robots can perceive and analyze audio signals, such as
speech, environmental sounds, and localization cues. This section
discusses the role of auditory perception in tasks like speech
recognition, sound source localization, and human-robot interaction,
highlighting the importance of audio-based information for comprehensive
robot perception.
         </p>
         <p>
          <strong>
           Range Sensing:
          </strong>
          Range sensing modalities, such as LiDAR (Light
Detection and Ranging) and depth cameras, provide robots with depth
information about their surroundings. By emitting and measuring the
time-of-flight or structured light patterns, robots can create detailed
3D representations of the environment. This section explores the
capabilities of range sensing modalities in object detection,
simultaneous localization and mapping (SLAM), and navigation in dynamic
environments.
         </p>
         <p>
          <strong>
           Environmental Sensing:
          </strong>
          Robots can also perceive the environment
using various sensors that capture information beyond the range of human
senses. This section covers sensing modalities like infrared sensors,
gas sensors, and environmental monitoring devices. These sensors enable
robots to detect temperature, gas concentrations, humidity, and other
environmental factors, making them suitable for applications in
environmental monitoring, disaster response, and industrial settings.
         </p>
         <p>
          <strong>
           Fusion and Integration of Sensing Modalities:
          </strong>
          To achieve a more
comprehensive understanding of the environment, robots often integrate
multiple sensing modalities. This section explores the challenges and
techniques involved in fusing data from different sensors, such as
sensor calibration, data alignment, and sensor fusion algorithms. It
also discusses the benefits of sensor fusion in improving perception
accuracy, robustness, and adaptability in real-world scenarios.
         </p>
         <p>
          <strong>
           Conclusion:
          </strong>
          Robot perception is a fascinating field that encompasses
multiple sensing modalities, each contributing to a robot’s ability to
perceive and understand its surroundings. By integrating visual,
tactile, auditory, range, and environmental sensing, robots can create a
rich representation of the world. This chapter provided an overview of
different sensing modalities, highlighting their significance in
enabling robots to navigate, interact, and make informed decisions in
complex environments. By harnessing the power of diverse sensing
modalities, robots continue to advance in their ability to perceive,
learn, and adapt, bringing us closer to a world where intelligent
machines coexist seamlessly with humans.
         </p>
        </div>
        <div class="section" id="decision">
         <h3>
          <span class="section-number">
           13.4.3.2.
          </span>
          Decision
          <a class="headerlink" href="#decision" title="Permalink to this heading">
           ¶
          </a>
         </h3>
         <p>
          <code class="xref std std-numref docutils literal notranslate">
           <span class="pre">
            decision
           </span>
          </code>
         </p>
         <p>
          In the field of robotics, decision-making plays a vital role in enabling
robots to interact intelligently with their environment. This chapter
explores various approaches to robot decision-making, including
heuristic policy, reinforcement learning, affordance learning, and
knowledge extraction from large models. By understanding these
techniques, we can gain insights into how robots can make informed and
adaptive decisions.
         </p>
         <p>
          <strong>
           Heuristic Policy:
          </strong>
          Heuristic policy refers to a decision-making
strategy based on predefined rules or heuristics. These rules are
typically crafted by human experts and capture domain-specific knowledge
to guide the robot’s actions. Heuristic policies provide a fast and
reliable way for robots to make decisions in certain contexts, but they
may lack adaptability and struggle in complex and dynamic environments.
Nevertheless, they can serve as a useful starting point for robot
decision-making before more sophisticated techniques are employed.
         </p>
         <p>
          <strong>
           Reinforcement Learning:
          </strong>
          Reinforcement learning (RL) is a powerful
approach that enables robots to learn decision-making policies through
interactions with the environment [@chen2022towards]. In RL, a robot
learns to maximize a numerical reward signal by taking actions and
observing their outcomes. By exploring different actions and receiving
feedback in the form of rewards or penalties, the robot gradually
discovers the optimal policy for decision-making.
         </p>
         <p>
          Reinforcement learning consists of the following key elements: a. Agent:
The robot or decision-making entity. b. Environment: The external world
or simulation in which the agent interacts. c. State: The representation
of the current situation or context. d. Action: The choices available to
the agent. e. Reward: The feedback signal indicating the desirability of
an action in a given state.
         </p>
         <p>
          <strong>
           Affordance Learning:
          </strong>
         </p>
         <div class="figure align-default" id="id13">
          <img alt="../_images/affordance.pdf" src="../_images/affordance.pdf"/>
          <p class="caption">
           <span class="caption-number">
            Fig. 13.4.3
           </span>
           <span class="caption-text">
            Affordance visualization results in robot manipulation
tasks [@geng2022end].
           </span>
           <a class="headerlink" href="#id13" title="Permalink to this image">
            ¶
           </a>
          </p>
         </div>
         <p>
          Affordance learning focuses on understanding the action possibilities
that the environment offers to a robot [@mo2021where2act; @wu2021vat;
@zhao2022dualafford; @wang2022adaafford; @geng2022end]. It involves
perceiving and extracting relevant information about the affordances,
which are the potential actions or interactions that an object or a
scene can afford. By recognizing and understanding affordances, robots
can make more informed decisions about their actions. Affordance
learning includes the following steps: a. Perception: Sensing the
environment through various sensors (e.g., cameras, depth sensors) to
capture relevant data. b. Feature Extraction: Extracting meaningful
features from the sensory data to represent the objects or scenes.
c. Affordance Recognition: Identifying and categorizing the potential
actions or interactions that the objects or scenes afford.
d. Decision-Making: Utilizing the recognized affordances to guide the
robot’s decision-making process.
         </p>
         <p>
          <strong>
           Knowledge Extraction from Large Models:
          </strong>
          With the advancement of deep
learning and large-scale language models, extracting knowledge from
these models has become increasingly valuable for decision-making in
robotics. These models are trained on vast amounts of data and capture
intricate patterns and relationships. By leveraging this knowledge,
robots can benefit from the collective intelligence captured in these
models [@brohan2023can; @liang2023cod; @huang2023visual]. Typically,
knowledge extraction from large models involves the following steps: a.
Model Interpretability: Understanding how the model processes input data
and generates predictions or decisions. b. Feature Extraction:
Extracting relevant features or representations from the model that
capture the essential information for decision-making. c. Transfer
Learning: Transferring knowledge from pre-trained models to improve
decision-making in specific robotic tasks. d. Decision Fusion: Combining
the knowledge extracted from the large models with other decision-making
techniques to make more informed and robust decisions.
         </p>
         <p>
          Robot decision-making is a critical aspect of enabling robots to
interact intelligently with their environment. Heuristic policies
provide initial guidelines, while reinforcement learning allows robots
to learn optimal decision-making policies through interaction with the
environment. Affordance learning helps robots recognize and understand
the potential actions in their surroundings. Finally, knowledge
extraction from large models leverages the wealth of information
captured in these models to enhance decision-making capabilities. By
integrating these techniques, robots can make more adaptive,
context-aware, and intelligent decisions in various scenarios, leading
to significant advancements in the field of robot learning.
         </p>
        </div>
       </div>
       <div class="section" id="deployment-in-real-environments">
        <h2>
         <span class="section-number">
          13.4.4.
         </span>
         Deployment in real environments
         <a class="headerlink" href="#deployment-in-real-environments" title="Permalink to this heading">
          ¶
         </a>
        </h2>
        <p>
         In the field of robotics, the ability of a robot to make intelligent
decisions is crucial for its effective interaction with the real world.
Over the years, significant progress has been made in developing
algorithms and techniques to enable robots to learn and make decisions
directly in real environments. In this chapter, we will explore some key
topics related to robot decision-making, including learning directly in
real environments, the Sim2Real approach, real-world feedback, learning
from real-world demonstrations, and the concept of Teacher-Student
Distillation.
        </p>
        <div class="section" id="learning-directly-in-real-environments">
         <span id="id4">
         </span>
         <h3>
          <span class="section-number">
           13.4.4.1.
          </span>
          Learning Directly in Real Environments
          <a class="headerlink" href="#learning-directly-in-real-environments" title="Permalink to this heading">
           ¶
          </a>
         </h3>
         <p>
          One of the fundamental approaches to robot decision-making involves
learning directly in real environments. Traditional methods often rely
on simulations or simplified scenarios, but learning in real-world
conditions provides robots with a more accurate understanding of the
complexities and uncertainties they encounter. This approach leverages
techniques such as reinforcement learning, where robots learn through
trial and error, optimizing their decision-making policies based on
feedback received from the environment.
         </p>
        </div>
        <div class="section" id="sim2real-and-real-world-feedback">
         <span id="id5">
         </span>
         <h3>
          <span class="section-number">
           13.4.4.2.
          </span>
          Sim2Real and Real-World Feedback
          <a class="headerlink" href="#sim2real-and-real-world-feedback" title="Permalink to this heading">
           ¶
          </a>
         </h3>
         <p>
          While learning in real environments is desirable, it can be challenging
due to constraints such as safety concerns, cost, and limited access to
real-world scenarios. The Sim2Real approach [@hofer2021sim2real]
addresses these challenges by training robots initially in simulation
environments, which are cheaper, safer, and provide more extensive data.
However, to ensure effective decision-making in the real world, the
trained policies need to be transferred and adapted to the physical
domain. Real-world feedback mechanisms, such as domain adaptation
techniques and reinforcement learning with fine-tuning, play a vital
role in this process by bridging the simulation-to-reality gap.
         </p>
        </div>
        <div class="section" id="learning-from-real-world-demonstrations">
         <span id="id6">
         </span>
         <h3>
          <span class="section-number">
           13.4.4.3.
          </span>
          Learning from Real-World Demonstrations
          <a class="headerlink" href="#learning-from-real-world-demonstrations" title="Permalink to this heading">
           ¶
          </a>
         </h3>
         <p>
          Learning from real-world demonstrations is another powerful paradigm for
robot decision-making. By observing and imitating human or expert
demonstrations, robots can acquire valuable knowledge about appropriate
decision-making strategies. This approach often involves techniques such
as imitation learning or inverse reinforcement learning, where robots
learn from the actions and decisions of skilled individuals. By
generalizing from a limited number of demonstrations, robots can learn
to make intelligent decisions in similar contexts.
         </p>
        </div>
        <div class="section" id="conclusion-1">
         <span id="teacher-student-distillation">
         </span>
         <span id="id7">
         </span>
         <h3>
          <span class="section-number">
           13.4.4.4.
          </span>
          Teacher-Student Distillation
          <a class="headerlink" href="#conclusion-1" title="Permalink to this heading">
           ¶
          </a>
         </h3>
         <p>
          Teacher-Student Distillation is a methodology that leverages the
knowledge of a more capable teacher model to enhance the decision-making
capabilities of a less experienced student model [@gou2021knowledge;
@hussein2017imitation; @ross2011reduction; @geng2023partmanip]. In the
context of robot learning, this approach involves training a
high-performing policy (the teacher) and using its expertise to guide
and improve the learning process of a less optimal policy (the student).
This technique can be particularly useful in scenarios where direct
reinforcement learning is challenging or time-consuming. The teacher
model can provide valuable feedback and constraints to facilitate
efficient decision-making in complex real-world environments.
         </p>
        </div>
        <div class="section" id="id8">
         <span id="id9">
         </span>
         <h3>
          <span class="section-number">
           13.4.4.5.
          </span>
          Conclusion
          <a class="headerlink" href="#id8" title="Permalink to this heading">
           ¶
          </a>
         </h3>
         <p>
          Robot decision-making in real environments is a multifaceted field that
encompasses various approaches and techniques. Learning directly in real
environments, leveraging Sim2Real methods, incorporating real-world
feedback, learning from demonstrations, and employing Teacher-Student
Distillation are all essential aspects of advancing the capabilities of
robots. As researchers and practitioners, we continue to explore these
topics to enhance robot learning and decision-making, enabling robots to
operate effectively and intelligently in diverse real-world settings.
Teacher-Student Distillation
         </p>
        </div>
       </div>
      </div>
     </div>
     <div class="side-doc-outline">
      <div class="side-doc-outline--content">
       <div class="localtoc">
        <p class="caption">
         <span class="caption-text">
          Table Of Contents
         </span>
        </p>
        <ul>
         <li>
          <a class="reference internal" href="#">
           13.4. Modern Robot Learning
          </a>
          <ul>
           <li>
            <a class="reference internal" href="#overview">
             13.4.1. Overview
            </a>
           </li>
           <li>
            <a class="reference internal" href="#robot-interaction-environments">
             13.4.2. Robot Interaction Environments
            </a>
            <ul>
             <li>
              <a class="reference internal" href="#learning-in-simulation-environments">
               13.4.2.1. Learning in Simulation Environments:
              </a>
             </li>
             <li>
              <a class="reference internal" href="#comparing-simulators-to-the-real-world">
               13.4.2.2. Comparing Simulators to the Real World:
              </a>
             </li>
             <li>
              <a class="reference internal" href="#hardware-considerations">
               13.4.2.3. Hardware Considerations:
              </a>
             </li>
             <li>
              <a class="reference internal" href="#conclusion">
               13.4.2.4. Conclusion:
              </a>
             </li>
            </ul>
           </li>
           <li>
            <a class="reference internal" href="#robot-skill-learning">
             13.4.3. Robot Skill Learning
            </a>
            <ul>
             <li>
              <a class="reference internal" href="#perception">
               13.4.3.1. Perception
              </a>
             </li>
             <li>
              <a class="reference internal" href="#decision">
               13.4.3.2. Decision
              </a>
             </li>
            </ul>
           </li>
           <li>
            <a class="reference internal" href="#deployment-in-real-environments">
             13.4.4. Deployment in real environments
            </a>
            <ul>
             <li>
              <a class="reference internal" href="#learning-directly-in-real-environments">
               13.4.4.1. Learning Directly in Real Environments
              </a>
             </li>
             <li>
              <a class="reference internal" href="#sim2real-and-real-world-feedback">
               13.4.4.2. Sim2Real and Real-World Feedback
              </a>
             </li>
             <li>
              <a class="reference internal" href="#learning-from-real-world-demonstrations">
               13.4.4.3. Learning from Real-World Demonstrations
              </a>
             </li>
             <li>
              <a class="reference internal" href="#conclusion-1">
               13.4.4.4. Teacher-Student Distillation
              </a>
             </li>
             <li>
              <a class="reference internal" href="#id8">
               13.4.4.5. Conclusion
              </a>
             </li>
            </ul>
           </li>
          </ul>
         </li>
        </ul>
       </div>
      </div>
     </div>
     <div class="clearer">
     </div>
    </div>
    <div class="pagenation">
     <a accesskey="P" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" href="Case_Study.html" id="button-prev" role="botton">
      <i class="pagenation-arrow-L fas fa-arrow-left fa-lg">
      </i>
      <div class="pagenation-text">
       <span class="pagenation-direction">
        Previous
       </span>
       <div>
        13.3. Case Study: Using ROS
       </div>
      </div>
     </a>
     <a accesskey="N" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" href="Chapter_Summary.html" id="button-next" role="botton">
      <i class="pagenation-arrow-R fas fa-arrow-right fa-lg">
      </i>
      <div class="pagenation-text">
       <span class="pagenation-direction">
        Next
       </span>
       <div>
        13.5. Chapter Summary
       </div>
      </div>
     </a>
    </div>
   </main>
  </div>
 </body>
</html>
