# Advanced Efficient Techniques

In addition to standard model compression methods, some advanced
approaches are being developed to accelerate the decoding process of the
large models. These methods include generating specific tokens using
smaller models and the ability to generate multiple tokens in a single
step, resulting in accelerating the decoding process. Furthermore, there
are techniques that utilize the memory hierarchy for high throughput
computation, aiming to decrease memory I/O, and as a result, be more
efficient.

## Speculative Decoding

Speculative decoding is a strategy to speed up the decoding process,
based on insights provided by Leviathan et al. [@leviathan2023fast].

1.  Complex modeling tasks frequently encompass simpler subtasks that
    can be effectively approximated using more efficient models.

2.  By combining speculative execution with a unique sampling approach,
    it is possible to accelerate exact decoding from larger models. This
    is achieved by processing them with the outputs from the
    approximation models in parallel.

Figure :numref:`ch-deploy/sd` is a brief overview of Speculative
Decoding. It involves initially generating a series of tokens using a
draft model, which is a smaller and less complex model. These generated
tokens are then verified in parallel with the target model, which is a
larger model. The tokens that are finally executed in the output are
those that are accepted by the target model from the initial draft
tokens. Additionally, if rejection occurs, one more token is resampled
and generated from the adjusted distribution. If there is no rejection,
an extra token is generated by the target model using the draft tokens
as context.

<figure id="fig:ch-deploy/sd">
<div class="center">
<img src="../img/ch08/sd.png" style="width:95.0%" />
</div>
<figcaption>Speculative Decoding Overview</figcaption>
</figure>

To elaborate, the process begins with the draft model generating a
series of $\gamma$ tokens, denoted as $x_1, x_2, ..., x_{\gamma}$.
Subsequently, it preserves the distributions
$q_{1}(x), q_{2}(x), ..., q_{\gamma}(x)$ of these tokens for future
verification by the target model. These $\gamma$ tokens are then
inputted into the target model in parallel to calculate the logits for
the respective token combinations
$p_{1}(x), p_{2}(x), ..., p_{\gamma+1}(x)$, derived from
$M_{\text{target}}(\text{prefix} + [x_1 + ... + x_{\gamma}])$. If the
condition $q(x) < p(x)$ is met, the token is retained. In contrast, if
not met, the token faces a rejection chance of $1 - \frac{p(x)}{q(x)}$,
following which it is reselected from an adjusted distribution:

$$p'(x) = norm(max(0, p(x) - q(x)))$$ 
:eqlabel:`equ:sd_adjusted` 

In the paper [@leviathan2023fast],
Leviathan et al. have proved the correctness of this adjusted
distribution for resampling.

Under the assumption that the execution time for a single step of the
Target model is denoted as $T$, and that of the draft model as $cT$,
where $0<c\leq1$. The standard procedure using the target model to
generate $\gamma + 1$ tokens would require a total time of
$\gamma T + T$. In contrast, with speculative decoding, where
$\gamma + 1$ tokens are produced ($\gamma$ by the draft model and one
additional by the target model concurrently during the parallel
verification), the time required would be $\gamma cT + T$. If all
$\gamma$ draft tokens are accepted by the target model and $c$ is small
enough to make $cT << T$, speculative decoding has the potential to
significantly reduce latency during the decoding process.

To further explain, if we denote $\alpha = E(\beta)$ where $\beta$ is
the acceptance rate with a given prefix and $E(\beta)$ is a natural
measure of how well the draft model can approximate the target model
assuming $\beta$s are i.i.d., the expected number of tokens generated by
the speculative process is $\frac{1-\alpha^{\gamma+1}}{1-\alpha}$
[@leviathan2023fast]. According to the speculative decoding time for one
superstep $\gamma cT + T$, the expected time for generating one token
with speculative decoding is
$\frac{(c\gamma+1)(1-\alpha)}{1-\alpha^{\gamma+1}}T$. By choosing a good
$\gamma$ and a well-aligned efficient draft model meaning big $\alpha$
and small $c$, the result is desired.

Nevertheless, as the value of $\gamma$ continues to rise, it becomes
progressively more difficult for a draft model to generate draft tokens
with a high acceptance rate by the target model, especially as the
likelihood of acceptance typically diminishes when $\gamma$ exceeds a
certain value. In the worst-case scenario, if all draft tokens generated
by the draft model are rejected by the target model, then only the one
token that is resampled from the adjusted distribution will be decoded
following the speculative process. In this situation, the time spent on
generating $\gamma$ tokens with the draft model represented as
$\gamma cT$ effectively becomes a complete waste of time when compared
to generating a single token directly with the target model; in
addition, the draft model is consuming the GPU memory.

Therefore, finding the best $\gamma$ or having a well-designed draft
model that is effectively accepted by the target model is of importance.
There are some strategies that can be employed to address this issue
effectively. For example:

**Self-Derived Drafts from Target Models**

Is it possible to utilize the target model directly as the draft model,
rather than employing a separate smaller model, which could lead to
increased GPU memory usage? The answer is yes. Similar to the original
approach, the modification involves switching the draft model into the
target model itself, followed by self-verifying these draft tokens. The
advantages of this method are:

1.  Since the draft model is almost the same as the target model, it is
    sufficiently robust to maintain a stable acceptance rate.

2.  Only need to keep one model in the GPU memory.

The challenge now lies in the ability to generate multiple future tokens
in a single decoding step. To achieve this, the concept involves
appending additional concurrent layers to the existing output layer of
the model. Stern et al. first proposed this method in
[@stern2018blockwise].

The training of these extra layers can either start from scratch with
the target model or involve fine-tuning a pre-trained model. This
approach forms the basis of the Medusa [@medusa]. Medusa's architecture
includes extra \"Medusa heads\" attached after the last hidden layer.
This design enables the model to generate a range of token candidates in
just one decoding step. Subsequently, these candidates undergo a
self-verification process, and only the accepted tokens are executed.

Other methodologies, such as implementing Knowledge Distillation between
draft and target models, employing multiple draft models instead of just
one, and replacing draft models with retrieval datasets proposed by
researchers are still being investigated to determine their
effectiveness and reliability.

Speculative decoding is an effective technique that uses smaller models
to reduce the overhead caused by larger models. By developing a
well-trained and aligned draft model, the efficiency of the decoding
process can be significantly improved.

## FlashAttention

FlashAttention is an advanced optimization technique utilizing the
memory hierarchy aimed at enhancing the efficiency of attention
computations in transformer models in terms of memory usage and speed.

Dao et al. were the first to suggest this approach, as indicated in
[@dao2022flashattention]. They noted the absence of *IO-awareness* --
the consideration of I/O interactions across GPU memory layers -- in the
classic Scaled Dot-Product Attention algorithm. To address this, they
introduced FlashAttention, an enhanced version of the attention
algorithm designed to minimize the intensive access to the GPU's high
bandwidth memory (HBM). This innovation led to significant gains in both
computational speed and throughput.

Figure :numref:`ch-deploy/memory` shows the memory hierarchy with
corresponding bandwidths. The main goal of FlashAttention is to avoid
reading and writing the large attention matrix to and from HBM. And
perform computation in SRAM as much as possible.

The standard Scaled Dot-Product Attention [@attention] formula is

$$\textbf{A} = Softmax(\frac{\textbf{QK}^T}{\sqrt{d_k}})\textbf{V}$$ 
:eqlabel:`equ:std_attn`

As $d_k$ is a scalar, we can simplify it into three parts:

$$
\begin{aligned}
    \textbf{S} = \textbf{QK}^T\\
    \textbf{P} = Softmax(\textbf{S})\\
    \textbf{O} = \textbf{PV}
\end{aligned}$$ 
:eqlabel:`equ:attn_sep`

The matrices **K**, **Q**, **V** are all stored in HBM. The standard
implementation of attention follows these steps:

1.  Load **K, Q** from HBM, compute **$S$ = $QK^T$**, and write **S** to
    the HBM.

2.  Read **S** from HBM, compute **P** = $Softmax$(**S**), and write
    **P** to HBM.

3.  Load **P** and **V** from HBM, compute **O** = **PV**, and write
    **O** to HBM. Finally, return **O**.

The standard implementation of attention involves frequent I/O
interactions with HBM for large matrices reads/writes, leading to
reduced speed due to the intensive memory access requirements. Moreover,
it stores large intermediate matrices in HBM for backward propagation.

To handle such issues, FlashAttention divides the input components **Q,
K**, and **V** into blocks. These blocks are then transferred from
slower HBM to faster SRAM. Once in SRAM, the attention output is
computed with respect to these blocks. Two strategies involved are
called **tiling** and **recomputation**.

**Tiling**: Assuming a vector $x\in \mathbb{R}^D$, the basic Softmax can
be calculated as: 

$$
\begin{aligned}
m(x) = \max\limits_{i} x_i\\
l_{1}(x) = [e^{x_{1} - m(x)},\, ...\,,e^{x_{D} - m(x)}]\\
s_{1}(x) = \sum_{i} l_{1}(x)_i\\
Softmax(x) = \frac{l_{1}(x)}{s_{1}(x)}
\end{aligned}
$$

Attention can be computed by blocks, so large Softmax can be decomposed
into separated parts. To elaborate, assuming a vector $x \in\mathbb{R}^{2D}$: 

$$
\begin{aligned}
x = [x_{1}, \,x_{2}], \quad x_{1}, \, x_{2} \in\mathbb{R}^D\\
m(x) = \max(m(x_{1}), \,m(x_{2}))\\
l(x) = [e^{m(x_{1})-m_(x)}l_{1}(x_1),\, ... \, ,e^{m(x_2)-m(x)}l_{1}(x_2)]\\
s(x) = e^{m(x_{1})-m(x)}s_{1}(x_1) + e^{m(x_2)-m(x)}s_{1}(x_2)\\
Softmax(x) = \frac{l(x)}{s(x)}
\end{aligned}
$$
