{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a0b607",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Advanced Efficient Techniques\n",
    "\n",
    "In addition to standard model compression methods, some advanced\n",
    "approaches are being developed to accelerate the decoding process of the\n",
    "large models. These methods include generating specific tokens using\n",
    "smaller models and the ability to generate multiple tokens in a single\n",
    "step, resulting in accelerating the decoding process. Furthermore, there\n",
    "are techniques that utilize the memory hierarchy for high throughput\n",
    "computation, aiming to decrease memory I/O, and as a result, be more\n",
    "efficient.\n",
    "\n",
    "## Speculative Decoding\n",
    "\n",
    "Speculative decoding is a strategy to speed up the decoding process,\n",
    "based on insights provided by Leviathan et al. [@leviathan2023fast].\n",
    "\n",
    "1.  Complex modeling tasks frequently encompass simpler subtasks that\n",
    "    can be effectively approximated using more efficient models.\n",
    "\n",
    "2.  By combining speculative execution with a unique sampling approach,\n",
    "    it is possible to accelerate exact decoding from larger models. This\n",
    "    is achieved by processing them with the outputs from the\n",
    "    approximation models in parallel.\n",
    "\n",
    "Figure :numref:`ch-deploy/sd` is a brief overview of Speculative\n",
    "Decoding. It involves initially generating a series of tokens using a\n",
    "draft model, which is a smaller and less complex model. These generated\n",
    "tokens are then verified in parallel with the target model, which is a\n",
    "larger model. The tokens that are finally executed in the output are\n",
    "those that are accepted by the target model from the initial draft\n",
    "tokens. Additionally, if rejection occurs, one more token is resampled\n",
    "and generated from the adjusted distribution. If there is no rejection,\n",
    "an extra token is generated by the target model using the draft tokens\n",
    "as context.\n",
    "\n",
    "<figure id=\"fig:ch-deploy/sd\">\n",
    "<div class=\"center\">\n",
    "<img src=\"../img/ch08/sd.png\" style=\"width:95.0%\" />\n",
    "</div>\n",
    "<figcaption>Speculative Decoding Overview</figcaption>\n",
    "</figure>\n",
    "\n",
    "To elaborate, the process begins with the draft model generating a\n",
    "series of $\\gamma$ tokens, denoted as $x_1, x_2, ..., x_{\\gamma}$.\n",
    "Subsequently, it preserves the distributions\n",
    "$q_{1}(x), q_{2}(x), ..., q_{\\gamma}(x)$ of these tokens for future\n",
    "verification by the target model. These $\\gamma$ tokens are then\n",
    "inputted into the target model in parallel to calculate the logits for\n",
    "the respective token combinations\n",
    "$p_{1}(x), p_{2}(x), ..., p_{\\gamma+1}(x)$, derived from\n",
    "$M_{\\text{target}}(\\text{prefix} + [x_1 + ... + x_{\\gamma}])$. If the\n",
    "condition $q(x) < p(x)$ is met, the token is retained. In contrast, if\n",
    "not met, the token faces a rejection chance of $1 - \\frac{p(x)}{q(x)}$,\n",
    "following which it is reselected from an adjusted distribution:\n",
    "\n",
    "$$p'(x) = norm(max(0, p(x) - q(x)))$$ \n",
    ":eqlabel:`equ:sd_adjusted` \n",
    "\n",
    "In the paper [@leviathan2023fast],\n",
    "Leviathan et al. have proved the correctness of this adjusted\n",
    "distribution for resampling.\n",
    "\n",
    "Under the assumption that the execution time for a single step of the\n",
    "Target model is denoted as $T$, and that of the draft model as $cT$,\n",
    "where $0<c\\leq1$. The standard procedure using the target model to\n",
    "generate $\\gamma + 1$ tokens would require a total time of\n",
    "$\\gamma T + T$. In contrast, with speculative decoding, where\n",
    "$\\gamma + 1$ tokens are produced ($\\gamma$ by the draft model and one\n",
    "additional by the target model concurrently during the parallel\n",
    "verification), the time required would be $\\gamma cT + T$. If all\n",
    "$\\gamma$ draft tokens are accepted by the target model and $c$ is small\n",
    "enough to make $cT << T$, speculative decoding has the potential to\n",
    "significantly reduce latency during the decoding process.\n",
    "\n",
    "To further explain, if we denote $\\alpha = E(\\beta)$ where $\\beta$ is\n",
    "the acceptance rate with a given prefix and $E(\\beta)$ is a natural\n",
    "measure of how well the draft model can approximate the target model\n",
    "assuming $\\beta$s are i.i.d., the expected number of tokens generated by\n",
    "the speculative process is $\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$\n",
    "[@leviathan2023fast]. According to the speculative decoding time for one\n",
    "superstep $\\gamma cT + T$, the expected time for generating one token\n",
    "with speculative decoding is\n",
    "$\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T$. By choosing a good\n",
    "$\\gamma$ and a well-aligned efficient draft model meaning big $\\alpha$\n",
    "and small $c$, the result is desired.\n",
    "\n",
    "Nevertheless, as the value of $\\gamma$ continues to rise, it becomes\n",
    "progressively more difficult for a draft model to generate draft tokens\n",
    "with a high acceptance rate by the target model, especially as the\n",
    "likelihood of acceptance typically diminishes when $\\gamma$ exceeds a\n",
    "certain value. In the worst-case scenario, if all draft tokens generated\n",
    "by the draft model are rejected by the target model, then only the one\n",
    "token that is resampled from the adjusted distribution will be decoded\n",
    "following the speculative process. In this situation, the time spent on\n",
    "generating $\\gamma$ tokens with the draft model represented as\n",
    "$\\gamma cT$ effectively becomes a complete waste of time when compared\n",
    "to generating a single token directly with the target model; in\n",
    "addition, the draft model is consuming the GPU memory.\n",
    "\n",
    "Therefore, finding the best $\\gamma$ or having a well-designed draft\n",
    "model that is effectively accepted by the target model is of importance.\n",
    "There are some strategies that can be employed to address this issue\n",
    "effectively. For example:\n",
    "\n",
    "**Self-Derived Drafts from Target Models**\n",
    "\n",
    "Is it possible to utilize the target model directly as the draft model,\n",
    "rather than employing a separate smaller model, which could lead to\n",
    "increased GPU memory usage? The answer is yes. Similar to the original\n",
    "approach, the modification involves switching the draft model into the\n",
    "target model itself, followed by self-verifying these draft tokens. The\n",
    "advantages of this method are:\n",
    "\n",
    "1.  Since the draft model is almost the same as the target model, it is\n",
    "    sufficiently robust to maintain a stable acceptance rate.\n",
    "\n",
    "2.  Only need to keep one model in the GPU memory.\n",
    "\n",
    "The challenge now lies in the ability to generate multiple future tokens\n",
    "in a single decoding step. To achieve this, the concept involves\n",
    "appending additional concurrent layers to the existing output layer of\n",
    "the model. Stern et al. first proposed this method in\n",
    "[@stern2018blockwise].\n",
    "\n",
    "The training of these extra layers can either start from scratch with\n",
    "the target model or involve fine-tuning a pre-trained model. This\n",
    "approach forms the basis of the Medusa [@medusa]. Medusa's architecture\n",
    "includes extra \\\"Medusa heads\\\" attached after the last hidden layer.\n",
    "This design enables the model to generate a range of token candidates in\n",
    "just one decoding step. Subsequently, these candidates undergo a\n",
    "self-verification process, and only the accepted tokens are executed.\n",
    "\n",
    "Other methodologies, such as implementing Knowledge Distillation between\n",
    "draft and target models, employing multiple draft models instead of just\n",
    "one, and replacing draft models with retrieval datasets proposed by\n",
    "researchers are still being investigated to determine their\n",
    "effectiveness and reliability.\n",
    "\n",
    "Speculative decoding is an effective technique that uses smaller models\n",
    "to reduce the overhead caused by larger models. By developing a\n",
    "well-trained and aligned draft model, the efficiency of the decoding\n",
    "process can be significantly improved.\n",
    "\n",
    "## FlashAttention\n",
    "\n",
    "FlashAttention is an advanced optimization technique utilizing the\n",
    "memory hierarchy aimed at enhancing the efficiency of attention\n",
    "computations in transformer models in terms of memory usage and speed.\n",
    "\n",
    "Dao et al. were the first to suggest this approach, as indicated in\n",
    "[@dao2022flashattention]. They noted the absence of *IO-awareness* --\n",
    "the consideration of I/O interactions across GPU memory layers -- in the\n",
    "classic Scaled Dot-Product Attention algorithm. To address this, they\n",
    "introduced FlashAttention, an enhanced version of the attention\n",
    "algorithm designed to minimize the intensive access to the GPU's high\n",
    "bandwidth memory (HBM). This innovation led to significant gains in both\n",
    "computational speed and throughput.\n",
    "\n",
    "Figure :numref:`ch-deploy/memory` shows the memory hierarchy with\n",
    "corresponding bandwidths. The main goal of FlashAttention is to avoid\n",
    "reading and writing the large attention matrix to and from HBM. And\n",
    "perform computation in SRAM as much as possible.\n",
    "\n",
    "The standard Scaled Dot-Product Attention [@attention] formula is\n",
    "\n",
    "$$\\textbf{A} = Softmax(\\frac{\\textbf{QK}^T}{\\sqrt{d_k}})\\textbf{V}$$ \n",
    ":eqlabel:`equ:std_attn`\n",
    "\n",
    "As $d_k$ is a scalar, we can simplify it into three parts:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\textbf{S} = \\textbf{QK}^T\\\\\n",
    "    \\textbf{P} = Softmax(\\textbf{S})\\\\\n",
    "    \\textbf{O} = \\textbf{PV}\n",
    "\\end{aligned}$$ \n",
    ":eqlabel:`equ:attn_sep`\n",
    "\n",
    "The matrices **K**, **Q**, **V** are all stored in HBM. The standard\n",
    "implementation of attention follows these steps:\n",
    "\n",
    "1.  Load **K, Q** from HBM, compute **$S$ = $QK^T$**, and write **S** to\n",
    "    the HBM.\n",
    "\n",
    "2.  Read **S** from HBM, compute **P** = $Softmax$(**S**), and write\n",
    "    **P** to HBM.\n",
    "\n",
    "3.  Load **P** and **V** from HBM, compute **O** = **PV**, and write\n",
    "    **O** to HBM. Finally, return **O**.\n",
    "\n",
    "The standard implementation of attention involves frequent I/O\n",
    "interactions with HBM for large matrices reads/writes, leading to\n",
    "reduced speed due to the intensive memory access requirements. Moreover,\n",
    "it stores large intermediate matrices in HBM for backward propagation.\n",
    "\n",
    "To handle such issues, FlashAttention divides the input components **Q,\n",
    "K**, and **V** into blocks. These blocks are then transferred from\n",
    "slower HBM to faster SRAM. Once in SRAM, the attention output is\n",
    "computed with respect to these blocks. Two strategies involved are\n",
    "called **tiling** and **recomputation**.\n",
    "\n",
    "**Tiling**: Assuming a vector $x\\in \\mathbb{R}^D$, the basic Softmax can\n",
    "be calculated as: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m(x) = \\max\\limits_{i} x_i\\\\\n",
    "l_{1}(x) = [e^{x_{1} - m(x)},\\, ...\\,,e^{x_{D} - m(x)}]\\\\\n",
    "s_{1}(x) = \\sum_{i} l_{1}(x)_i\\\\\n",
    "Softmax(x) = \\frac{l_{1}(x)}{s_{1}(x)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Attention can be computed by blocks, so large Softmax can be decomposed\n",
    "into separated parts. To elaborate, assuming a vector $x \\in\\mathbb{R}^{2D}$: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x = [x_{1}, \\,x_{2}], \\quad x_{1}, \\, x_{2} \\in\\mathbb{R}^D\\\\\n",
    "m(x) = \\max(m(x_{1}), \\,m(x_{2}))\\\\\n",
    "l(x) = [e^{m(x_{1})-m_(x)}l_{1}(x_1),\\, ... \\, ,e^{m(x_2)-m(x)}l_{1}(x_2)]\\\\\n",
    "s(x) = e^{m(x_{1})-m(x)}s_{1}(x_1) + e^{m(x_2)-m(x)}s_{1}(x_2)\\\\\n",
    "Softmax(x) = \\frac{l(x)}{s(x)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Figure :numref:`ch-deploy/flashattn` shows a brief overview of\n",
    "FlashAttention with two blocks. Following decomposition, Softmax\n",
    "calculations can be executed block by block. Therefore, **K, Q** and\n",
    "**V** are initially divided into blocks. Subsequently, compute the\n",
    "Softmax values together with the respective $s(x)$ and $m(x)$.\n",
    "Ultimately, aggregate **O** blocks, the outcomes of the block-wise\n",
    "Softmax values with the multiplication of corresponding **V** block\n",
    "vectors. To enhance the efficiency of these steps, it's necessary to\n",
    "load all the required matrix blocks from the HBM to the on-chip SRAM for\n",
    "the current step's computation. All the calculations take place on-chip,\n",
    "that is, within the SRAM. To ensure that all required blocks are\n",
    "sufficiently proper to fit within the on-chip SRAM, which has a capacity\n",
    "of 20MB, careful consideration must be given to setting the size of\n",
    "these blocks. For **K, Q** and **V** $\\in\\mathbb{R}^{N \\times d}$, the\n",
    "block size is set to $\\lfloor \\frac{M}{4d} \\rfloor$ where M is the SRAM\n",
    "size and the output block size is set to be\n",
    "$min(\\lfloor \\frac{M}{4d} \\rfloor, d)$ [@dao2022flashattention].\n",
    "Post-computation of each block, the resulting output block along with\n",
    "the corresponding $s(x)$ and $m(x)$ are transferred back to the HBM.\n",
    "These blocks are sufficiently small for reads/writes to avoid causing\n",
    "significant latency; in addition, all related computations are\n",
    "implemented in one CUDA kernel using **kernel fusion**. This avoids\n",
    "repeatedly reading and writing from and to HBM.\n",
    "\n",
    "<figure id=\"fig:ch-deploy/memory\">\n",
    "<div class=\"center\">\n",
    "<img src=\"../img/ch08/Memory hierarchy.png\"\n",
    "style=\"width:80.0%\" />\n",
    "</div>\n",
    "<figcaption>Memory Hierarchy Overview</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure id=\"fig:ch-deploy/flashattn\">\n",
    "<div class=\"center\">\n",
    "<img src=\"../img/ch08/flashattn.png\" style=\"width:80.0%\" />\n",
    "</div>\n",
    "<figcaption>FlashAttention Overview with Two Blocks</figcaption>\n",
    "</figure>\n",
    "\n",
    "**Recomputation**:\n",
    "\n",
    "Standard attention requires $O(N^2)$ memory to store intermediate\n",
    "matrices **S** and **P** for gradient computation w.r.t. **Q, K, V** in\n",
    "the backward pass. For FlashAttention, **S** and **P** can be recomputed\n",
    "with the HBM-stored $s(x)$, $m(x)$ and **O** in SRAM easily. Therefore,\n",
    "only $O(N)$ memory is required. Furthermore, FlashAttention has fewer\n",
    "HBM accesses than Standard Attention which results in faster runtime\n",
    "[@dao2022flashattention].\n",
    "\n",
    "The standard FlashAttention implementation doesn't eliminate the\n",
    "redundant computation of zero elements within the attention mechanism.\n",
    "To address this, a mask is incorporated in FlashAttention to focus\n",
    "computation exclusively on non-zero elements. Termed as Block-Sparse\n",
    "FlashAttention, this approach is also discussed in\n",
    "[@dao2022flashattention]. By using sparsity, Block-Sparse FlashAttention\n",
    "effectively reduces the larger component of the I/O complexity, leading\n",
    "to a direct improvement in performance.\n",
    "\n",
    "However, FlashAttention has not been fully optimized. Dao noted that its\n",
    "inefficiency stems from suboptimal work distribution among various\n",
    "thread blocks and warps on the GPU. This leads to either low occupancy\n",
    "or unnecessary shared memory reads and writes. Thus, Dao proposed\n",
    "**FlashAttention-2** [@dao2023flashattention2] which has better\n",
    "parallelism and work partitioning.\n",
    "\n",
    "FlashAttention-2 includes several tweaks to reduce the non-matmul\n",
    "operations.\n",
    "\n",
    "1.  Remain output **O** blocks un-scaled until the very end of the loop.\n",
    "\n",
    "2.  Instead of saving both $s(x)$ and $m(x)$ in HBM, save\n",
    "    $logsumexp_{i} = m_{i} + log(s_{i})$ which is enough for backward\n",
    "    pass.\n",
    "\n",
    "3.  For blocks where column indices are greater than row indices, which\n",
    "    occupy about half of the blocks in large sequences, computation is\n",
    "    skipped. It leads to a 1.7-1.8X speedup compared to those without\n",
    "    this skip.\n",
    "\n",
    "4.  Only use the row-wise $logsumexp$ instead of both the row-wise max\n",
    "    $m(x)$ and row-wise sum $s(x)$ of exponentials in the softmax.\n",
    "\n",
    "For parallelism, In the original version of FlashAttention, parallel\n",
    "processing was done over the batch size and number of heads, with one\n",
    "thread block processing one attention head. There are as many thread\n",
    "blocks as the product of the batch size and the number of heads. This\n",
    "works well on an A100 GPU, which has 108 Streaming Multiprocessors\n",
    "(SMs), as long as the number of thread blocks is large enough to engage\n",
    "most of the SMs, like 80 or more.\n",
    "\n",
    "However, for long sequences, this isn't as efficient because of the\n",
    "smaller number of thread blocks. FlashAttention-2 introduces additional\n",
    "parallelization over the sequence length dimension, which significantly\n",
    "speeds up the process in these cases by improving GPU occupancy, i.e.\n",
    "the fraction of GPU resources being used.\n",
    "\n",
    "In the forward pass, the method schedules different parts of the\n",
    "sequence length on different thread blocks that operate independently.\n",
    "The backward pass also incorporates parallelization over the sequence\n",
    "length. To update the gradients of the query matrix **dQ**, it uses\n",
    "atomic additions to synchronize updates between different thread blocks.\n",
    "\n",
    "Within each thread block, work partitioning for each wrap is also of\n",
    "importance. Usually, 4 to 8 warps are allocated to each thread block. To\n",
    "handle this condition, FlashAttention-2 introduces significant\n",
    "improvements in both the forward and backward passes of the algorithm.\n",
    "In the forward pass, unlike FlashAttention which splits **K** and **V**\n",
    "across 4 warps (the \\\"split-K\\\" scheme) leading to inefficient shared\n",
    "memory operations, FlashAttention-2 splits **Q** across the warps while\n",
    "keeping **K** and **V** accessible to all. This change eliminates the\n",
    "need for inter-warp communication and reduces shared memory\n",
    "reads/writes, resulting in a faster runtime. Each warp directly\n",
    "multiplies its slice of **Q** with **K** and then with **V**,\n",
    "simplifying the computation of the output slice. In the backward pass,\n",
    "FlashAttention-2 continues to avoid the \\\"split-K\\\" scheme, aligning the\n",
    "warps in a way that minimizes shared memory operations. Despite\n",
    "requiring some synchronization due to complex dependencies among inputs\n",
    "and gradients, this approach still leads to a speedup by reducing the\n",
    "shared memory reads/writes.\n",
    "\n",
    "FlashAttention has gained significant attention in the industry for its\n",
    "remarkable performance, offering accelerated attention computations in\n",
    "both forward and backward passes while also reducing memory I/O\n",
    "complexity. An enhanced version, FlashAttention-2, achieves a notable 2X\n",
    "speedup over the standard FlashAttention [@dao2022flashattention].\n",
    "Moreover, continuous optimization efforts are being made, promising an\n",
    "even more potent version of FlashAttention in the future.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}