{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c9defd",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Model Compression\n",
    ":label:`ch-deploy/model-compression`\n",
    "\n",
    "The previous section briefly described the purpose of model conversion\n",
    "and focused on some common model optimization methods for model\n",
    "deployment. Hardware restrictions differ depending on where models are\n",
    "deployed. For instance, smartphones are more sensitive to the model\n",
    "size, usually supporting models only at the MB level. Larger models need\n",
    "to be compressed using compression techniques before they can be\n",
    "deployed on different computing hardware.\n",
    "\n",
    "## Quantization\n",
    "\n",
    "Model quantization is a technique that approximates floating-point\n",
    "weights of contiguous values (usually float32 or many possibly discrete\n",
    "values) at the cost of slightly reducing accuracy to a limited number of\n",
    "discrete values (usually int8). As shown in Figure\n",
    ":numref:`ch-deploy/quant-minmax`, $T$ represents the data range\n",
    "before quantization. In order to reduce the model size, model\n",
    "quantization represents floating-point data with fewer bits. As such,\n",
    "the memory usage during inference can be reduced, and the inference on\n",
    "processors that are good at processing low-precision operations can be\n",
    "accelerated.\n",
    "\n",
    "![Principles ofquantization](../img/ch08/ch09-quant-minmax.png)\n",
    ":label:`ch-deploy/quant-minmax`\n",
    "\n",
    "The number of bits and the range of data represented by different data\n",
    "types in a computer are different. Based on service requirements, a\n",
    "model may be quantized into models with different number of bits based\n",
    "on service requirements. Generally, single-precision floating-point\n",
    "numbers are used to represent a deep neural network. If signed integers\n",
    "can be used to approximate parameters, the size of the quantized weight\n",
    "parameters may be reduced to a quarter of the original size. Using fewer\n",
    "bits to quantize a model results in a higher compression rate --- 8-bit\n",
    "quantization is the mostly used in the industry. The lower limit is\n",
    "1-bit quantization, which can compress a model to 1/32 of its original\n",
    "size. During inference, efficient XNOR and BitCount bit-wise operations\n",
    "can be used to accelerate the inference.\n",
    "\n",
    "According to the uniformity of the original ranges represented by the\n",
    "quantized data, quantization can be further divided into linear\n",
    "quantization and non-linear quantization. Because the weights and\n",
    "activations of a deep neural network are usually not uniform in\n",
    "practice, non-linear quantization can theoretically achieve a smaller\n",
    "loss of accuracy. In real-world inference, however, non-linear\n",
    "quantization typically involves higher computation complexity, meaning\n",
    "that linear quantization is more commonly used. The following therefore\n",
    "focuses on the principles of linear quantization.\n",
    "\n",
    "In Equation\n",
    ":eqref:`ch-deploy/quantization-q`, assume that $r$ represents\n",
    "the floating-point number before quantization. We are then able to\n",
    "obtain the integer $q$ after quantization.\n",
    "\n",
    "$$q=clip(round(\\frac{r}{s}+z),q_{min},q_{max})$$ \n",
    ":eqlabel:`ch-deploy/quantization-q`\n",
    "\n",
    "$clip(\\cdot)$ and $round(\\cdot)$ indicate the truncation and rounding\n",
    "operations, and $q_{min}$ and $q_{max}$ indicate the minimum and maximum\n",
    "values after quantization, respectively. $s$ is the quantization\n",
    "interval, and $z$ is the bias representing the data offset. The\n",
    "quantization is symmetric if the bias ($z$) used in the quantization is\n",
    "0, or asymmetric in other cases. Symmetric quantization reduces the\n",
    "computation complexity during inference because it avoids computation\n",
    "related to $z$. In contrast, asymmetric quantization determines the\n",
    "minimum and maximum values based on the actual data distribution, and\n",
    "the information about the quantized data is more effectively used. As\n",
    "such, asymmetric quantization reduces the loss of accuracy caused by\n",
    "quantization.\n",
    "\n",
    "According to the shared range of quantization parameters $s$ and $z$,\n",
    "quantization methods may be classified into layer-wise quantization and\n",
    "channel-wise quantization. In the former, separate quantization\n",
    "parameters are defined for each layer. Whereas the latter involves\n",
    "defining separate quantization parameters for each channel.\n",
    "Finer-grained channel-wise quantization yields higher quantization\n",
    "precision, but increases the computation complexity.\n",
    "\n",
    "Model quantization can also be classified into quantization aware\n",
    "training (QAT) and post-training quantization (PTQ) based on whether\n",
    "training is involved. In QAT, fake-quantization operators are added, and\n",
    "statistics on the input and output ranges before and after quantization\n",
    "are collected during training to improve the accuracy of the quantized\n",
    "model. This method is therefore suitable for scenarios that place strict\n",
    "requirements on accuracy. In PTQ, models are directly quantized after\n",
    "training, requiring only a small amount of calibration data. This method\n",
    "is therefore suitable for scenarios that place strict requirements on\n",
    "usability and have limited training resources.\n",
    "\n",
    "**1. Quantization aware training**\n",
    "\n",
    "QAT simulates quantization during training by including the accuracy\n",
    "loss introduced by fake-quantization operators. In this way, the\n",
    "optimizer can minimize the quantization error during training, leading\n",
    "to higher model accuracy. QAT involves the following steps:\n",
    "\n",
    "1.  Initialization: Set initial values for the $q_{min}$/$q_{max}$\n",
    "    ranges of weights and activations.\n",
    "\n",
    "2.  Building a network for simulated quantization: Insert\n",
    "    fake-quantization operators after weights and activations that\n",
    "    require quantization.\n",
    "\n",
    "3.  Running QAT: Compute the range (i.e., $q_{min}$ and $q_{max}$) for\n",
    "    each weight and activation of the quantized network layer. Then,\n",
    "    perform forward computation with the quantization loss considered,\n",
    "    so that the loss can be involved in subsequent backpropagation and\n",
    "    network parameter update.\n",
    "\n",
    "4.  Exporting the quantized network: Obtain $q_{min}$ and $q_{max}$, and\n",
    "    compute the quantization parameters $s$ and $z$. Substitute the\n",
    "    quantization parameters into the quantized formula to transform the\n",
    "    network weights into quantized integer values. Then, delete the\n",
    "    fake-quantization operators, and add quantization and dequantization\n",
    "    operators before and after the quantization network layer,\n",
    "    respectively.\n",
    "\n",
    "**2. Post-training quantization**\n",
    "\n",
    "PTQ can be divided into two types: weight quantization and full\n",
    "quantization. Weight quantization quantizes only the weights of a model\n",
    "to compress its size, and then the weights are dequantized to the\n",
    "original float32 format during inference. The subsequent inference\n",
    "process is the same as that of a common float32 model. The advantage of\n",
    "weight quantization is that calibration dataset and quantized operators\n",
    "are not required, and that the accuracy loss is small. However, it does\n",
    "not improve the inference performance, because the operators used during\n",
    "inference are still float32. Full quantization quantizes both the\n",
    "weights and activations of a model, and the quantized operators are\n",
    "executed to accelerate model inference. The quantization of activations\n",
    "requires a small number of calibration datasets (training dataset or\n",
    "inputs of real scenarios) to collect the distribution of the activations\n",
    "at each layer and calibrate the quantized operators. Calibration\n",
    "datasets are used as the input during the quantization of activations.\n",
    "After the inference, the distribution of activations at each layer is\n",
    "collected to obtain quantization parameters. The process is summarized\n",
    "as follows:\n",
    "\n",
    "1.  Use a histogram to represent the distribution $P_f$ of the original\n",
    "    float32 data.\n",
    "\n",
    "2.  Select several $q_{min}$ and $q_{max}$ values from a given search\n",
    "    space, quantize the activations, and obtain the quantized data\n",
    "    $Q_q$.\n",
    "\n",
    "3.  Use a histogram to represent the distribution of $Q_q$.\n",
    "\n",
    "4.  Compute the distribution difference between $Q_q$ and $P_f$, and\n",
    "    find the $q_{min}$ and $q_{max}$ values corresponding to the\n",
    "    smallest difference between $Q_q$ and $P_f$ in order to compute the\n",
    "    quantization parameters. Common indicators used to measure the\n",
    "    distribution differences include symmetric Kullback-Leibler\n",
    "    divergence and Jenson-Shannon divergence.\n",
    "\n",
    "In addition, the inherent error of quantization requires calibration\n",
    "during quantization. Take the matrix multiplication\n",
    "$a=\\sum_{i=1}^Nw_ix_i+b$ as an example. $w$ denotes the weight, $x$ the\n",
    "activation, and $b$ the bias. To overcome the quantization error, we\n",
    "first calibrate the quantized mean value, and then obtain the mean value\n",
    "of each channel output by the float32 operator and the quantized\n",
    "operator. Assume that the mean value output by the float32 operator of\n",
    "channel $i$ is $a_i$, and that output by the quantized operator after\n",
    "dequantization is $a_{qi}$. From this, we can obtain the final mean\n",
    "value by adding the mean value difference $a_i-a_q$ of the two channels\n",
    "to the corresponding channel. In this manner, the final mean value is\n",
    "consistent with that output by the float32 operator. We also need to\n",
    "ensure that the distribution after quantization is the same as that\n",
    "before quantization. Assume that the mean value and variance of the\n",
    "weight of a channel are $E(w_c)$ and $||w_c-E(w_c)||$, and the mean\n",
    "value and variance after quantization are $E(\\hat{w_c})$ and\n",
    "$||\\hat{w_c}-E(\\hat{w_c})||$, respectively. Equation\n",
    ":eqref:`ch-deploy/post-quantization` is the calibration of the\n",
    "weight:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{w_c}\\leftarrow\\zeta_c(\\hat{w_c}+u_c) \\\\\n",
    "u_c=E(w_c)-E(\\hat{w_c})   \\\\\n",
    "\\zeta_c=\\frac{||w_c-E(w_c)||}{||\\hat{w_c}-E(\\hat{w_c})||}\n",
    "\\end{aligned}\n",
    "$$ \n",
    ":eqlabel:`ch-deploy/post-quantization`\n",
    "\n",
    "As a general model compression method, quantization can significantly\n",
    "improve the memory and compression efficiency of neural networks, and\n",
    "has been widely used.\n",
    "\n",
    "## Model Sparsification\n",
    "\n",
    "Model sparsification reduces the memory and computation overheads by\n",
    "removing some components (such as weights, feature maps, and convolution\n",
    "kernels) from a neural network. It is a type of strong inductive bias\n",
    "introduced to reduce the computation complexity of the model, just like\n",
    "weight quantization, weight sharing, and pooling.\n",
    "\n",
    "**1. Motivation of model sparsification**\n",
    "\n",
    "Convolution on a convolutional neural network can be considered as a\n",
    "weighted linear combination of the input and the weights of the\n",
    "convolution kernel. In this sense, tiny weights have a relatively small\n",
    "impact on the output. Model sparsification can be justified based on two\n",
    "assumptions:\n",
    "\n",
    "1.  Most neural network models have over-parameterized weights. The\n",
    "    number of weight parameters can reach tens or even hundreds of\n",
    "    millions.\n",
    "\n",
    "2.  For most computer vision tasks such as detection, classification,\n",
    "    and segmentation, useful information accounts for only a small\n",
    "    proportion in an activation feature map generated during inference.\n",
    "\n",
    "As such, model sparsification can be classified into two types according\n",
    "to the source of sparsity: weight sparsification and activation\n",
    "sparsification. Both types reduce the computation workload and model\n",
    "storage requirements by reducing redundant components in a model. In\n",
    "model sparsification, some weak connections are pruned based on the\n",
    "absolute value of weights or activations (i.e. the weight or activation\n",
    "of such connections is set to 0), with the goal of improving the model\n",
    "performance. The sparsity of a model is measured by the proportion of\n",
    "zero-value weights or activation tensors. Because the accuracy of a\n",
    "model typically decreases as its sparsity increases, we hope to minimize\n",
    "such loss when increasing the sparsity.\n",
    "\n",
    "Neurobiology was the inspiration for inventing neural networks --- it\n",
    "has also inspired the sparsification of neural network models.\n",
    "Neurobiologists found that most mammalian brains, including humans, have\n",
    "a process called synapse pruning, which occurs between infancy and\n",
    "adulthood. During synapse pruning, neuron axons and dendrites decay and\n",
    "die off, and the neuron connections are continuously simplified and\n",
    "reconstructed. This process allows brains to work more efficiently and\n",
    "consume less energy.\n",
    "\n",
    "**2. Structured and unstructured sparsification**\n",
    "\n",
    "Let's first look at weight sparsification. It can be classified into\n",
    "structured and unstructured sparsification. Structured sparsification\n",
    "involves pruning channels or convolution kernels in order to generate\n",
    "regular and smaller weight matrices that are more likely to obtain\n",
    "speedup on CPUs and GPUs. However, this mode is coarse-grained, meaning\n",
    "that it severely reduces the model accuracy.\n",
    "\n",
    "In contrast, unstructured sparsification allows a weight at any location\n",
    "to be pruned, meaning it is a fine-grained mode that causes less loss to\n",
    "the model accuracy. However, the unstructured mode limits the speedup of\n",
    "sparse models on hardware for a number of reasons:\n",
    "\n",
    "1.  The irregular layout of weights requires many control flow\n",
    "    instructions. For instance, the presence of zero values introduces\n",
    "    many `if-else` instructions for decision-making, which inevitably\n",
    "    reduces instruction-level parallelism.\n",
    "\n",
    "2.  The computation of convolution kernels is typically multi-threaded.\n",
    "    However, the irregular layout of weight matrices on memory causes\n",
    "    thread divergence and load imbalance, which therefore affects\n",
    "    thread-level parallelism.\n",
    "\n",
    "3.  The irregular layout of weight matrices on the memory hinders data\n",
    "    locality and reduces the cache hit rate. Consequently, the\n",
    "    load/store efficiency is reduced.\n",
    "\n",
    "In an attempt to solve these problems, recent work combines structured\n",
    "sparsification with unstructured sparsification. This approach\n",
    "incorporates the advantages of both modes, and overcomes their\n",
    "disadvantages to an extent.\n",
    "\n",
    "**3. Sparsification strategies**\n",
    "\n",
    "Given a neural network model, after deciding to sparsify the weights or\n",
    "activations, we need to determine when and how to perform the\n",
    "sparsification. The most common sparsification process is currently\n",
    "pre-training, pruning, and fine-tuning. With this process, we need to\n",
    "sparsify and fine-tune a converged dense model obtained through\n",
    "training. Given the fact that a pre-trained model contains knowledge it\n",
    "has learned, sparsification on such models will achieve a better effect\n",
    "than directly on the initial model. In addition to pruning the\n",
    "pre-trained model, we usually interleave pruning with network training.\n",
    "Compared with one-shot pruning, iterative pruning is integrated more\n",
    "closely with training, so that redundant convolution kernels can be\n",
    "identified more efficiently. As such, iterative pruning is widely used.\n",
    "\n",
    "To illustrate how to prune a network, we will use Deep\n",
    "Compression [@han2015deep] as an example. Removing most weights leads to\n",
    "a loss of accuracy of the neural network, as shown in Figure\n",
    ":numref:`ch-deploy/deepcomp`. Fine-tuning a pruned sparse neural\n",
    "network can help improve model accuracy, and the pruned network may be\n",
    "quantized to represent weights using fewer bits. In addition, using\n",
    "Huffman coding can further reduce the memory cost of the deep neural\n",
    "network.\n",
    "\n",
    "![Deep Compressionalgorithm](../img/ch08/ch09-deepcomp.png)\n",
    ":label:`ch-deploy/deepcomp`\n",
    "\n",
    "In addition to removing redundant neurons, a dictionary learning-based\n",
    "method can be used to remove unnecessary weights on a deep convolutional\n",
    "neural network. By learning the bases of convolution kernels, the\n",
    "original convolutional kernels can be transformed into the coefficient\n",
    "domain for sparsification. An example of this approach is the work by\n",
    "Bagherinezhad et al. [@bagherinezhad2017lcnn], in which they proposed\n",
    "that the original convolution kernel can be decomposed into a weighted\n",
    "linear combination of the base of the convolution kernel and sparse\n",
    "coefficient.\n",
    "\n",
    "## Knowledge Distillation\n",
    "\n",
    "Knowledge distillation (KD), also known as the teacher-student learning\n",
    "algorithm, has gained much attention in the industry. Large deep\n",
    "networks tend to deliver good performance in practice, because\n",
    "over-parameterization increases the generalization capability when it\n",
    "comes to new data. In KD, a large pre-trained network serves as the\n",
    "teacher, a deep and thin brand-new neural network serves as the student,\n",
    "supervised by the teacher network. The key to this learning algorithm is\n",
    "how to transfer knowledge converted by the teacher to the student.\n",
    "\n",
    "Hinton et al. [@Distill] first proposed a teacher-student learning\n",
    "framework. It is used for the learning of deep and thin neural networks\n",
    "by minimizing the differences between the teacher and student neural\n",
    "networks. The teacher network is denoted as $\\mathcal{N}_{T}$ with\n",
    "parameters $\\theta_T$, and the student network is denoted as\n",
    "$\\mathcal{N}_{S}$ with parameters $\\theta_S$. In general, the student\n",
    "network has fewer parameters than the teacher network.\n",
    "\n",
    "[@Distill] proposed KD, which makes the classification result of the\n",
    "student network more closely resembles the ground truth as well as the\n",
    "classification result of the teacher network, that is, Equation :eqref:`c2Fcn:distill`.\n",
    "\n",
    "$$\\mathcal{L}_{KD}(\\theta_S) = \\mathcal{H}(o_S,\\mathbf{y}) +\\lambda\\mathcal{H}(\\tau(o_S),\\tau(o_T)),$$ \n",
    ":eqlabel:`c2Fcn:distill`\n",
    "\n",
    "where $\\mathcal{H}(\\cdot,\\cdot)$ is the cross-entropy function, $o_S$\n",
    "and $o_T$ are outputs of the student network and the teacher network,\n",
    "respectively, and $\\mathbf{y}$ is the label. The first item in\n",
    "Equation :eqref:`c2Fcn:distill` makes the classification result of the\n",
    "student network resemble the expected ground truth, and the second item\n",
    "aims to extract useful information from the teacher network and transfer\n",
    "the information to the student network, $\\lambda$ is a weight parameter\n",
    "used to balance two objective functions, and $\\tau(\\cdot)$ is a soften\n",
    "function that smooths the network output.\n",
    "\n",
    "Equation :eqref:`c2Fcn:distill` only extracts useful information from the\n",
    "output of the teacher network classifier --- it does not mine\n",
    "information from other intermediate layers of the teacher network.\n",
    "Romero et al. [@FitNet] proposed an algorithm for transferring useful\n",
    "information from any layer of a teacher network to a small student\n",
    "network. Note that not all inputs are useful for convolutional neural\n",
    "network computing and subsequent task execution. For example, in an\n",
    "image containing an animal, it is important to classify and identify the\n",
    "region where the animal is rather than the background information.\n",
    "Therefore, it is an efficient way to select useful information from the\n",
    "teacher network. Zagoruyko and Komodakis [@attentionTS] proposed a\n",
    "learning method based on an attention loss function to improve the\n",
    "performance of the student network. This method introduces an attention\n",
    "module. The attention module generates an attention map, which\n",
    "identifies the importance of different areas of an input image to the\n",
    "classification result. The attention map is then transferred from the\n",
    "teacher network to the student network, as depicted in Figure\n",
    " :numref:`ch-deploy/attentionTS`.\n",
    "\n",
    "KD is an effective method to optimize small networks. It can be combined\n",
    "with other compression methods such as pruning and quantization to train\n",
    "efficient models with higher accuracy and less computation workload.\n",
    "\n",
    "<figure id=\"fig:ch-deploy/attentionTS\">\n",
    "<div class=\"center\">\n",
    "<img src=\"../img/ch08/distillation.png\" style=\"width:80.0%\" />\n",
    "</div>\n",
    "<figcaption>Teacher-student neural network learning\n",
    "algorithm</figcaption>\n",
    "</figure>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}