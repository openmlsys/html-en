{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b383774",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Further Reading\n",
    "\n",
    "1.  A Distributed Graph-Theoretic Framework for Automatic\n",
    "    Parallelization in Multi-Core Systems[^1]\n",
    "\n",
    "2.  SCOP: Scientific Control for Reliable Neural Network Pruning[^2]\n",
    "\n",
    "3.  Searching for Low-Bit Weights in Quantized Neural Networks[^3]\n",
    "\n",
    "4.  GhostNet: More Features from Cheap Operations[^4]\n",
    "\n",
    "5.  AdderNet: Do We Really Need Multiplications in Deep Learning?[^5]\n",
    "\n",
    "6.  Blockwise Parallel Decoding for Deep Autoregressive Models[^6]\n",
    "\n",
    "7.  Medusa: Simple framework for accelerating LLM generation with\n",
    "    multiple decoding heads[^7]\n",
    "\n",
    "8.  FlashAttention-2: Faster Attention with Better Parallelism and Work\n",
    "    Partitioning[^8]\n",
    "\n",
    "[^1]: <https://proceedings.mlsys.org/paper/2021/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf>\n",
    "\n",
    "[^2]: <https://arxiv.org/abs/2010.10732>\n",
    "\n",
    "[^3]: <https://arxiv.org/abs/2009.08695>\n",
    "\n",
    "[^4]: <https://arxiv.org/abs/1911.11907>\n",
    "\n",
    "[^5]: <https://arxiv.org/abs/1912.13200>\n",
    "\n",
    "[^6]: <https://arxiv.org/abs/1811.03115>\n",
    "\n",
    "[^7]: <https://www.together.ai/blog/medusa>\n",
    "\n",
    "[^8]: <https://arxiv.org/abs/2307.08691>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}