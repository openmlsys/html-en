{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68e4d3e",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Generating a Computational Graph\n",
    "\n",
    "In the previous section, we explored the ingredients of a computational\n",
    "graph. Now let's proceed to the next question --- how is a computational\n",
    "graph automatically generated? Machine learning frameworks support two\n",
    "approaches to implementing computational graphs: static and dynamic. The\n",
    "static approach builds a static (unchanging) graph based on information\n",
    "such as the network topology and parameter variables described by the\n",
    "frontend language. Because frontend languages are independent, static\n",
    "graphs are especially suitable for model deployment (e.g., deploying a\n",
    "facial recognition application on mobile devices).\n",
    "\n",
    "Unlike the static approach, the dynamic approach dynamically generates a\n",
    "temporary graph based on the frontend description each time the model is\n",
    "executed. Dynamic graphs are easy to debug, making it possible to\n",
    "fine-tune models efficiently on the fly. Major machine learning\n",
    "frameworks such as TensorFlow and MindSpore are compatible with both\n",
    "approaches. And although PyTorch uses dynamic graphs, it also offers\n",
    "dynamic-to-static conversion support for efficient model execution. To\n",
    "choose the right approach for a specific task, we need to consider the\n",
    "task requirements as well as the pros and cons of each approach.\n",
    "\n",
    "## Static Graph\n",
    "\n",
    "The static graph approach decouples the definition and execution\n",
    "processes. That is, a static graph is compiled before it is executed, as\n",
    "shown in Figure :numref:`ch04/ch04-static`.\n",
    "\n",
    "![Generating and executing a staticgraph](../img/ch04/static.png)\n",
    ":label:`ch04/ch04-static`\n",
    "\n",
    "When a model program is generated using the frontend language, the\n",
    "machine learning framework first analyzes the model topology for\n",
    "information such as the connections between network layers, parameter\n",
    "variable settings, and loss functions. The framework then compiles the\n",
    "model description into fixed code (i.e., a static computational graph)\n",
    "that can be invoked and executed by the computing backend. In this case,\n",
    "subsequent training or inference on this model is no longer\n",
    "frontend-dependent. Specifically, when input data is fed into the static\n",
    "graph, the operators in the graph are directly scheduled to hardware for\n",
    "execution. And to improve hardware computational efficiency, we can also\n",
    "convert a static graph into other equivalent structures through various\n",
    "optimization strategies.\n",
    "\n",
    "Code `ch04/code4` shows an example of generating and executing a\n",
    "simple static graph. In the frontend definition phase, some machine\n",
    "learning frameworks require developers to declare predefined\n",
    "configuration items including tensor placeholders, loss functions,\n",
    "optimization functions, network building and runtime environments, and\n",
    "network executors, as well as in-graph control statements using control\n",
    "flow operators. The design of machine learning frameworks has recently\n",
    "been improved to provide easy-to-use APIs and a unified model building\n",
    "paradigm. For example, MindSpore enables unified frontend programming\n",
    "representations featuring dynamic and static integration. To illustrate,\n",
    "let's consider the following simple model.\n",
    "\n",
    "**ch04/code4**\n",
    "\n",
    "```python\n",
    "def model(X, flag):\n",
    "    if flag > 0:     \n",
    "        Y = matmul(W1, X)   \n",
    "    else:     \n",
    "        Y = matmul(W2, X)\n",
    "    Y = Y + b\n",
    "    Y = relu(Y)\n",
    "    return Y\n",
    "```\n",
    "\n",
    "The machine learning framework does not load input data when generating\n",
    "a static graph. Instead, *placeholder* tensors are used to hold places\n",
    "of input data. In the static graph defined in Code\n",
    "`ch04/code4`, we need to create a placeholder for input\n",
    "$\\bf{X}$ in line 1. Because no actual input is fed into the model during\n",
    "static graph generation, the control flow defined in line 2 cannot make\n",
    "control decisions at build time. As such, we need to add the control\n",
    "flow operator and the computational subgraph of each branch to the\n",
    "static graph. When the model receives actual inputs during runtime,\n",
    "different branches are taken (by running the corresponding computational\n",
    "subgraphs) depending on different inputs. However, not all machine\n",
    "learning frameworks are able to compile Python control flows as their\n",
    "static graph equivalents. In order to implement control flows in this\n",
    "case, we can use the control primitives provided by the framework.\n",
    "\n",
    "![Generating a staticgraph](../img/ch04/static_gen.png)\n",
    ":label:`ch04/ch04-static-gen`\n",
    "\n",
    "Static computational graphs offer two distinct advantages. First, they\n",
    "yield better performance with less memory. When building a static graph,\n",
    "the machine learning framework acquires the complete model topology\n",
    "containing global information of the model, which facilitates the\n",
    "formulation of graph optimization strategies (e.g., the operator fusion\n",
    "strategy that fuses two or more operators into a larger one). As shown\n",
    "in Figure :numref:`ch04/ch04-static-gen`, the Add and ReLU operators are\n",
    "fused into one operator to reduce the loads/stores of intermediate\n",
    "results and low-level scheduling overhead, thereby improving the\n",
    "execution performance and efficiency with a lower memory footprint.\n",
    "Static graphs allow for many optimization strategies at build time,\n",
    "which we will discuss in later sections.\n",
    "\n",
    "Second, by converting static graphs into executable code within the\n",
    "machine learning framework, we can directly deploy our models on various\n",
    "hardware platforms to provide efficient inference services. Also, we can\n",
    "store static graphs using serialization techniques for future execution\n",
    "(either model training or inference), eliminating the need to rebuild\n",
    "the frontend source code from scratch every time before execution.\n",
    "\n",
    "Once the frontend code of the model is compiled into a static graph, the\n",
    "graph structure is fixed. If we introduce any optimizations to the\n",
    "graph, the optimized code can differ significantly from the original.\n",
    "However, the optimized code is not intuitively visible, meaning that it\n",
    "is sometimes impossible to locate a runtime error based on the returned\n",
    "code line number in the optimized code. Consider a simple case. Assuming\n",
    "that the Add and ReLU operators in Code\n",
    "`ch04/code4` have been fused for optimization, if a runtime\n",
    "error related to the fused operator is reported, it would be hard for us\n",
    "to determine the exact error location (Add or ReLU).\n",
    "\n",
    "In addition, in the daunting process of model debugging and testing,\n",
    "intermediate results cannot be printed in real time. To make this\n",
    "happen, we need to insert additional code to the source code and then\n",
    "recompile the source code for execution, making debugging less\n",
    "efficient. By contrast, the dynamic graph approach offers more\n",
    "flexibility.\n",
    "\n",
    "## Dynamic Graph\n",
    "\n",
    "Figure :numref:`ch04/ch04-eager1` shows the principle of the dynamic\n",
    "graph approach. A dynamic graph is defined as it runs. The frontend\n",
    "interpreter parses the graph code and the machine learning framework\n",
    "distributes the operators in the graph to the backend for just-in-time\n",
    "(JIT) execution. Adopting the user-friendly imperative programming\n",
    "paradigm, the dynamic graph approach allows developers to create neural\n",
    "network models at the frontend and is therefore favored by a vast number\n",
    "of deep learning researchers.\n",
    "\n",
    "![Dynamic graph principle](../img/ch04/eager.png)\n",
    ":label:`ch04/ch04-eager1`\n",
    "\n",
    "Next, we reuse the pseudocode in the previous section to compare the\n",
    "dynamic and static graph approaches.\n",
    "\n",
    "While these two approaches differ only slightly in their frontend\n",
    "representations, they differ dramatically in terms of their compilation\n",
    "and execution mechanisms. Unlike the static graph approach, the dynamic\n",
    "graph approach calls the built-in operator distribution function of the\n",
    "machine learning framework through the Python API to distribute Python\n",
    "operators to the hardware backend (e.g., CPU, GPU, or NPU) for\n",
    "accelerated computing, which then returns the computational result to\n",
    "the frontend. This process does not generate a static computational\n",
    "graph. Instead, the framework describes the model topology using the\n",
    "frontend language, schedules and executes the model based on\n",
    "computational dependencies, and dynamically generates a temporary graph.\n",
    "\n",
    "Figure :numref:`ch04/ch04-dynamic-gen` shows the process of generating a\n",
    "dynamic graph.\n",
    "\n",
    "![Generating a dynamicgraph](../img/ch04/eager-gen.png)\n",
    ":label:`ch04/ch04-dynamic-gen`\n",
    "\n",
    "Forward computation is run through the neural network in the sequence\n",
    "defined by the model declaration. Once the model receives input\n",
    "$\\bf{X}$, the machine learning framework starts to generate a dynamic\n",
    "graph by adding the input node to the graph and sending the data to the\n",
    "downstream node. The control flow (if available) makes a data flow\n",
    "decision immediately. For example, in Figure\n",
    ":numref:`ch04/ch04-dynamic-gen`, if the conditional returns true,\n",
    "only the Matmul operator node with respect to tensor $\\bf{W1}$ is added\n",
    "to the graph. Then, the machine learning framework inserts the Add and\n",
    "ReLU operator nodes based on the operator sequence and computational\n",
    "dependencies defined in the code. For each newly added operator node,\n",
    "the machine learning framework distributes and executes the operator,\n",
    "returns the computational result, and prepares to pass the result to the\n",
    "next node. When forward computation resumes, the last dynamic graph\n",
    "becomes invalid and a new dynamic graph is created according to current\n",
    "input and control decision. In contrast with a static graph that\n",
    "represents the entire model described in the frontend language, a\n",
    "dynamic graph is generated on the fly as the control flow and data flow\n",
    "evolve over time. For this reason, the machine learning framework has\n",
    "few opportunities to optimize the model in the dynamic graph setting.\n",
    "\n",
    "In the static graph setting, as the model definition is entirely\n",
    "available, a complete forward computational graph and a complete\n",
    "backward computational graph can be constructed simultaneously. However,\n",
    "in the dynamic graph setting, gradients are calculated for\n",
    "backpropagation as the forward pass proceeds. Specifically, the machine\n",
    "learning framework collects information of each backward operator and\n",
    "tensor participating in gradient computation based on the information of\n",
    "each operator called in the forward pass. Once the forward pass ends,\n",
    "the operator and tensor information for backpropagation becomes\n",
    "available. With this information, the machine learning framework creates\n",
    "a backward computational graph and runs it on hardware to complete\n",
    "gradient computation and parameter update.\n",
    "\n",
    "As shown in Figure :numref:`ch04/ch04-dynamic-gen`, when the Matmul operator with\n",
    "respect to tensor $\\bf{W1}$ is called, the framework runs the Matmul\n",
    "operator to calculate the product of inputs $\\bf{X}$ and $\\bf{W1}$, and\n",
    "then records the operator and tensor $\\bf{X}$ that will participate in\n",
    "backpropagation based on the backward computation process\n",
    "Grad\\_$\\bf{W1}$=Grad\\_$\\bf{Y}*\\bf{X}$, thereby completing the forward\n",
    "pass and producing a backward computational graph.\n",
    "\n",
    "Although the optimization techniques useful in the static graph setting\n",
    "do not work for dynamic graphs (because the complete network structure\n",
    "is unknown until the dynamic graph runs), researchers and developers can\n",
    "easily analyze errors and debug results during model testing and\n",
    "optimization. This is made possible by dynamic graphs supporting JIT\n",
    "computing and returning computational results immediately with the\n",
    "execution of each statement.\n",
    "\n",
    "Also, the dynamic graph approach enables flexible execution using native\n",
    "control flows provided by the frontend --- unlike static graphs, which\n",
    "involve complex control flows along with programming and debugging\n",
    "difficulties. Consequently, the dynamic graph approach lowers the\n",
    "barriers to programming for beginners while also improving the iteration\n",
    "efficiency of algorithm development and model optimization.\n",
    "\n",
    "## Dynamic Graph vs. Static Graph\n",
    "\n",
    "The two approaches for implementing computational graphs have their pros\n",
    "and cons, as described in\n",
    "TableÂ :numref:`ch04/ch4-graph`.\n",
    "\n",
    ":Static graph vs. dynamic graph\n",
    "\n",
    "|Feature                          |Static Graph                                     |Dynamic Graph |\n",
    "|---------------------------------|-------------------------------------------------|---------------------------------------------- |\n",
    "|On-the-fly intermediate results  |No                                               |Yes |\n",
    "|Code debugging                   |Difficult                                        |Easy |\n",
    "|Control flow implementation      |Specialized syntax                               |Frontend syntax |\n",
    "|Performance                      |Better, supporting wide optimization strategies  |Poor, supporting limited graph optimizations |\n",
    "|Memory footprint                 |Low                                              |High |\n",
    "|Direct deployment                |Yes                                              |No |\n",
    ":label:`ch04/ch4-graph`\n",
    "\n",
    "Compared with the dynamic graph approach, the static graph approach\n",
    "seems to be less user-friendly to developers because intermediate\n",
    "results are not available on the fly, code debugging is difficult, and\n",
    "implementing control flows is complex. However, static graphs ensure\n",
    "higher execution performance than dynamic graphs. See the example in\n",
    "Code `ch04/code5`.\n",
    "\n",
    "**ch04/code5**\n",
    "\n",
    "```python\n",
    "def model(X1, X2):\n",
    "    Y1 = matmul(X1, W1)\n",
    "    Y2 = matmul(X2, W2)\n",
    "    Y = Y1 + Y2\n",
    "    output = relu(Y)\n",
    "    return output\n",
    "```\n",
    "\n",
    "If the static approach is used to implement Code\n",
    "`ch04/code5`, the machine learning framework creates a\n",
    "complete computational graph. Because tensors $\\bf{Y_1}$ and $\\bf{Y_2}$\n",
    "are computed independently from each other, we can implement automatic\n",
    "parallelism on them in order to improve the computational efficiency.\n",
    "Furthermore, the static approach allows many more optimization\n",
    "strategies to improve efficiency while also lowering memory footprint,\n",
    "for example, fusing operators Add and ReLU to reduce the loads and\n",
    "stores of the intermediate variable $\\bf{Y}$. Conversely, if the dynamic\n",
    "approach is used without a manually configured parallelism strategy, the\n",
    "machine learning framework is unaware of the independence between\n",
    "operators due to the lack of a complete computational graph.\n",
    "Consequently, the framework has to execute the operators, including Add\n",
    "and ReLU, in a defined order and store the intermediate variable\n",
    "$\\bf{Y}$. To further reduce memory footprint, the static approach\n",
    "narrows down the intermediate variables to be stored for backpropagation\n",
    "beforehand in the forward pass, based on the forward and backward\n",
    "computational graphs defined prior to execution. This is not feasible in\n",
    "the dynamic approach, where the backward computational graph is defined\n",
    "only after the forward pass is complete. As such, more intermediate\n",
    "variables have to be stored in the forward pass to ensure the\n",
    "backpropagation efficiency, resulting in higher memory footprint.\n",
    "\n",
    "To choose one approach over the other, we should consider their pros and\n",
    "cons in addition to analyzing specific task requirements. For academic\n",
    "research purposes or in the model design and debugging phases, the\n",
    "dynamic graph approach is suggested because it allows for quick testing\n",
    "of experimental ideas and iterative update of the model structure. In\n",
    "other cases where the model structure is determinant, to accelerate the\n",
    "training process or deploy a model on specific hardware, using the\n",
    "static graph approach offers higher efficiency.\n",
    "\n",
    "## Conversion Between and Combination of Dynamic and Static Graphs\n",
    ":label:`conversion_between_and_combination_of_dynamic_and_static_graphs`\n",
    "\n",
    "Dynamic graphs are easy to debug and suitable for model design and\n",
    "testing, whereas static graphs improve execution efficiency and shorten\n",
    "model training time. Is there a way for the machine learning framework\n",
    "to combine the merits of both approaches? Major machine learning\n",
    "frameworks, such as TensorFlow, MindSpore, PyTorch, and PaddlePaddle,\n",
    "have added support to convert between dynamic and static graphs,\n",
    "allowing developers to program using the dynamic graph approach and\n",
    "letting the framework automatically convert the code to a static\n",
    "equivalent for execution.\n",
    "\n",
    "Table :numref:`ch04/ch4-eagertoscript` lists the APIs for dynamic graph\n",
    "to static graph conversion provided by major frameworks.\n",
    "\n",
    ":Dynamic graph to static graph conversion support of major frameworks\n",
    "\n",
    "|                                  Framework                                   | Dynamic Graph to Static Graph Conversion |\n",
    "|------------------------------------------------------------------------------|------------------------------------------ |\n",
    "|                                 TensorFlow                                   | |\n",
    "|         where AutoGraph automatically transforms a control flow to           | |\n",
    "|                      the equivalent static statement.                        | |\n",
    "|                                  MindSpore                                   | |\n",
    "|     `context.set_context(mode=context.GRAPH_MODE)`: static graph mode,       | |\n",
    "|           `@ms_function`: builds a static graph from source code.            | |\n",
    "|                                   PyTorch                                    | |\n",
    "|      `torch.jit.trace()`: builds a static graph by tracing operators.        | |\n",
    "|                                PaddlePaddle                                  | |\n",
    "|`paddle.jit.TracedLayer.trace()`: builds a static graph by tracing operators. | |\n",
    ":label:`ch04/ch4-eagertoscript`\n",
    "\n",
    "These dynamic-to-static conversion methods fall into the following two\n",
    "categories:\n",
    "\n",
    "1.  **Tracing**: A static graph is built by tracing operator scheduling\n",
    "    in a dynamic graph.\n",
    "\n",
    "2.  **Source code transformation**: The frontend code is inspected and\n",
    "    built as static graph code. And the static graph executor is\n",
    "    automatically called to run the static graph.\n",
    "\n",
    "The *tracing* method goes through two simple phases. The first is to\n",
    "generate a dynamic graph, following a workflow similar to that shown in\n",
    "Figure :numref:`ch04/ch04-dynamic-gen`. The machine learning framework\n",
    "runs the created dynamic graph and traces the data flow and operator\n",
    "scheduling in the dynamic graph to produce a static graph. Note that the\n",
    "dynamic graph is not destroyed; instead, it is preserved as a static\n",
    "graph for subsequent execution. As the machine learning framework\n",
    "finishes executing the dynamic graph, a static graph is produced. In the\n",
    "second phase when the model is called again, the machine learning\n",
    "framework runs the static graph for computation. The tracing technique\n",
    "only traces the operators scheduled when the dynamic graph is run for\n",
    "the first time. However, if the model has a data-dependent conditional,\n",
    "only one branch of the conditional can be traced --- the traced graph\n",
    "would be unable to take alternate branches. Similarly, the traced graph\n",
    "cannot include every iteration if there is a data-dependent loop.\n",
    "\n",
    "Unlike dynamic graph code which is parsed and executed by the frontend\n",
    "interpreter, a static graph must be first created by the graph compiler\n",
    "of the machine learning framework before execution. Because the graph\n",
    "compiler cannot directly deal with dynamic graph code, the source code\n",
    "transformation--based method is introduced to convert the dynamic graph\n",
    "code into static code description.\n",
    "\n",
    "The *source code transformation*--based method can overcome the\n",
    "drawbacks involved in the tracing method and also consists of two\n",
    "phases, as shown in Figure :numref:`ch04/ch04-ast`. The first involves lexical and syntax\n",
    "analysis. Specifically, the lexical analyzer scans and analyzes every\n",
    "character in the dynamic graph code, splits the source text by removing\n",
    "any white spaces or comments, and returns a stream of tokens. Then, the\n",
    "syntax analyzer or parser analyzes the token stream, eliminates any\n",
    "errors, and generates a parse tree as the output of the phase. In the\n",
    "second phase, the built-in translators of the machine learning framework\n",
    "scan and translate each part of the abstract syntax tree to map the\n",
    "grammatical structures from dynamic graph format into static graph\n",
    "format. Any control flow written in the frontend language is transformed\n",
    "into the corresponding static graph API in this phase, so as to include\n",
    "every branch of the control flow in the resulting graph. Next, we can\n",
    "easily generate static graph code from the translated syntax tree.\n",
    "\n",
    "![Source code transformation](../img/ch04/ast.png)\n",
    ":label:`ch04/ch04-ast`\n",
    "\n",
    "In numerous instances, the utilization of either tracing or source code\n",
    "transformation proves to be a more convenient approach in the conversion\n",
    "of a model to a static graph. Both tracing and source code\n",
    "transformation can be combined to cater to the specific requirements of\n",
    "a model segment. For instance, PyTorch offers both methods for\n",
    "transforming dynamic graphs into static graphs, and frequently, a hybrid\n",
    "approach is employed. Scripted functions can invoke traced functions,\n",
    "which is advantageous when implementing control-flow mechanisms within a\n",
    "straightforward model, such as utilizing beam search in a\n",
    "sequence-to-sequence model with an encoder module produced through\n",
    "tracing. Traced functions, on the other hand, can call script functions,\n",
    "which is beneficial when control-flow is needed in a limited section of\n",
    "a model, typically a feed-forward network.\n",
    "\n",
    "To improve the computational efficiency, we can transform the entire\n",
    "model graph for fast deployment on hardware. Alternatively, we can\n",
    "consider transforming some of the model functions into static subgraphs\n",
    "and embedding them into the global dynamic graph as individual\n",
    "operators, so that these exact functions would run in the form of static\n",
    "graphs at execution time. This not only improves computational\n",
    "efficiency but also retains flexibility for code debugging.\n",
    "\n",
    "Code `ch04/code6` shows a simple model, which can be built into a\n",
    "dynamic graph as a whole. In this example, we transform the\n",
    "`add_and_relu` module into a static subgraph. The model runs on the\n",
    "input data in a predefined sequence, resulting in a temporary dynamic\n",
    "graph. When the `Y=add_and_relu(Y,b)` statement is executed, the machine\n",
    "learning framework automatically runs the static subgraph transformed\n",
    "from the module, achieving a performance gain by combining the\n",
    "advantages of dynamic and static graphs.\n",
    "\n",
    "**ch04/code6**\n",
    "\n",
    "```python\n",
    "def add_and_relu(Y, b):\n",
    "    Y = Y + b\n",
    "    Y = relu(Y)\n",
    "    return Y\n",
    "def model(X, flag):\n",
    "    if flag > 0:     \n",
    "        Y = matmul(W1, X)   \n",
    "    else:     \n",
    "        Y = matmul(W2, X)\n",
    "        Y = add_and_relu(Y, b)\n",
    "    return Y\n",
    "```\n",
    "\n",
    "Dynamic-to-static conversion is mostly found in the model deployment\n",
    "stage, as a workaround to the hardware constraints on dynamic graph\n",
    "deployment, which requires the frontend model definition code for\n",
    "topology discovery in addition to the file of already-trained\n",
    "parameters. To remove the frontend dependency, once model training in\n",
    "dynamic graph mode is complete, we may convert the model into static\n",
    "graph format and serialize the model and parameter files, thereby\n",
    "expanding the list of supported hardware.\n",
    "\n",
    "However, the process of translating a dynamic graph into a static graph\n",
    "can become more intricate when dealing with reverse graph dependencies\n",
    "and dynamic shapes. Additionally, the performance of the executing\n",
    "engine may be compromised during complex graph transformations. To\n",
    "address this, frameworks like PyTorch have introduced more aggressive\n",
    "dynamic transformation methods. PyTorch's dynamo module not only\n",
    "implements source code transformation, but also replaces the Python\n",
    "execution engine with lower-level APIs. This approach resembles the\n",
    "combination of a compiler and interpreter found in modern Python code\n",
    "execution engines like CPython, resulting in optimal performance.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}