{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02686a6b",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Memory Allocation\n",
    "\n",
    "Memory allocation is a crucial aspect of conventional computer memory\n",
    "hierarchy, acting as a link between cache and disk storage. It provides\n",
    "more storage capacity than the cache and enables faster access compared\n",
    "to disk storage. With the progress of deep learning, accommodating large\n",
    "deep neural networks within the memory of hardware accelerators or AI\n",
    "processors has become increasingly challenging. To overcome this\n",
    "obstacle, various solutions have been developed, including memory reuse,\n",
    "contiguous memory allocation, and in-place memory allocation. Proper\n",
    "implementation of contiguous memory allocation and in-place memory\n",
    "allocation can enhance the execution efficiency of operators and further\n",
    "optimize performance.\n",
    "\n",
    "## Device Memory\n",
    "\n",
    "In a deep learning architecture, the memory closest to the hardware\n",
    "accelerator (such as the GPU or AI processor) is usually referred to as\n",
    "the device memory, and that closest to the CPU is referred to as the\n",
    "host memory. As shown in Figure\n",
    ":numref:`ch07/ch07-compiler-backend-memory-01`, the CPU can\n",
    "directly access the host memory but not the device memory. Similarly,\n",
    "the AI processor can directly access the device memory but not the host\n",
    "memory. In a typical network training process, data needs to be loaded\n",
    "from disk storage to the host memory, where it is then processed. After\n",
    "that, the data is copied from the host memory to the device memory, so\n",
    "that the device can directly access the data. When the computation is\n",
    "finished, the user can obtain the training result once the result data\n",
    "is copied from the device memory back to the host memory.\n",
    "\n",
    "![Host memory and devicememory](../img/ch07/host-device-memory.png)\n",
    ":label:`ch07/ch07-compiler-backend-memory-01`\n",
    "\n",
    "## Process of Memory Allocation\n",
    "\n",
    "The memory allocation module allocates device memory to the input and\n",
    "output of each operator in a graph. The compiler frontend interprets the\n",
    "user script into an IR, based on which the compiler backend performs\n",
    "operator selection and optimization to determine information such as the\n",
    "shape, data type, and format of each input/output tensor of each\n",
    "operator. With this information, the size of each input/output tensor of\n",
    "each operator can be calculated using Equation\n",
    ":eqref:`ch05/equation-04`:\n",
    "\n",
    "$$\n",
    "\\text{size}=\\prod_{i=0}^{\\text{dimension }}\\text{shape}_i \\times \\text{sizeof}\\left ( \\text{datatype} \\right )\n",
    "$$ \n",
    ":eqlabel:`equation:ch05/equation-04`\n",
    "\n",
    "Unaligned memory access can be time-consuming, because the transfer of\n",
    "data to and from memory is most efficient in chunks of 4, 8, or 16\n",
    "bytes. When the size of the data to be transferred is not a multiple of\n",
    "any of these sizes, one or more empty bytes are padded to align the data\n",
    "in memory.\n",
    "\n",
    "Figure\n",
    ":numref:`ch07/ch07-compiler-backend-memory-02` illustrates an\n",
    "example of memory allocation.\n",
    "\n",
    "![Memory allocationexample](../img/ch07/memory_allocate.png)\n",
    ":label:`ch07/ch07-compiler-backend-memory-02`\n",
    "\n",
    "In this example, memory addresses are assigned to the input tensor,\n",
    "Conv2D's weight, and Conv2D's output. Subsequently, a memory address is\n",
    "allocated to the input of BatchNorm. Since the input of BatchNorm is the\n",
    "same as the output of Conv2D, which already has a allocated memory\n",
    "address, the output address of Conv2D can be shared with the input of\n",
    "BatchNorm. This approach avoids redundant memory allocation and\n",
    "unnecessary memory copies. The entire training process in this example\n",
    "involves allocating memory for three types based on their data lifetime:\n",
    "the initial input of the graph, the weights or attributes of operators,\n",
    "and the output tensor of the final operator.\n",
    "\n",
    "Frequent allocations and deallocations of memory blocks of various sizes\n",
    "using functions like `malloc` can significantly degrade performance. To\n",
    "mitigate this issue, memory pools can be employed. Memory pools involve\n",
    "pre-allocating a specific amount of memory, allowing memory blocks to be\n",
    "dynamically allocated from the pool as needed and returned for reuse.\n",
    "\n",
    "Memory pools are widely utilized in AI frameworks to manage frequent\n",
    "allocations of device memory and ensure consistent memory lifetime for\n",
    "tensors. Different AI frameworks adopt similar memory pool designs.\n",
    "Figure\n",
    ":numref:`ch07/ch07-compiler-backend-memory-03` presents an\n",
    "example of memory allocation in an AI framework. In this case, each\n",
    "tensor's memory is allocated from a pre-allocated device memory space\n",
    "using double pointers to offset the start and end addresses. Weight\n",
    "tensors of operators are allocated memory by offsetting from the start\n",
    "address (with a lifetime lasting throughout the training process). The\n",
    "output tensor of each operator is allocated memory by offsetting from\n",
    "the end address (with a shorter lifetime that terminates when the tensor\n",
    "is no longer needed in the computation process). This approach allows\n",
    "operator memory to be allocated using offset pointers from pre-allocated\n",
    "device memory, significantly reducing the time required compared to\n",
    "direct memory allocations from the device.\n",
    "\n",
    "![Memory allocation using double offsetpointers](../img/ch07/device_malloc.png)\n",
    ":label:`ch07/ch07-compiler-backend-memory-03`\n",
    "\n",
    "## Memory Reuse\n",
    "\n",
    "In a machine learning system, memory reuse is achieved by analyzing the\n",
    "lifespan of a tensor and, once it reaches the end of its lifespan,\n",
    "releasing its device memory back to the memory pool for future reuse by\n",
    "other tensors. The objective of memory reuse is to enhance memory\n",
    "utilization and enable the accommodation of larger models within the\n",
    "constraints of limited device memory. By reusing memory instead of\n",
    "continuously allocating new memory for tensors, the system can optimize\n",
    "memory utilization and mitigate the memory limitations inherent in deep\n",
    "learning computations.\n",
    "\n",
    "Figure\n",
    ":numref:`ch07/ch07-compiler-backend-memory-02` provides an\n",
    "example, where output 1 becomes unused once the computation of the\n",
    "BatchNorm operator is complete. In this case, the device memory of\n",
    "output 1 can be reclaimed and reused for output 3 (if output 3 does not\n",
    "require a larger memory size than output 1).\n",
    "\n",
    "Figure\n",
    ":numref:`ch07/ch07-compiler-backend-memory-04` depicts memory\n",
    "lifetime using coordinate charts. The horizontal axes represent the\n",
    "tensor lifetime, and the vertical axes represent the memory sizes.\n",
    "During its lifetime, a tensor occupies a specific amount of device\n",
    "memory. The objective of memory allocation is to find an optimal\n",
    "solution that accommodates the maximum number of non-conflicting\n",
    "rectangular blocks (each denoting a tensor's lifetime and memory size)\n",
    "in the same memory. In Figure\n",
    ":numref:`ch07/ch07-compiler-backend-memory-04`, the memory can\n",
    "accommodate only four rectangular blocks (i.e., tensors T0, T1, T2, and\n",
    "T3) when no memory reuse policy is applied, as shown in the left chart.\n",
    "\n",
    "![Memory lifetimecharts](../img/ch07/combine_memory_resue_and_no_reuse_cn.png)\n",
    ":label:`ch07/ch07-compiler-backend-memory-04`\n",
    "\n",
    "To determine an appropriate memory reuse policy, we face an NP-complete\n",
    "problem. AI frameworks often employ greedy algorithms, such as best-fit,\n",
    "which allocate memory by searching for the smallest available block in\n",
    "the memory pool one at a time. However, this approach only yields a\n",
    "locally optimal solution rather than a globally optimal one. To\n",
    "approximate a globally optimal solution, a method called Safe Optimized\n",
    "Memory Allocation Solver (SOMAS) can be considered.\n",
    "\n",
    "SOMAS addresses the computational graph by conducting aggregative\n",
    "analysis on parallel streams and data dependencies. This analysis\n",
    "reveals the ancestor-descendant relationships between operators. By\n",
    "generating a global set of mutually exclusive constraints concerning the\n",
    "lifetime of each tensor, SOMAS combines multiple heuristic algorithms to\n",
    "achieve an optimal solution for static memory planning. Through SOMAS,\n",
    "an optimized memory reuse outcome is obtained, resulting in increased\n",
    "reusable memory.\n",
    "\n",
    "As shown in the right chart of Figure\n",
    ":numref:`ch07/ch07-compiler-backend-memory-04`, with the SOMAS\n",
    "algorithm, the number of tensors allowed in the same memory is increased\n",
    "to seven.\n",
    "\n",
    "## Optimization Techniques for Memory Allocation\n",
    "\n",
    "In the following, we describe the typical optimization techniques for\n",
    "memory allocation.\n",
    "\n",
    "### Memory Fusion\n",
    "\n",
    "Commonly used memory allocation methods operate at the tensor level,\n",
    "often resulting in discontinuous device addresses across tensors.\n",
    "However, certain specialized operators, like AllReduce for\n",
    "communication, require contiguous memory allocation. Executing a\n",
    "communication operator involves waiting for communication, which is a\n",
    "significant performance bottleneck in large-scale distributed systems.\n",
    "It includes data transfer and computation. To minimize communication\n",
    "time, we can fuse multiple communication operators into a composite\n",
    "operator. This allows for contiguous memory allocation of the operator\n",
    "input, as depicted in Figure\n",
    ":numref:`ch07/ch07-compiler-backend-memory-06`.\n",
    "\n",
    "Additionally, the time spent in communication can be reduced during the\n",
    "weight initialization task in distributed neural network training. This\n",
    "task involves broadcasting the initialized weight from one process to\n",
    "all processes. If a network contains multiple weights (which is often\n",
    "the case), these broadcasts are repeated. To minimize communication time\n",
    "in this scenario, a typical approach is to allocate contiguous memory\n",
    "addresses to all weights on the network and then perform a single\n",
    "broadcast operation.\n",
    "\n",
    "![Memory fusion of communicationoperators](../img/ch07/memory_fusion.png)\n",
    ":label:`ch07/ch07-compiler-backend-memory-06`\n",
    "\n",
    "### In-place Operators\n",
    "\n",
    "In the memory allocation process depicted in\n",
    "Figure :numref:`ch07/ch07-compiler-backend-memory-02`, the input and\n",
    "output of each operator are assigned different memory addresses.\n",
    "However, this approach can lead to memory waste and performance\n",
    "degradation for several other operators. Examples include optimizer\n",
    "operators used to update neural network weights, Python's `+=` or `*=`\n",
    "operators that modify variable values, and the `a[0]=b` operator that\n",
    "updates the value of `a[0]` with `b`. These operators share a common\n",
    "purpose: updating the input value. The concept of in-place can be\n",
    "illustrated using the `a[0]=b` operator.\n",
    "\n",
    "In the original implementation shown on the left of Figure\n",
    ":numref:`ch07/ch07-compiler-backend-memory-08`, the operator\n",
    "involves three steps: copying tensor `a` to tensor `a’`, assigning\n",
    "tensor `b` to tensor `a’`, and then copying tensor `a’` back to tensor\n",
    "`a`. However, by performing the operation in-place, as depicted on the\n",
    "right of Figure\n",
    ":numref:`ch07/ch07-compiler-backend-memory-08`, this process is\n",
    "simplified to a single step: copying tensor `b` to the position\n",
    "corresponding to tensor `a`. This reduces data copy time by eliminating\n",
    "two copies and eliminates the need to allocate memory for tensor `a’`.\n",
    "\n",
    "![Memory allocation of an in-placeoperator](../img/ch07/inplace-op.png)\n",
    ":label:`ch07/ch07-compiler-backend-memory-08`\n",
    "\n",
    "## Data Compression\n",
    "\n",
    "Deep neural networks (DNNs) in modern training heavily rely on GPUs to\n",
    "effectively train intricate networks with hundreds of layers. A\n",
    "prominent challenge faced by both researchers and industry professionals\n",
    "is the constraint imposed by the available GPU main memory as networks\n",
    "become deeper. This limitation restricts the size of networks that can\n",
    "be trained. To address this issue, researchers have recognized the value\n",
    "of employing DNN-layer-specific encoding schemes. Consequently, they\n",
    "have directed their attention towards storing encoded representations of\n",
    "the intermediate layer outputs (feature maps) that are required for the\n",
    "backward pass. These encoded representations are stored during the\n",
    "temporal gap between their uses and are decoded only when needed for the\n",
    "backward pass. The full-fidelity feature maps are promptly discarded\n",
    "after use, resulting in a noteworthy reduction in memory consumption.\n",
    "\n",
    "## Memory Swap\n",
    "\n",
    "Machine learning frameworks frequently necessitate users to optimize\n",
    "their memory utilization to guarantee that the DNN can be accommodated\n",
    "within the memory capacity of the GPU. This constraint restricts\n",
    "researchers from thoroughly investigating diverse machine learning\n",
    "algorithms, compelling them to make concessions either in terms of\n",
    "network architecture or by distributing the computational load across\n",
    "multiple GPUs. One feasible approach is to incorporate DRAM to\n",
    "facilitate memory swapping. By transferring temporarily inactive data to\n",
    "DRAM, we can optimize GPU utilization. In recent studies, researchers\n",
    "have implemented a cautious approach to allocating GPU memory for the\n",
    "immediate computational needs of a specific layer. This strategy\n",
    "effectively reduces both the maximum and average memory usage, enabling\n",
    "researchers to train more extensive networks. To elaborate further, the\n",
    "researchers promptly release feature maps from GPU memory in the absence\n",
    "of any potential reuse. Alternatively, if there is a possibility of\n",
    "future reuse but no immediate requirement, the feature maps are\n",
    "offloaded to CPU memory and subsequently prefetched back to GPU memory.\n",
    "\n",
    "The fundamental concept behind memory swapping is straightforward and\n",
    "inherent. However, its implementation remains challenging and\n",
    "necessitates prior expertise in our compiler frontend. One such\n",
    "expertise involves maximizing the overlap between computation and data\n",
    "swapping time. A precise cost model is essential for evaluating the\n",
    "estimated time required for data movement and the time cost associated\n",
    "with each layer in DNN (Deep Neural Network). Additionally, there are\n",
    "numerous strategies to explore in auto scheduling and auto tuning.\n",
    "Fortunately, there is an abundance of literature available that\n",
    "addresses these issues. For additional information, please refer to the\n",
    "Further Readings section.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}