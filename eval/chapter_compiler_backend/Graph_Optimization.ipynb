{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16a35e6",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Graph Optimization\n",
    "\n",
    "Graph optimization techniques at the backend primarily focus on\n",
    "hardware-oriented approaches. These techniques can be categorized as\n",
    "hardware-agnostic, such as memory I/O optimization, or specific to\n",
    "particular hardware, such as subgraph transformation to accommodate\n",
    "hardware instruction restrictions.\n",
    "\n",
    "## Hardware-Agnostic Optimizations\n",
    "\n",
    "Hardware-agnostic optimizations involve subgraph transformation, which\n",
    "replaces a subgraph in a computational graph with a hardware-friendly\n",
    "equivalent.\n",
    "\n",
    "One example of such optimization is memory I/O optimization. In deep\n",
    "learning models, operators can be categorized as either\n",
    "compute-intensive (e.g., Conv and FC) or memory-intensive (e.g., ReLU\n",
    "and element-wise Sum). Memory-intensive operators are mainly used for\n",
    "element-wise operations. Often, both types of operators are used\n",
    "together in a typical deep learning model, such as the combination of\n",
    "\\\"Conv + ReLU\\\". By fusing ReLU and Conv into a composite operator, we\n",
    "can reduce memory access latency, bandwidth pressure, and improve\n",
    "execution efficiency.\n",
    "\n",
    "Figure :numref:`ch07/ch07-compiler-backend-03` illustrates an example of\n",
    "fusing \\\"Conv + Conv + Sum + ReLU\\\". This fusion optimization eliminates\n",
    "two read operations and two write operations, optimizing the read and\n",
    "write of the outputs generated by Conv and Sum.\n",
    "\n",
    "![Element-wise operatorfusion](../img/ch07/conv_sum_relu.png)\n",
    ":label:`ch07/ch07-compiler-backend-03`\n",
    "\n",
    "Furthermore, automatic operator generation technology enables more\n",
    "flexible general optimizations in addition to fusion-based optimizations\n",
    "for specific operator types. An example of this technology is graph\n",
    "kernel fusion (available on AI frameworks such as TensorFlow and\n",
    "MindSpore). It aims to reduce inefficient memory movements and enable\n",
    "intensive computing through three steps: operator expansion,\n",
    "aggregation, and reconstruction.\n",
    "\n",
    "Figure\n",
    ":numref:`ch07/ch07-compiler-backend-graph-kernel` provides an\n",
    "overview of graph kernel fusion, which involves the following steps:\n",
    "\n",
    "1.  Expander: Composite operators (Op1, Op3, and Op4) in the\n",
    "    computational graph are expanded into combinations of basic\n",
    "    operators, as represented by the graph nodes with dash lines.\n",
    "\n",
    "2.  Aggregation: The basic operator (Op2) and expanded operators are\n",
    "    aggregated into larger operator combinations.\n",
    "\n",
    "3.  Reconstruction: The basic operators are classified based on the\n",
    "    input-to-output affinity, such as elemwise, broadcast, reduce, and\n",
    "    transform. This classification allows the derivation of general\n",
    "    compute rules (e.g., elemwise + reduce) to facilitate efficient\n",
    "    execution. The operator combination is then analyzed and filtered\n",
    "    iteratively, leading to the creation of new operators (New Op1 and\n",
    "    New Op2) through reconstruction. These new operators are designed to\n",
    "    be hardware-friendly.\n",
    "\n",
    "Graph kernel fusion enables joint optimization beyond operator\n",
    "boundaries by expanding and aggregating the computational graph. It\n",
    "generates new hardware-friendly operators through reconstruction based\n",
    "on general compute rules, thereby facilitating efficient execution.\n",
    "However, it should be noted that this approach involves additional\n",
    "memory movements.\n",
    "\n",
    "![Graph kernelfusion](../img/ch07/graph_kernel.png)\n",
    ":label:`ch07/ch07-compiler-backend-graph-kernel`\n",
    "\n",
    "## Hardware-Specific Optimizations\n",
    "\n",
    "Hardware-specific optimizations are tailored to address the restrictions\n",
    "imposed by specific hardware instructions and memory formats associated\n",
    "with particular hardware devices.\n",
    "\n",
    "### Hardware Instruction Restrictions\n",
    "\n",
    "Hardware instruction restrictions arise when certain IR nodes lack\n",
    "direct operator counterparts on a specific hardware device. In such\n",
    "cases, subgraph transformation can be employed to overcome these\n",
    "restrictions. Let's consider an example. The Concat operator on the\n",
    "accelerator supports a maximum of 63 inputs. If the Concat node in the\n",
    "frontend IR exceeds this limit, we can partition the node into multiple\n",
    "smaller Concat nodes. Figure\n",
    ":numref:`ch07/ch07-compiler-backend-04` illustrates how we can\n",
    "split a 100-input Concat node into two smaller nodes, one with 63 inputs\n",
    "and the other with 37 inputs, to meet the 63-input requirement of the\n",
    "accelerator.\n",
    "\n",
    "![Partitioning of the Concatoperator](../img/ch07/concat.png)\n",
    ":label:`ch07/ch07-compiler-backend-04`\n",
    "\n",
    "### Memory Format Restrictions\n",
    "\n",
    "Different platforms define varying formats for different operators to\n",
    "achieve optimal performance. When the formats are inconsistent with a\n",
    "particular framework, a common approach is to insert format\n",
    "transformation operations to reformat the operator output. However, this\n",
    "introduces additional memory movements.\n",
    "\n",
    "Figure :numref:`ch07/ch07-compiler-backend-05` provides an example to\n",
    "illustrate this scenario. Consider that the default format in an AI\n",
    "framework is NCHW, but the hardware accelerator is optimized for\n",
    "performing convolution with inputs and outputs in NC1HWC0 format. To\n",
    "bridge this gap, the output of the first Conv operator is formatted to\n",
    "NCHW using a TransData operator. It is then reformatted to NC1HWC0 using\n",
    "another TransData operator before being passed to the next Conv\n",
    "operator. The two TransData operations (depicted as dashed lines in the\n",
    "figure) are inverse operations of each other. By employing pattern\n",
    "matching on the computational graph, such operations can be easily\n",
    "eliminated.\n",
    "\n",
    "![Elimination of format transformationoperations](../img/ch07/transdata.png)\n",
    ":label:`ch07/ch07-compiler-backend-05`\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}