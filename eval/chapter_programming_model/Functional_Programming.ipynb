{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad8d88f",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Functional Programming\n",
    "\n",
    "In the following, we will discuss the reasons behind the growing trend\n",
    "of incorporating functional programming into the design of machine\n",
    "learning frameworks.\n",
    "\n",
    "## Benefits of Functional Programming\n",
    "\n",
    "Training constitutes the most critical phase in machine learning, and\n",
    "the manner in which training is depicted hinges significantly on\n",
    "optimizer algorithms. Predominantly, contemporary machine learning tasks\n",
    "utilize first-order optimizers, favored for their ease of use. With\n",
    "machine learning advancing at a rapid pace, both software and hardware\n",
    "are incessantly updated to stay abreast. Consequently, an increasing\n",
    "number of researchers are beginning to investigate higher-order\n",
    "optimizers, noted for their superior convergence performance. Frequently\n",
    "utilized second-order optimizers, such as the Newton method,\n",
    "quasi-Newton method, and AdaHessians, necessitate the computation of a\n",
    "Hessian matrix incorporating second-order derivative information. Two\n",
    "considerable challenges arise from this computation: 1) how to manage\n",
    "such a hefty computational load efficiently; 2) how to express\n",
    "higher-order derivatives in programmatic language.\n",
    "\n",
    "In recent times, numerous large AI models have been introduced, which\n",
    "include (with the number of parameters noted in parentheses) OpenAI\n",
    "GPT-3 (175B) in 2020; PanGu (100B), PanGu-$\\alpha$ (200B), Google's\n",
    "Switch Transformer (1.6T), and WuDao (1.75T) in 2021; along with\n",
    "Facebook's NLLB-200 (54B) in 2022. The demand for ultra-large model\n",
    "training is escalating, and data parallelism alone cannot meet this\n",
    "growing requirement. Conversely, model parallelism demands manual model\n",
    "segmentation, a process that is time-intensive and laborious.\n",
    "Consequently, the main challenge future machine learning frameworks must\n",
    "overcome is how to actualize automatic parallelism. At its core, a\n",
    "machine learning model is a representation of a mathematical model.\n",
    "Hence, the ability to succinctly represent machine learning models has\n",
    "risen to a key concern in the design of programming paradigms for\n",
    "machine learning frameworks.\n",
    "\n",
    "Recognizing the challenges presented by the practical implementation of\n",
    "machine learning frameworks, researchers have identified that functional\n",
    "programming could offer beneficial solutions. Functional programming, in\n",
    "computer science, is a programming paradigm that envisions computation\n",
    "as the evaluation of mathematical functions, actively avoiding state\n",
    "changes and data mutations. This paradigm harmonizes well with\n",
    "mathematical reasoning. Neural networks are composed of interconnected\n",
    "nodes, with each node performing basic mathematical operations.\n",
    "Functional programming languages allow developers to portray these\n",
    "mathematical operations in a language that closely mirrors the\n",
    "operations, enhancing the readability and maintainability of programs.\n",
    "Concurrently, in functional languages, functions are kept separate,\n",
    "simplifying the management of concurrency and parallelism.\n",
    "\n",
    "In summary, functional programming is anticipated to confer the\n",
    "following benefits to machine learning frameworks:\n",
    "\n",
    "1.  It is suited for machine learning scenarios where higher-order\n",
    "    derivatives are needed.\n",
    "\n",
    "2.  It simplifies the development of parallel programming interfaces.\n",
    "\n",
    "3.  It results in a more concise code representation.\n",
    "\n",
    "## Framework Support for Functional Programming\n",
    "\n",
    "Machine learning frameworks have increasing support for functional\n",
    "programming. In 2018, Google rolled out JAX. Contrary to traditional\n",
    "machine learning frameworks, JAX amalgamates neural network computation\n",
    "and numerical computation. Its interfaces are compatible with native\n",
    "data science interfaces in Python, such as NumPy and SciPy. Moreover,\n",
    "JAX extends distribution, vectorization, high-order derivation, and\n",
    "hardware acceleration in a functional programming style, characterized\n",
    "by Lambda closure and no side effects.\n",
    "\n",
    "In 2020, Huawei introduced MindSpore, the functional differential\n",
    "programming architecture of which allows users to concentrate on the\n",
    "native mathematical expressions of machine learning models. In 2022,\n",
    "taking inspiration from Google's JAX, PyTorch launched functorch.\n",
    "Functorch is essentially a library aimed at providing composable vmap\n",
    "(vectorization) and autodiff transforms compatible with PyTorch modules\n",
    "and PyTorch autograd, thereby achieving excellent eager-mode\n",
    "performance. It can be inferred that functorch meets the requirements\n",
    "for distributed parallelism in PyTorch static graphs. Code\n",
    "`ch02/code2.4` gives an example of functorch.\n",
    "\n",
    "**ch02/code2.4**\n",
    "\n",
    "```\n",
    "from functorch import combine_state_for_ensemble, vmap\n",
    "minibatches = data[:num_models]\n",
    "models = [MLP().to(device) for _ in range(num_models)]\n",
    "fmodel, params, buffers = combine_state_for_ensemble(models)\n",
    "predictions1_vmap = vmap(fmodel, out_dims=1)(params, buffers, minibatches)\n",
    "```\n",
    "\n",
    "Functorch introduces *vmap*, standing for \\\"vectorized map\\\". Its role\n",
    "is to adapt functions designed for individual inputs so that they can\n",
    "handle batches of inputs, therefore facilitating efficient vectorized\n",
    "calculations. Unlike the batch processing capabilities of standard\n",
    "PyTorch modules, vmap can convert any operation to be batch-aware\n",
    "without the need to alter the operation's original structure. Moreover,\n",
    "vmap offers greater flexibility to batch dimensions, allowing users to\n",
    "specify which dimension should be treated as the batch dimension\n",
    "(specifying the $out\\_dim$ argument), a contrast to the default\n",
    "behaviour of the standard PyTorch where the first dimension is usually\n",
    "chosen as the batch dimension.\n",
    "\n",
    "By tracing the development of machine learning frameworks, it becomes\n",
    "evident that the functional programming paradigm become increasingly\n",
    "popular. This can be attributed to functional programming's ability to\n",
    "express machine learning models intuitively and its convenience for\n",
    "implementing automatic differentiation, high-order derivation, and\n",
    "parallel execution. Consequently, future machine learning frameworks are\n",
    "likely to adopt layered frontend interfaces that are not exclusively\n",
    "designed for machine learning scenarios. Instead, they will primarily\n",
    "offer differential programming in their abstraction designs, making\n",
    "gradient-based software easy to be developed for various applications.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}