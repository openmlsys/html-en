{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df0dcb4",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Machine Learning Workflow\n",
    "\n",
    "In machine learning systems, the fundamental design objective of\n",
    "programming models is to offer comprehensive workflow programming\n",
    "support for developers. A typical machine learning task adheres to the\n",
    "workflow depicted in Figure :numref:`ch03/workflow`. This workflow involves loading the\n",
    "training dataset, training, testing, and debugging models. The following\n",
    "APIs are defined to facilitate customization within the workflow\n",
    "(assuming that high-level APIs are provided as Python functions):\n",
    "\n",
    "1.  **Data Processing API:** Users first require a data processing API\n",
    "    to read datasets from a disk. Subsequently, they need to preprocess\n",
    "    the data to make it suitable for input into machine learning models.\n",
    "    Code `ch02/code2.2.1` is an example of how PyTorch can be used\n",
    "    to load data and create data loaders for both training and testing\n",
    "    purposes.\n",
    "\n",
    "**ch02/code2.2.1**\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "data_path = '/path/to/data'\n",
    "dataset = pickle.load(open(data_path, 'rb')) # Example for a pkl file\n",
    "batch_size = ... # You can make it an argument of the script\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "training_dataset = CustomDataset(dataset['training_data'], dataset['training_labels'])\n",
    "testing_dataset = CustomDataset(dataset['testing_data'], dataset['testing_labels'])\n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)  # Create a training dataloader\n",
    "testing_dataloader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False) # Create a testing dataloader\n",
    "```\n",
    "\n",
    "2.  **Model Definition API:** Once the data is preprocessed, users need\n",
    "    a model definition API to define machine learning models. These\n",
    "    models include model parameters and can perform inference based on\n",
    "    given data. Code\n",
    "    `ch02/code2.2.2` is an example of how to create a custom\n",
    "    model in Pytorch:\n",
    "\n",
    "**ch02/code2.2.2**\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)  # A single linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "```\n",
    "\n",
    "3.  **Optimizer Definition API:** The outputs of models need to be\n",
    "    compared with user labels, and their difference is evaluated using a\n",
    "    loss function. The optimizer definition API enables users to define\n",
    "    their own loss functions and import or define optimization\n",
    "    algorithms based on the actual loss. These algorithms calculate\n",
    "    gradients and update model parameters. Code\n",
    "    `ch02/code2.2.3` is an example of an optimizer definition\n",
    "    in Pytorch:\n",
    "\n",
    "**ch02/code2.2.3**\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "import torch.nn\n",
    "model = CustomModel(...)\n",
    "# Optimizer definition (Adam, SGD, etc.)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, momentum=0.9) \n",
    "loss = nn.CrossEntropyLoss() # Loss function definition\n",
    "```\n",
    "\n",
    "4.  **Training API:** Given a dataset, model, loss function, and\n",
    "    optimizer, users require a training API to define a loop that reads\n",
    "    data from datasets in a mini-batch mode. In this process, gradients\n",
    "    are computed repeatedly, and model parameters are updated\n",
    "    accordingly. This iterative update process is known as *training*.\n",
    "    Code `ch02/code2.2.4` is an example of how to train a model in\n",
    "    Pytorch:\n",
    "\n",
    "**ch02/code2.2.4**\n",
    "\n",
    "```python\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # Select your training device\n",
    "model.to(device) # Move the model to the training device\n",
    "model.train() # Set the model to train mode\n",
    "epochs = ... # You can make it an argument of the script\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(training_dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() # zero the parameter gradients\n",
    "        output = model(data) # Forward pass\n",
    "        loss_value = loss(output, target) # Compute the loss\n",
    "        loss_value.backward() # Backpropagation\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "5.  **Testing and Debugging APIs:** Throughout the training process,\n",
    "    users need a testing API to evaluate the accuracy of the model\n",
    "    (training concludes when the accuracy exceeds the set goal).\n",
    "    Additionally, a debugging API is necessary to verify the performance\n",
    "    and correctness of the model. Code\n",
    "    `ch02/code2.2.5` is an example of model evaluation in\n",
    "    Pytorch:\n",
    "\n",
    "**ch02/code2.2.5**\n",
    "\n",
    "```python\n",
    "model.eval() # Set the model to evaluation mode\n",
    "overall_accuracy = []\n",
    "for batch_idx, (data, target) in enumerate(testing_dataloader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model(data) # Forward pass\n",
    "    accuracy = your_metrics(output, target) # Compute the accuracy\n",
    "    overall_accuracy.append(accuracy) # Print the accuracy\n",
    "# For debugging, you can print logs inside the training or evaluation loop, or use python debugger.\n",
    "```\n",
    "\n",
    "![Workflow within a machine learningsystem](../img/ch03/workflow.pdf)\n",
    ":label:`ch03/workflow`\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}