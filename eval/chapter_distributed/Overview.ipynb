{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56f5319",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Overview\n",
    "\n",
    "This section provides an overview of the need for distributed training\n",
    "systems.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "The principal objective of implementing distributed training systems is\n",
    "to circumvent the restrictions imposed by single-node training systems,\n",
    "primarily characterized by their computational and memory constraints.\n",
    "\n",
    "### Computational Constraints\n",
    "\n",
    "A single processor, confined by its inherent limitations, can only yield\n",
    "a certain extent of computational power, quantified in terms of\n",
    "*floating-point operations per second (FLOPS)*. The advent of\n",
    "distributed training systems emerged as an innovative resolution to\n",
    "overcome these constraints associated with a single processor's\n",
    "computational prowess.\n",
    "\n",
    "Figure :numref:`ch010/ch10-computation-increase` illustrates the\n",
    "escalating demands for computational power required by machine learning\n",
    "models compared to the growth rate of a processor's computational\n",
    "capabilities over the past few years. In this context, computational\n",
    "power is measured in petaFLOP/s-day, a unit implying the execution of\n",
    "$10^{15}$ neural network operations every second for an entire day,\n",
    "summing up to approximately $10^{20}$ operations in total. According to\n",
    "Moore's Law, the computational power of CPUs approximately doubles every\n",
    "18 months. This exponential growth principle also extends to\n",
    "accelerators, such as GPUs and TPUs, which are leveraged to support\n",
    "machine learning computations with their immense computational\n",
    "abilities.\n",
    "\n",
    "However, the evolution of machine learning models is outpacing this\n",
    "growth rate. A few years back, machine learning models, like AlexNet,\n",
    "could only recognize a limited array of objects. Now, with models like\n",
    "AlphaStar, we have reached a point where machines can outperform humans\n",
    "in executing certain intricate tasks. In this short timeframe, the\n",
    "computational demands of machine learning models have escalated 56-fold\n",
    "every 18 months.\n",
    "\n",
    "Distributed computing is designed to reconcile this divergence between\n",
    "the performance of processors and the rising demand for computational\n",
    "power. By capitalizing on the myriad of processors available in\n",
    "expansive data centers and cloud computing facilities and managing them\n",
    "effectively through distributed training systems, we can cater to the\n",
    "surging computational requirements of evolving models.\n",
    "\n",
    "![Machine Learning Model Size vs. Hardware ComputationalCapability](../img/ch10/ch10-computation-increase.png)\n",
    ":label:`ch010/ch10-computation-increase`\n",
    "\n",
    "### Memory Constraints\n",
    "\n",
    "The process of training machine learning models often necessitates\n",
    "substantial memory. Take, for instance, a neural network model boasting\n",
    "100 billion parameters in a 32-bit floating-point format (4 bytes); it\n",
    "would demand 400 GB of memory to store all parameters. In practice,\n",
    "additional memory is needed to store activation values and gradients.\n",
    "Assuming these are also stored in a 32-bit floating-point format, an\n",
    "extra 800 GB of memory would be required. This would result in an\n",
    "overall memory requirement exceeding 1200 GB (or 1.2 TB). Nevertheless,\n",
    "current accelerators, such as the NVIDIA A100, can only provide a\n",
    "maximum memory of 80 GB.\n",
    "\n",
    "However, unlike individual accelerators, whose memory growth is largely\n",
    "hindered by factors such as hardware specifications, heat dissipation,\n",
    "and costs, distributed training systems have the potential to train\n",
    "models with hundreds of billions of parameters across hundreds of\n",
    "accelerators simultaneously. This approach can fulfill the model's\n",
    "memory requirements in the terabyte range.\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "Data centers, housing hundreds of clusters with each cluster operating\n",
    "hundreds to thousands of servers, provide an ideal environment for\n",
    "distributed training. We can harness the power of numerous servers in a\n",
    "distributed training system to parallelly train a machine learning\n",
    "model.\n",
    "\n",
    "![Comparison between single-node computing and multi-Node distributedcomputing](../img/ch10/ch10-single-vs-multi.png)\n",
    ":label:`ch010/ch10-single-vs-multi`\n",
    "\n",
    "For enhancing the efficiency of the distributed training system, it is\n",
    "crucial to assess the computational power and memory usage of computing\n",
    "tasks, ensuring no single task turns into a bottleneck. As depicted in\n",
    "Figure :numref:`ch010/ch10-single-vs-multi`, the system evenly\n",
    "distributes a task across all computing nodes by partitioning the input\n",
    "data into segments. Each model training job, which takes a dataset\n",
    "(e.g., training samples) or a group of tasks (e.g., operators) as input,\n",
    "is run on a computing node (e.g., a GPU) to generate outputs (e.g.,\n",
    "gradients).\n",
    "\n",
    "Distributed execution generally comprises three steps:\n",
    "\n",
    "1.  *Partitioning* the input into smaller segments.\n",
    "\n",
    "2.  *Distributing* these partitions across multiple compute nodes for\n",
    "    parallel computing.\n",
    "\n",
    "3.  *Merging* the outputs from all compute nodes to generate a result\n",
    "    akin to that of single-node computing.\n",
    "\n",
    "This process fundamentally adheres to the divide-and-conquer approach,\n",
    "where each compute node runs a small portion of the workload in parallel\n",
    "with others, thus expediting the overall computing process.\n",
    "\n",
    "## Benefits\n",
    "\n",
    "Distributed training systems bring the following benefits:\n",
    "\n",
    "1.  **Improved system performance:** Distributed training significantly\n",
    "    improves training performance. Generally, we use the\n",
    "    time-to-accuracy metric to measure the performance of a distributed\n",
    "    training system. This metric is determined by two parameters: time\n",
    "    taken to process all training samples one time and the accuracy\n",
    "    improved within the time. By adding parallel compute nodes, we can\n",
    "    shorten the time taken to process all training samples one time and\n",
    "    therefore achieve smaller time-to-accuracy values.\n",
    "\n",
    "2.  **Reduced costs:** Distributed training reduces the cost of training\n",
    "    machine learning models. Due to the limited heat dissipation\n",
    "    capacity of a single node, nodes with higher computing power will\n",
    "    incur higher costs in terms of dissipating heat. By combining\n",
    "    multiple compute nodes, we can obtain the same computing power in a\n",
    "    more cost-effective way. This drives cloud service providers (such\n",
    "    as Amazon and Microsoft) to focus more on providing distributed\n",
    "    machine learning systems.\n",
    "\n",
    "3.  **Hardware fault protection:** Machine learning training clusters\n",
    "    typically run commodity hardware (such as disks and NICs). As such,\n",
    "    hardware faults are inevitable over long-term operations. In\n",
    "    single-node training, the failure of one hardware device will cause\n",
    "    the entire training job to fail. In distributed training, a training\n",
    "    job is jointly completed by multiple hardware devices. This means\n",
    "    that the system can transfer the workload on the faulty device to a\n",
    "    good one, eliminating concerns that hardware faults will interrupt\n",
    "    training.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}