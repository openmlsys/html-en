{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6410e497",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Pipeline Parallelism with Micro-Batching\n",
    "\n",
    "In addition to data parallelism and model parallelism, pipeline\n",
    "parallelism is another common method for distributed training. Pipeline\n",
    "parallelism is often used in large-scale model-parallel systems that are\n",
    "designed to overcome the memory shortage in individual devices through\n",
    "intra- and inter-operator parallelism. However, when such systems are\n",
    "running, downstream devices in the computational graph remain idle for a\n",
    "long time --- they cannot perform computation until it is completed in\n",
    "upstream devices. Consequently, the average utilization of devices is\n",
    "low. This phenomenon is known as *model parallelism bubbles*.\n",
    "\n",
    "To reduce bubbles, we can build pipelines in training systems.\n",
    "Essentially, this approach divides each mini-batch in the training data\n",
    "into multiple micro-batches. For example, if a mini-batch with $D$\n",
    "training samples is divided into $M$ micro-batches, then each\n",
    "micro-batch has $D/M$ data samples. These micro-batches enter the\n",
    "training system in sequence and go through forward and backward\n",
    "computation. In this way, the system computes a gradient of each\n",
    "micro-batch, and then caches the gradient. After the preceding steps are\n",
    "completed for all micro-batches, the cached gradients are summed to\n",
    "compute the average gradient (equivalent to the gradient of the entire\n",
    "mini-batch) based on which the model parameters are updated.\n",
    "\n",
    "Figure :numref:`ch010/ch10-pipeline-parallel` shows an execution example\n",
    "of a pipeline training system. In this example, model parameters are\n",
    "split among four devices for storage. To fully utilize the four devices,\n",
    "the mini-batch is divided into two micro-batches. It is assumed that\n",
    "$F_{i,j}$ represents the $i$th forward computation task of the $j$th\n",
    "micro-batch, and $B_{i,j}$ represents the $i$th backward computation\n",
    "task of the $j$th micro-batch. After completing the forward computation\n",
    "of the first micro-batch (denoted as $F_{0,0}$), device 1 sends the\n",
    "intermediate result to device 2, which then starts the corresponding\n",
    "forward computation task (denoted as $F_{1,0}$). At the same time,\n",
    "device 1 starts the forward computation task of the second micro-batch\n",
    "(denoted as $F_{0,1}$). Forward computation will be finally completed on\n",
    "the last device in the pipeline, that is, device 3.\n",
    "\n",
    "![Pipeline-parallelsystem](../img/ch10/ch10-pipeline-parallel.png)\n",
    ":label:`ch010/ch10-pipeline-parallel`\n",
    "\n",
    "The system then starts backward computation: Device 4 starts the\n",
    "backward computation task of the first micro-batch (denoted as\n",
    "$B_{3,0}$). On completion, the intermediate result is sent to device 3,\n",
    "which then starts the corresponding backward computation task (denoted\n",
    "as $B_{2,0}$). At the same time, device 4 caches the gradient of the\n",
    "first micro-batch and starts computation for the second micro-batch\n",
    "(denoted as $B_{3,1}$). When device 4 completes all backward\n",
    "computation, it sums the locally cached gradients and then divides the\n",
    "sum by the number of micro-batches to compute the average gradient. This\n",
    "gradient is used to update model parameters.\n",
    "\n",
    "It should be noted that gradient computation often needs the activation\n",
    "values generated in forward computation. In classical model-parallel\n",
    "systems, activation values are cached in memory and directly used during\n",
    "backward computation to avoid repeated computation. In pipeline training\n",
    "systems, however, these values are not cached due to the strain on\n",
    "memory resources. Instead, they are recomputed during backward\n",
    "computation.\n",
    "\n",
    "For optimal performance of pipeline training systems, the sizes of\n",
    "micro-batches need to be debugged. Upon completion of forward\n",
    "computation tasks, devices have to wait until all backward computation\n",
    "tasks start, meaning that the devices remain idle during this period. As\n",
    "shown in Figure\n",
    ":numref:`ch010/ch10-pipeline-parallel`, after completing the two\n",
    "forward computation tasks, device 1 waits for an extended period of time\n",
    "before it can start the two backward computation tasks. This waiting\n",
    "time is referred to as *pipeline bubbles*. To minimize pipeline bubbles,\n",
    "a common practice is to increase the number of micro-batches to the\n",
    "maximum extent so that backward computation can start as early as\n",
    "possible. However, this practice may result in each micro-batch\n",
    "containing insufficient training samples, failing to fully utilize the\n",
    "massive computing cores in hardware accelerators. Therefore, the optimal\n",
    "number of micro-batches is jointly determined by a plurality of factors\n",
    "(such as pipeline depth, micro-batch size, and quantity of computing\n",
    "cores in an accelerator).\n",
    "\n",
    "## Expert Parallelism\n",
    "\n",
    "Mixture of Experts (MoE) models have gained significant popularity due\n",
    "to their ability to effectively scale the size of models. Traditional\n",
    "neural networks typically require a linear increase in computational\n",
    "resources as the number of parameters grows, making extremely large\n",
    "networks prohibitively expensive to train. MoE models address this\n",
    "challenge by specializing different parts of the model---referred to as\n",
    "experts---to handle specific subsets of the data or tasks, thereby\n",
    "enabling a more efficient use of computational resources.\n",
    "\n",
    "MoE models achieve this specialization by dynamically selecting which\n",
    "experts to activate for a given input. This approach allows the model to\n",
    "scale to billions of parameters without a proportional increase in\n",
    "computational demand during inference. In other words, the demand for\n",
    "computational resources grows sub-linearly with the number of\n",
    "parameters. This efficiency makes MoE models particularly attractive for\n",
    "applications that require both high capacity and scalability, such as\n",
    "natural language processing and recommendation systems.\n",
    "\n",
    "An MoE model consists of the following key components: (i) *Experts*:\n",
    "These are individual neural network sub-models, each trained to handle\n",
    "different aspects or subsets of the data. For example, in a language\n",
    "model, one expert might specialize in syntax while another focuses on\n",
    "semantics, (ii) *Gating Network*: This component determines which\n",
    "experts to activate for a given input. The gating network takes the\n",
    "input data and outputs a set of weights or selections that decide the\n",
    "contribution of each expert to the final output, and (iii) *Aggregation\n",
    "Mechanism*: After the experts process the input, their outputs are\n",
    "combined (aggregated) to produce the final result. The aggregation is\n",
    "typically weighted by the gating network's output, allowing the model to\n",
    "emphasize the contributions of the most relevant experts.\n",
    "\n",
    "To parallelize an MoE model, the expert parallelism strategy will follow\n",
    "several key steps: (i)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}