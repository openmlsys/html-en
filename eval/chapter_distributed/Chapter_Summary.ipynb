{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ecc3ff",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Chapter Summary\n",
    "\n",
    "1.  The advent of large-scale machine learning models has sparked an\n",
    "    exponential increase in the need for computational power and memory,\n",
    "    leading to the emergence of distributed training systems.\n",
    "\n",
    "2.  Distributed training systems often utilize data parallelism, model\n",
    "    parallelism, or a combination of both, based on memory limitations\n",
    "    and computational constraints.\n",
    "\n",
    "3.  Pipeline parallelism is another technique adopted by distributed\n",
    "    training systems, which involves partitioning a mini-batch into\n",
    "    micro-batches and overlapping the forward and backward propagation\n",
    "    of different micro-batches.\n",
    "\n",
    "4.  Although distributed training systems usually function in compute\n",
    "    clusters, these networks sometimes lack the sufficient bandwidth for\n",
    "    the transmission of substantial gradients produced during training.\n",
    "\n",
    "5.  To meet the demand for comprehensive communication bandwidth,\n",
    "    machine learning clusters integrate heterogeneous high-performance\n",
    "    networks, such as NVLink, NVSwitch, and InfiniBand.\n",
    "\n",
    "6.  To accomplish synchronous training of a machine learning model,\n",
    "    distributed training systems frequently employ a range of collective\n",
    "    communication operators, among which the AllReduce operator is\n",
    "    popularly used for aggregating the gradients computed by distributed\n",
    "    nodes.\n",
    "\n",
    "7.  Parameter servers play a crucial role in facilitating asynchronous\n",
    "    training and sparse model training. Moreover, they leverage model\n",
    "    replication to address issues related to data hotspots and server\n",
    "    failures.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}