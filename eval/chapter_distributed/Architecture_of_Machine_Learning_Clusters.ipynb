{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b7efc1",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Architecture of Machine Learning Clusters\n",
    "\n",
    "Distributed model training is usually implemented in a compute cluster.\n",
    "Next, we will introduce the composition of a compute cluster and explore\n",
    "the design of a cluster network.\n",
    "\n",
    "Figure :numref:`ch010/ch10-datacentre` shows the typical architecture of\n",
    "a machine learning cluster. There are many servers deployed in such a\n",
    "cluster, and each server has several hardware accelerators. To\n",
    "facilitate server management, multiple servers are placed into one\n",
    "*rack*, which is connected to a *top of rack (ToR) switch*. If ToR\n",
    "switches are fully loaded but more new racks need to be connected, we\n",
    "can add a *spine switch* between ToR switches. Such a structure forms a\n",
    "multi-level tree. It is worth noting that cross-rack communication\n",
    "within a cluster may encounter network bottlenecks. This is because the\n",
    "network links used to construct the cluster network have the same\n",
    "specifications (necessary to facilitate hardware procurement and device\n",
    "management), increasing the probability of *network bandwidth\n",
    "oversubscription* on the network links from the ToR switches to the\n",
    "spine switch.\n",
    "\n",
    "Network bandwidth oversubscription can be defined as a situation wherein\n",
    "the peak bandwidth required exceeds the actual bandwidth available on\n",
    "the network. In the cluster shown in Figure\n",
    ":numref:`ch010/ch10-datacentre`, when server 1 and server 2 send\n",
    "data to server 3 through their respective network links (say 10 Gb/s of\n",
    "data), ToR switch 1 aggregates the data (that is, 20 Gb/s) and sends it\n",
    "to spine switch 1. However, because there is only one network link (10\n",
    "Gb/s) between spine switch 1 and ToR switch 1, the peak bandwidth\n",
    "required is twice the actual bandwidth available, hence network\n",
    "bandwidth oversubscription. In real-world machine learning clusters, the\n",
    "ratio between peak bandwidth and actual bandwidth is generally between\n",
    "1:4 and 1:16. One approach for avoiding network bottlenecks is to\n",
    "restrict network communication within individual racks. This approach\n",
    "has become a core design requirement for distributed machine learning\n",
    "systems.\n",
    "\n",
    "![Architecture of a machine learningcluster](../img/ch10/ch10-datacentre.png)\n",
    ":label:`ch010/ch10-datacentre`\n",
    "\n",
    "So, how much network bandwidth is required for training a large-scale\n",
    "neural network in a compute cluster? Assume a neural network has\n",
    "hundreds of billions of parameters (e.g., GPT-3 --- a huge language\n",
    "model released by OpenAI --- has nearly 175 billion parameters). If each\n",
    "parameter is expressed with a 32-bit floating-point number, a single\n",
    "model replica in data parallelism mode will generate 700 GB (175 billion\n",
    "$*$ 4 bytes) of local gradient data in each round of training iteration.\n",
    "If there are three model replicas, at least 1.4 TB \\[700 GB $*$\n",
    "$(3-1)$\\] of gradient data needs to be transmitted. This is because for\n",
    "$N$ replicas, only $N-1$ of them need to be transmitted for computation.\n",
    "To ensure that the model replicas will not diverge from the parameters\n",
    "in the main model, the average gradient --- once computed --- is\n",
    "broadcast to all model replicas (1.4 TB of data) for updating local\n",
    "parameters in these model replicas.\n",
    "\n",
    "Currently, machine learning clusters generally use Ethernet to construct\n",
    "networks between different racks. The bandwidth of mainstream commercial\n",
    "Ethernet links ranges from 10 Gb/s to 25 Gb/s.Â [^1] Using Ethernet to\n",
    "transmit massive gradients will encounter severe transmission latency.\n",
    "Because of this, new machine learning clusters (such as NVIDIA DGX) are\n",
    "often configured with the faster InfiniBand. A single InfiniBand link\n",
    "can provide 100 Gb/s or 200 Gb/s bandwidth. Even this high-speed network\n",
    "still faces high latency when transmitting TB-level local gradients.\n",
    "Even if network latency is ignored, it takes at least 40 seconds to\n",
    "transmit 1 TB of data on a 200 Gb/s link.\n",
    "\n",
    "To address this issue, InfiniBand uses remote direct memory access\n",
    "(RDMA) as the core of its programming interfaces. RDMA enables\n",
    "InfiniBand to provide high-bandwidth, low-latency data read and write\n",
    "functions. As such, its programming interfaces are vastly different from\n",
    "the TCP/IP socket interfaces used by conventional Ethernet. For\n",
    "compatibility purposes, people use the IP-over-InfiniBand (IPoIB)\n",
    "technology, which ensures that legacy applications can invoke socket\n",
    "interfaces whereas the underlying layer invokes the RDMA interfaces of\n",
    "InfiniBand through IPoIB.\n",
    "\n",
    "To support multiple accelerators (typically 2--16) within a server, a\n",
    "common practice is to build a heterogeneous network on the server. Take\n",
    "server 1 in Figure :numref:`ch010/ch10-datacentre` as an example. This server is\n",
    "equipped with two CPUs, which communicate with each other through\n",
    "QuickPath Interconnect (QPI). Within a CPU interface (socket), the\n",
    "accelerator and CPU are connected by a PCIe bus. Accelerators use\n",
    "high-bandwidth memory (HBM), which offers much more bandwidth than PCIe\n",
    "does. A prominent example is the NVIDIA A100 server: In this server, HBM\n",
    "offers 1935 GB/s bandwidth, whereas PCIe 4.0 offers only 64 GB/s\n",
    "bandwidth. PCIe needs to be shared by all accelerators within the\n",
    "server, meaning that it becomes a significant communication bottleneck\n",
    "when multiple accelerators simultaneously transmit data through PCIe. To\n",
    "solve this problem, machine learning servers tend to use accelerator\n",
    "high-speed interconnect technologies (e.g., NVIDIA GPU NVLink). Such\n",
    "technology bypasses PCIe to achieve high-speed communication. A\n",
    "prominent example is NVIDIA A100 GPU --- its NVLink provides 600 GB/s\n",
    "bandwidth, enabling accelerators to transmit large amounts of data to\n",
    "each other.\n",
    "\n",
    "## AI Cluster Network Topology\n",
    "\n",
    "[^1]: Network bandwidth is typically measured in Gb/s, whereas memory\n",
    "    bandwidth is in GB/s --- *b* stands for bit, and *B* stands for\n",
    "    byte.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}