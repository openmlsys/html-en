{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f0ccdf",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Training Large Language Models\n",
    "\n",
    "## Fully Sharded Data Parallelism\n",
    "\n",
    "Data parallelism is the most versatile strategy for parallelizing deep\n",
    "learning models. However, vanilla data parallelism requires each device\n",
    "to have sufficient memory for accommodating the whole model replica,\n",
    "limiting its applicability in training models with billions of\n",
    "parameters. For example, a 7.5B model already requires 120 GB of memory\n",
    "for its model training states (parameters, gradients, and optimizer\n",
    "states), which is much more than the 80GB of memory in the latest\n",
    "generation of GPUs.\n",
    "\n",
    "Fully sharded data parallelism (FSDP) is a memory-optimized variation of\n",
    "data parallelism. FSDP shards the model training states across multiple\n",
    "accelerators to reduce the memory consumption of each accelerator. As a\n",
    "result, each accelerator holds a shard of training states of multiple\n",
    "layers. In the forward pass, each accelerator needs to introduce an\n",
    "all-gather operation to collect the parameters for the other shards. The\n",
    "parameters from the other shards are discarded immediately once the\n",
    "corresponding computation is finished. In the backward pass, it\n",
    "re-collects parameters with all-gather first, then each accelerator can\n",
    "compute local gradients. The gradients perform reduce-scatter operation\n",
    "to aggregate and redistribute gradients across the accelerators.\n",
    "Finally, each accelerator uses the sharded optimizer states and\n",
    "gradients to update its parameter shard.\n",
    "\n",
    "FSDP also comes with two reduced variants: the first variant shards only\n",
    "the optimizer states while keeping the parameters and gradients as a\n",
    "whole, and the second variant shards also the gradients in addition to\n",
    "the optimizer states. These variants have less communication volume than\n",
    "the full version but have more memory consumption.\n",
    "\n",
    "One of the primary advantages of FDSP is the significant reduction in\n",
    "memory requirements per device, enabling the training of much larger\n",
    "models and batch sizes than vanilla data parallelism. Compared to tensor\n",
    "model parallelism, it requires no code refactoring and has fewer\n",
    "constraints to the layer type and model architecture so that it can be\n",
    "simply applied to a wide range of models. In contrast to pipeline model\n",
    "parallelism, it has no load imbalance issue since all devices are\n",
    "executing the same set of layers.\n",
    "\n",
    "On the other hand, FSDP has communication overheads due to the higher\n",
    "communication volume from gathering parameters in forward and backward\n",
    "phases and reducing gradients, especially in networks with limited\n",
    "bandwidth.\n",
    "\n",
    "## Mixture of Experts\n",
    "\n",
    "Mixture-of-Experts (MoE) is a machine learning technique that has been\n",
    "employed in transformer models to scale model capacity while maintaining\n",
    "constant computation per token during both training and inference.\n",
    "Typically, an MoE layer replaces of the dense feed-forward layer of a\n",
    "transformer model. This layer is composed of a group of trainable expert\n",
    "networks, each of which is feed-forward network (FFN) in practice, along\n",
    "with a gating network that selects a subset of experts for each input\n",
    "token. MoE-based models have deomonstrated comparable accuracy as their\n",
    "dense counterparts, with the same model size, while requiring less\n",
    "computation. Notable examples of large-scale MoE-based transformer\n",
    "models with more than 1 trillion parameters are Switch Transformer and\n",
    "Gshard.\n",
    "\n",
    "Despite MoE capability to scale model size significantly, the memory\n",
    "requirement of MoE-based grows linearly with the model size, making it\n",
    "challenging for a single device to handle. Expert Parallelism is a form\n",
    "of parallelism to address this challenge by distributing subsets of\n",
    "experts across multiple devices. During execution, the gating network\n",
    "assigns tokens to experts and these tokens are dispatched to the devices\n",
    "hosting their assigned experts uasing all-to-all communication. Once the\n",
    "expert computations are complete, the tokens are sent back to their\n",
    "source device via another round of all-to-all communication.\n",
    "\n",
    "From a system perspective, the primary advantage of MoE is its ability\n",
    "to improve computational efficiency and reduce memory consumption. By\n",
    "increasing the number of experts in proportion to the number of devices,\n",
    "the growth of computation and memory requirement per device can be kept\n",
    "nearly constant. However, the major drawback of expert parallelism is\n",
    "the communication overhead due to the two additional rounds of\n",
    "all-to-all communications per MoE layer. This overhead can become a\n",
    "significant bottleneck, especially when communication spans multiple\n",
    "nodes, leading to decreased efficiency. The other drawback comes from\n",
    "the imbalance of expert workloads. The tokens are assigned to experts\n",
    "based on the indeterministic result of the router network, although this\n",
    "imbalance issue can be mitigated with the introduction of auxiliary load\n",
    "balancing loss to encourage a balanced load across experts. However, the\n",
    "load imbalance can still be a significant problem, especially during the\n",
    "early stage of training or working with a small batch size.\n",
    "\n",
    "## Activation Recomputation\n",
    "\n",
    "During the backward pass, the layers need to access the activations\n",
    "generated during the forward pass to compute the gradients. These\n",
    "activations are stored in memory, and for very deep models, the\n",
    "accumulating memory used for the saved activations can consume a\n",
    "considerable amount of the accelerator's memory. For example, each layer\n",
    "of GPT-3 175B model requires approximately 34 GB of activation memory\n",
    "for a batch size of 1.\n",
    "\n",
    "Activation recomputation (or activation checkpointing) is a technique\n",
    "that trades extra computation for memory. Instead of storing all\n",
    "activations during the forward pass, it only stores (or checkpoints) the\n",
    "input activations of a group of layers and recomputes other required\n",
    "activations using an extra forward pass during the backward pass. The\n",
    "activations to be saved can be determined heuristically or optimally\n",
    "using certain computationally intensive optimization algorithms such as\n",
    "mixed integer linear programming.\n",
    "\n",
    "The primary advantage is the substantial reduction in memory usage,\n",
    "allowing for the training of larger models or the use of larger batch\n",
    "sizes. In addition, this technique is not restricted to any type of\n",
    "model architecture and is generally available in most deep learning\n",
    "frameworks.\n",
    "\n",
    "On the contrary, the recomputation introduces computational overheads,\n",
    "which can be very considerable when recomputing the\n",
    "computational-intensive layers. As a result, the saved activations have\n",
    "to be chosen carefully. For transformer architecture, if only the\n",
    "activations at transformer layer boundaries are saved, 30%-40% execution\n",
    "time overhead can be observed. Although some algorithms, such as\n",
    "Checkmate, are proposed to find the optimal recomputation plan, they\n",
    "require a long search time (\\> hours) for large models.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}