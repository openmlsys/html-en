{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea922ce",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Distributed Training\n",
    "\n",
    "As the field of machine learning continues to accelerate at a rapid\n",
    "pace, it has given rise to increasingly sophisticated models. These\n",
    "models are characterized by a staggering quantity of parameters, the\n",
    "gigantic size of a training dataset, and highly sophisticated\n",
    "structures, which in turn place significant demands on both computing\n",
    "and memory resources. Consequently, the limitations of single-machine\n",
    "systems have become increasingly apparent, and they no longer suffice\n",
    "for training these large machine learning models. This necessitates the\n",
    "advent of distributed training systems, designed to alleviate the strain\n",
    "on resources.\n",
    "\n",
    "In this chapter, we dive deep into the fundamentals, design aspects, and\n",
    "practical implementations of distributed machine learning systems. We\n",
    "commence our discussion by elucidating what distributed training systems\n",
    "entail, followed by an exploration of the rationale behind their design\n",
    "and the potential benefits they offer. Subsequently, we scrutinize the\n",
    "most commonly adopted methods of distributed training, encompassing data\n",
    "parallelism, model parallelism, and pipeline parallelism. Each of these\n",
    "methods can typically be implemented via one of two techniques:\n",
    "collective communication or parameter servers, both of which come with\n",
    "their unique sets of merits and drawbacks.\n",
    "\n",
    "The key learning objectives of this chapter are as follows:\n",
    "\n",
    "1.  Grasping the advantages offered by distributed training systems.\n",
    "\n",
    "2.  Understanding widely-used parallelism methods, namely, data\n",
    "    parallelism, model parallelism, hybrid parallelism, and pipeline\n",
    "    parallelism.\n",
    "\n",
    "3.  Comprehending the architecture of a machine learning cluster.\n",
    "\n",
    "4.  Understanding collective communication operators and their\n",
    "    applications in distributed training systems.\n",
    "\n",
    "5.  Developing an understanding of parameter server architectures.\n",
    "\n",
    "```toc\n",
    ":maxdepth: 2\n",
    "\n",
    "Overview\n",
    "Parallelism_Methods\n",
    "Pipeline_Parallelism_with_Micro-Batching\n",
    "Architecture_of_Machine_Learning_Clusters\n",
    "Collective_Communication\n",
    "Parameter_Server\n",
    "Federated_Learning\n",
    "Training_Large_Language_Models\n",
    "Chapter_Summary\n",
    "Further_Reading\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}