{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e25148f",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Components of Hardware Accelerators\n",
    "\n",
    "A hardware accelerator typically comprises multiple on-chip caches and\n",
    "various types of arithmetic units. In this section, we'll examine the\n",
    "fundamental components of hardware accelerators, using the Nvidia Volta\n",
    "GPU architecture as a representative example.\n",
    "\n",
    "## Architecture of Accelerators\n",
    "\n",
    "Contemporary graphics processing units (GPUs) offer remarkable computing\n",
    "speed, ample memory storage, and impressive I/O bandwidth. A top-tier\n",
    "GPU frequently surpasses a conventional CPU by housing double the number\n",
    "of transistors, boasting a memory capacity of 16 GB or greater, and\n",
    "operating at frequencies reaching up to 1 GHz. The architecture of a GPU\n",
    "comprises streaming processors and a memory system, interconnected\n",
    "through an on-chip network. These components can be expanded\n",
    "independently, allowing for customized configurations tailored to the\n",
    "target market of the GPU.\n",
    "\n",
    "Figure :numref:`ch06/ch06-gv100` illustrates the architecture of the\n",
    "Volta GV100 . This architecture has:\n",
    "\n",
    "![Volta GV100](../img/ch06/V100.png)\n",
    ":label:`ch06/ch06-gv100`\n",
    "\n",
    "1.  6 GPU processing clusters (GPCs), each containing:\n",
    "    1.  7 texture processing clusters (TPCs), each containing two\n",
    "        streaming multiprocessors (SMs).\n",
    "    2.  14 SMs.\n",
    "2.  84 SMs, each containing:\n",
    "    1.  64 32-bit floating-point arithmetic units\n",
    "    2.  64 32-bit integer arithmetic units\n",
    "    3.  32 64-bit floating-point arithmetic units\n",
    "    4.  8 Tensor Cores\n",
    "    5.  4 texture units\n",
    "3.  8 512-bit memory controllers.\n",
    "\n",
    "As shown in Figure :numref:`ch06/ch06-gv100`, a GV100 GPU contains 84 SMs (Streaming\n",
    "Multiprocessors), 5376 32-bit floating-point arithmetic units, 5376\n",
    "32-bit integer arithmetic units, 2688 64-bit floating-point arithmetic\n",
    "units, 672 Tensor Cores, and 336 texture units. A pair of memory\n",
    "controllers controls an HBM2 DRAM stack. Different vendors may use\n",
    "different configurations (e.g., Tesla V100 has 80 SMs).\n",
    "\n",
    "## Memory Units\n",
    "\n",
    "The memory units of a hardware accelerator resemble a CPU's memory\n",
    "controller. However, they encounter a bottleneck when retrieving data\n",
    "from the computer system's DRAM, as it is slower compared to the\n",
    "processor's computational speed. Without a cache for quick access, the\n",
    "DRAM bandwidth becomes inadequate to handle all transactions of the\n",
    "accelerator. Consequently, if program instructions or data cannot be\n",
    "swiftly retrieved from the DRAM, the accelerator's efficiency diminishes\n",
    "due to prolonged idle time. To tackle this DRAM bandwidth issue, GPUs\n",
    "employ a hierarchical design of memory units. Each type of memory unit\n",
    "offers its own maximum bandwidth and latency. To fully exploit the\n",
    "computing power and enhance processing speed, programmers must select\n",
    "from the available memory units and optimize memory utilization based on\n",
    "varying access speeds.\n",
    "\n",
    "1.  **Register file**: Registers serve as the swiftest on-chip memories.\n",
    "    In contrast to CPUs, each SM in a GPU possesses tens of thousands of\n",
    "    registers. Nevertheless, excessively utilizing registers for every\n",
    "    thread can result in a reduced number of thread blocks that can be\n",
    "    scheduled within the SM, leading to fewer executable threads. This\n",
    "    underutilization of hardware capabilities hampers performance\n",
    "    considerably. Consequently, programmers must judiciously determine\n",
    "    the appropriate number of registers to employ, taking into account\n",
    "    the algorithm's demands.\n",
    "\n",
    "2.  **Shared memory**: The shared memory is a level-1 cache that is\n",
    "    user-controllable. Each SM features a 128 KB level-1 cache, with the\n",
    "    ability for programmers to manage up to 96 KB as shared memory. The\n",
    "    shared memory offers a low access latency, requiring only a few\n",
    "    dozen clock cycles, and boasts an impressive bandwidth of up to 1.5\n",
    "    TB/s. This bandwidth is significantly higher than the peak bandwidth\n",
    "    of the global memory, which stands at 900 GB/s. In high-performance\n",
    "    computing (HPC) scenarios, engineers must possess a thorough\n",
    "    understanding of how to leverage shared memory effectively.\n",
    "\n",
    "3.  **Global memory**: Both GPUs and CPUs are capable of reading from\n",
    "    and writing to global memory. Global memory is visible and\n",
    "    accessible by all threads on a GPU, whereas other devices like CPUs\n",
    "    need to traverse buses like PCIe and NV-Link to access the global\n",
    "    memory. The global memory represents the largest memory space\n",
    "    available in a GPU, with capacities reaching over 80 GB. However, it\n",
    "    also exhibits the longest memory latency, with a load/store latency\n",
    "    that can extend to hundreds of clock cycles.\n",
    "\n",
    "4.  **Constant memory**: The constant memory is a virtual address space\n",
    "    in the global memory and does not occupy a physical memory block. It\n",
    "    serves as a high-speed memory, specifically designed for rapid\n",
    "    caching and efficient broadcasting of a single value to all threads\n",
    "    within a warp.\n",
    "\n",
    "5.  **Texture memory**: Texture memory is a specialized form of global\n",
    "    memory that is accessed through a dedicated texture cache to enhance\n",
    "    performance. In earlier GPUs without caches, the texture memory on\n",
    "    each SM served as the sole cache for data. However, the introduction\n",
    "    of level-1 and level-2 caches in modern GPUs has rendered the\n",
    "    texture memory's role as a cache obsolete. The texture memory proves\n",
    "    most beneficial in enabling GPUs to execute hardware-accelerated\n",
    "    operations while accessing memory units. For instance, it allows\n",
    "    arrays to be accessed using normalized addresses, and the retrieved\n",
    "    data can be automatically interpolated by the hardware.\n",
    "    Additionally, the texture memory supports both hardware-accelerated\n",
    "    bilinear and trilinear interpolation for 2D and 3D arrays,\n",
    "    respectively. Moreover, the texture memory facilitates automatic\n",
    "    handling of boundary conditions based on array indices. This means\n",
    "    that operations on array elements can be carried out without\n",
    "    explicit consideration of boundary situations, thus avoiding the\n",
    "    need for extra conditional branches in a thread.\n",
    "\n",
    "## Compute Units\n",
    "\n",
    "Hardware accelerators offer a variety of compute units to efficiently\n",
    "handle various neural networks.\n",
    "Figure :numref:`ch06/ch06-compute-unit` demonstrates how different\n",
    "layers of neural networks select appropriate compute units.\n",
    "\n",
    "![Computeunits](../img/ch06/compute_unit.png)\n",
    ":label:`ch06/ch06-compute-unit`\n",
    "\n",
    "1.  **Scalar Unit**: calculates one scalar element at a time, similar to\n",
    "    the standard reduced instruction set computer (RISC).\n",
    "\n",
    "2.  **1D Vector Unit**: computes multiple elements at a time, similar to\n",
    "    the SIMD used in traditional CPU and GPU architectures. It has been\n",
    "    widely used in HPC and signal processing.\n",
    "\n",
    "3.  **2D Matrix Unit**: computes the inner product of a matrix and a\n",
    "    vector or the outer product of a vector within one operation. It\n",
    "    reuses data to reduce communication costs and memory footprint,\n",
    "    which achieves the performance of matrix multiplication.\n",
    "\n",
    "4.  **3D Cube Unit**: completes matrix multiplication within one\n",
    "    operation. Specially designed for neural network applications, it\n",
    "    can reuse data to compensate for the gap between the data\n",
    "    communication bandwidth and computing.\n",
    "\n",
    "The compute units on a GPU mostly include Scalar Units and 3D Cube\n",
    "Units. As shown in Figure :numref:`ch06/ch06-SM`, each SM has 64 32-bit floating-point\n",
    "arithmetic units, 64 32-bit integer arithmetic units, 32 64-bit\n",
    "floating-point arithmetic units, which are Scalar Units, and 8 Tensor\n",
    "Cores, which are 3D Cube Units specially designed for neural network\n",
    "applications.\n",
    "\n",
    "![Volta GV100 SM](../img/ch06/SM.png)\n",
    ":label:`ch06/ch06-SM`\n",
    "\n",
    "A Tensor Core is capable of performing one $4\\times4$ matrix\n",
    "multiply-accumulate operation per clock cycle, as shown in\n",
    "Figure :numref:`ch06/ch06-tensorcore`.\n",
    "\n",
    "```\n",
    "D = A * B + C\n",
    "```\n",
    "\n",
    "![Tensor Core's $4\\times4$ matrix multiply-accumulateoperation](../img/ch06/tensor_core.png)\n",
    ":label:`ch06/ch06-tensorcore`\n",
    "\n",
    "$\\bf{A}$, $\\bf{B}$, $\\bf{C}$, and $\\bf{D}$ are $4\\times4$ matrices.\n",
    "Input matrices $\\bf{A}$ and $\\bf{B}$ are FP16 matrices, and accumulation\n",
    "matrices $\\bf{C}$ and $\\bf{D}$ can be either FP16 or FP32 matrices.\n",
    "Tesla V100's Tensor Cores are programmable matrix multiply-accumulate\n",
    "units that can deliver up to 125 Tensor Tera Floating-point Operations\n",
    "Per Second (TFLOPS) for training and inference applications, resulting\n",
    "in a ten-fold increase in computing speed when compared with common FP32\n",
    "compute units.\n",
    "\n",
    "## Domain Specific Architecture\n",
    "\n",
    "![Da Vinciarchitecture](../img/ch06/davinci_architecture.png)\n",
    ":label:`ch06/ch06-davinci_architecture`\n",
    "\n",
    "Domain Specific Architecture (DSA) has been an area of interest in\n",
    "meeting the fast-growing demand for computing power by deep neural\n",
    "networks. As a typical DSA design targeting image, video, voice, and\n",
    "text processing, neural network processing units (or namely deep\n",
    "learning hardware accelerators) are system-on-chips (SoCs) containing\n",
    "special compute units, large memory units, and the corresponding control\n",
    "units. A neural processing unit, for example, Ascend chip, typically\n",
    "consists of a control CPU, a number of AI computing engines, multi-level\n",
    "on-chip caches or buffers, and the digital vision pre-processing (DVPP)\n",
    "module.\n",
    "\n",
    "The computing core of AI chips is composed of AI Core, which is\n",
    "responsible for executing scalar- and tensor-based arithmetic-intensive\n",
    "computing. Consider the Ascend chip as an example. Its AI Core adopts\n",
    "the Da Vinci   architecture.\n",
    "Figure :numref:`ch06/ch06-davinci_architecture` shows the architecture\n",
    "of an AI Core, which can be regarded as a simplified version of modern\n",
    "microprocessor architecture from the control perspective. It includes\n",
    "three types of basic computing units: Cube Unit, Vector Unit, and Scalar\n",
    "Unit. These units are used to compute on tensors, vectors, and scalars,\n",
    "respectively, in three independent pipelines centrally scheduled through\n",
    "the system software to coordinate with each other for higher efficiency.\n",
    "Similar to GPU designs, the Cube Unit functions as the computational\n",
    "core of the AI Core and delivers parallel acceleration for matrix\n",
    "multiply-accumulate operations. Specifically, it can multiply two\n",
    "$16\\times16$ matrices in a single instruction --- equivalent to\n",
    "completing 4096 (=$16\\times16\\times16$) multiply-accumulate operations\n",
    "within an extremely short time --- with precision comparable to FP16\n",
    "operations.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}