{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b69dbf",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Performance Optimization Methods\n",
    "\n",
    "Hardware accelerators boast intricate computational and memory\n",
    "architectures. To maximize their performance, developers frequently need\n",
    "to grasp a variety of performance optimization methods. Common methods\n",
    "encompass enhancing arithmetic intensity, capitalizing effectively on\n",
    "shared memory, optimizing the memory load/store pipeline, among others.\n",
    "The subsequent sections will elucidate these methods through practical\n",
    "programming examples, all aimed towards a singular objective:\n",
    "accelerating an FP32 GEMM program.\n",
    "\n",
    "## Implementing General Matrix Multiplication\n",
    "\n",
    "Code `lst:cpu` shows a reference implementation of GEMM in C++.\n",
    "\n",
    "**lst:cpu**\n",
    "\n",
    "```cpp\n",
    "float A[M][K];\n",
    "float B[K][N];\n",
    "float C[M][N];\n",
    "float alpha, beta;\n",
    "\n",
    "for (unsigned m = 0; m < M; ++m) {\n",
    "    for (unsigned n = 0; n < N; ++n) {\n",
    "        float c = 0;\n",
    "        for (unsigned k = 0; k < K; ++k) {\n",
    "            c += A[m][k] * B[k][n];\n",
    "        }\n",
    "        C[m][n] = alpha * c + beta * C[m][n];\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "ach element in matrix $C$ is independently computed, and numerous GPU\n",
    "threads can be launched to compute the corresponding elements in matrix\n",
    "$C$ in parallel. The GPU kernel function is shown in\n",
    "Code `lst:gpu`.\n",
    "\n",
    "**lst:gpu**\n",
    "\n",
    "```cpp\n",
    "__global__ void gemmKernel(const float * A,\n",
    "const float * B, float * C,\n",
    "float alpha, float beta, unsigned M, unsigned N,\n",
    "unsigned K) {\n",
    "    unsigned int m = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "    unsigned int n = threadIdx.y + blockDim.y * blockIdx.y;\n",
    "    if (m >= M || n >= N)\n",
    "    return;\n",
    "    float c = 0;\n",
    "    for (unsigned k = 0; k < K; ++k) {\n",
    "        c += A[m * K + k] * B[k * N + n];\n",
    "    }\n",
    "    c = c * alpha;\n",
    "    float result = c;\n",
    "    if (beta != 0) {\n",
    "        result = result + C[m * N + n] * beta;\n",
    "    }\n",
    "    C[m * N + n] = result;\n",
    "}\n",
    "```\n",
    "\n",
    "Figure :numref:`cuda_naive_gemm` shows the layout of the implementation.\n",
    "Each element in matrix $C$ is computed by one thread. The row index $m$\n",
    "and column index $n$ of the element in matrix $C$ corresponding to the\n",
    "thread are computed in lines 5 and 6 of the GPU kernel. Then, in lines 9\n",
    "to 11, the thread loads the row vector in matrix $A$ according to the\n",
    "row index and the column vector in matrix $B$ according to the column\n",
    "index, computes the vector inner product. The thread also stores the\n",
    "result back to $C$ matrix in line 17.\n",
    "\n",
    "![Simple implementation ofGEMM](../img/ch06/practise/naive.png)\n",
    ":label:`cuda_naive_gemm`\n",
    "\n",
    "The method of launching the kernel function is shown in\n",
    "Code `lst:launch`.\n",
    "\n",
    "**lst:launch**\n",
    "\n",
    "```cpp\n",
    "void gemmNaive(const float *A, const float *B, float *C,\n",
    "float alpha, float beta, unsigned M,\n",
    "unsigned N, unsigned K) {\n",
    "    dim3 block(16, 16);\n",
    "    dim3 grid((M - 1) / block.x + 1, (N - 1) / block.y + 1);\n",
    "    \n",
    "    gemmKernel<<<grid, block>>>(A, B, C, alpha, beta, M, N, K);\n",
    "}\n",
    "```\n",
    "\n",
    "Each thread block processes $16\\times16$ elements in matrix $C$.\n",
    "Therefore, $(M - 1) / 16 + 1 \\times (N - 1) / 16 + 1$ thread blocks are\n",
    "used to compute the entire matrix $C$.\n",
    "\n",
    "Eigen is used to generate data and compute the GEMM result on the CPU.\n",
    "In addition, error computing and time profiling code are implemented for\n",
    "the GPU computing result. For details, see\n",
    "[first_attempt.cu](https://github.com/openmlsys/openmlsys-cuda/blob/main/first_attempt.cu).\n",
    "After the program is compiled and executed, output results are as\n",
    "follows:\n",
    "\n",
    "```\n",
    "Average time: 48.961 ms\n",
    "Max error: 0.000092\n",
    "```\n",
    "\n",
    "The peak GPU throughput can be approximated by using the following\n",
    "formula: 2 $\\times$ Frequency $\\times$ Number of single-precision\n",
    "compute units. The number of single-precision compute units equals the\n",
    "number of SMs in the GPU multiplied by the number of single-precision\n",
    "compute units in each SM. The results are as follows:\n",
    "\n",
    "```\n",
    "FP32 peak throughput 29767.680 GFLOPS\n",
    "Average Throughput: 185.313 GFLOPS\n",
    "```\n",
    "\n",
    "A significant gap exists between the performance that can be achieved by\n",
    "the current code and the peak device performance. In an entire computing\n",
    "process, the process with the highest computing density is matrix\n",
    "multiplication $A\\times B$. Its time complexity is $O(M*N*K)$, whereas\n",
    "that time complexity of the entire computing process is\n",
    "$O(M*N*K+2*M*N)$. Therefore, optimizing matrix multiplication is key to\n",
    "improving performance.\n",
    "\n",
    "## Enhancing Arithmetic Intensity\n",
    "\n",
    "Arithmetic intensity is the ratio of computational instructions to\n",
    "load/store instructions. Modern GPUs typically have numerous compute\n",
    "units, constrained only by a limited load/store bandwidth. This\n",
    "limitation often leaves these units waiting for data loading in a\n",
    "program. Thus, boosting arithmetic intensity is a crucial step to\n",
    "improve program performance.\n",
    "\n",
    "In the GPU kernel function discussed previously, we can approximate its\n",
    "arithmetic intensity by dividing the total number of floating-point\n",
    "operations by the number of data reads. When calculating the inner\n",
    "product within $K$ loops, floating-point multiplication and addition\n",
    "operations occur each time elements from matrix $A$ and $B$ are loaded.\n",
    "Consequently, the arithmetic intensity is 1, derived from two 32-bit\n",
    "floating-point operations divided by two 32-bit data load/store\n",
    "instructions.\n",
    "\n",
    "In the original code, each thread handles one element in matrix $C$,\n",
    "computing the inner product of a row in matrix $A$ and a column in\n",
    "matrix $B$. In essence, we can elevate the arithmetic intensity by\n",
    "amplifying the elements in matrix $C$ that each thread can process,\n",
    "computing the inner product of multiple rows in matrix $A$ and multiple\n",
    "columns in matrix $B$. More specifically, if $m$ elements in matrix $A$\n",
    "and $n$ elements in matrix $B$ are loaded concurrently while calculating\n",
    "the inner product in $K$ loops, there are $m+n$ 32-bit load/store\n",
    "instructions and $2mn$ 32-bit computational instructions. Hence, the\n",
    "arithmetic intensity becomes $\\frac{2mn}{m+n}$. Therefore, by increasing\n",
    "$m$ and $n$, we can optimize the arithmetic intensity.\n",
    "\n",
    "In the preceding section, a `float` pointer was employed to access\n",
    "global memory and store data in it, utilizing the hardware instructions\n",
    "`LDG.E` and `STG.E`. Multiple `float` elements can be loaded\n",
    "concurrently using the 128-bit wide instructions `LDG.E.128` and\n",
    "`STG.E.128`. These wide instructions can streamline the instruction\n",
    "sequence, potentially saving dozens of instruction issue cycles compared\n",
    "to four standard instructions, thereby enabling the issue of more\n",
    "computational instructions within the saved time. Wide instructions can\n",
    "also enhance the cache line hit rate. Despite these benefits, we advise\n",
    "against the blanket use of wide instructions in all code. Instead,\n",
    "programmers should prioritize direct optimization methods, such as\n",
    "parallel design and local data reuse.\n",
    "\n",
    "A specific implementation is stacking four `float` numbers to form a\n",
    "128-bit `float4` class. The load/store operations will be completed\n",
    "using a wide instruction for the `float4` class. For details about the\n",
    "code implementation, see\n",
    "[util.cuh](https://github.com/openmlsys/openmlsys-cuda/blob/main/util.cuh).\n",
    "\n",
    "Note that each thread needs to load four `float` numbers (instead of\n",
    "one) from matrix $A$ and matrix $B$, requiring each thread to process\n",
    "$4\\times 4$ blocks (`thread tile`) in matrix $C$. Each thread loads data\n",
    "from matrix $A$ and matrix $B$ from left to right and from top to\n",
    "bottom, computes the data, and stores the data to matrix $C$, as shown\n",
    "in Figure :numref:`use_float4`.\n",
    "\n",
    "![Enhancing arithmeticintensity](../img/ch06/practise/use_float4.png)\n",
    ":label:`use_float4`\n",
    "\n",
    "For details about the complete code, see\n",
    "[gemm_use_128.cu](https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm_use_128.cu).\n",
    "We can further increase the amount of data processed by each thread in\n",
    "order to improve the arithmetic intensity more, as shown in\n",
    "Figure :numref:`use_tile`. For\n",
    "details about the code used to achieve this, see\n",
    "[gemm_use_tile.cu](https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm_use_tile.cu).\n",
    "\n",
    "![Further enhancement of the arithmetic intensity by adding matrixblocks processed by eachthread](../img/ch06/practise/use_tile.png)\n",
    ":label:`use_tile`\n",
    "\n",
    "The test results are as follows:\n",
    "\n",
    "```\n",
    "Max Error: 0.000092\n",
    "Average Time: 6.232 ms, Average Throughput: 1378.317 GFLOPS\n",
    "```\n",
    "\n",
    "To sample and analyze performance indicators, we will use the analysis\n",
    "tool Nsight Compute released by NVIDIA. This tool, designed for GPU\n",
    "kernel functions, samples and collects GPU activity data by hooking\n",
    "drivers. The following commands can be used to analyze the performance:\n",
    "\n",
    "```\n",
    "bash\n",
    "ncu --set full -o <profile_output_file> <profile_process>\n",
    "```\n",
    "\n",
    "`–set full` indicates that all data is sampled. `-o` indicates that the\n",
    "result is output as a file. `<profile_output_file>` indicates the output\n",
    "file name without the file name extension. `<profile_process>` indicates\n",
    "the executable file to be analyzed and its arguments. For example, to\n",
    "analyze `first_attempt` and name the output result\n",
    "`first_attepmt_prof_result`, run the following instructions:\n",
    "\n",
    "```\n",
    "ncu --set full -o first_attepmt_prof_result ./first_attempt\n",
    "```\n",
    "\n",
    "If the system displays a message indicating that you do not have\n",
    "permission to run this command, prefix it with `sudo` and run it again.\n",
    "After obtaining the output file, the program `nv-nsight-cu` can be used\n",
    "to view the file. We compared the profiling results of the new GPU\n",
    "kernel function and the previous one.\n",
    "\n",
    "The result shows that the number of `LDG` instructions decreases by 84%,\n",
    "and the value of `Stall LG Throttle` decreases by 33%. By using wide\n",
    "instructions to increase the compute density, we are able to reduce the\n",
    "number of global load/store instructions, thereby cutting the amount of\n",
    "time needed to wait before issuing instructions. The improvement on\n",
    "`Arithmetic Intensity` proves that our analysis of the arithmetic\n",
    "intensity is correct. The gemm_use_tile.cu test results are as follows:\n",
    "\n",
    "```\n",
    "Max Error: 0.000092\n",
    "Average Time: 3.188 ms, Average Throughput: 2694.440 GFLOPS\n",
    "```\n",
    "\n",
    "The analysis using Nsight Compute shows that the code can also improve\n",
    "other indicators, such as `Stall LG Throttle`.\n",
    "\n",
    "## Caching Data in Shared Memory\n",
    "\n",
    "By increasing the amount of data that a thread can load in one go, we\n",
    "can improve the arithmetic intensity and performance. However, this\n",
    "method decreases the degree of parallelism because it reduces the total\n",
    "number of enabled threads. Other hardware features need to be exploited\n",
    "in order to improve performance without compromising the degree of\n",
    "parallelism. In earlier code, several thread blocks are enabled, each of\n",
    "which processes one or more matrix blocks in matrix $C$. As shown in\n",
    "Figure :numref:`duplicated_data`, thread $x$ and thread $y$ process the same\n",
    "row in matrix $C$, so they load the same data from matrix $A$. The\n",
    "shared memory can be used to improve the program throughput by enabling\n",
    "different threads in the same thread block to load unique data and reuse\n",
    "shared data.\n",
    "\n",
    "![Threads loading redundantdata](../img/ch06/practise/duplicated_data.png)\n",
    ":label:`duplicated_data`\n",
    "\n",
    "We have previously mentioned that the inner product can be computed by\n",
    "loading and accumulating data in $K$ loops. Specifically, in each loop,\n",
    "threads that process the same row in matrix $C$ load the same data from\n",
    "matrix $A$, and threads that process the same column in matrix $C$ load\n",
    "the same data from matrix $B$. However, the code needs to be optimized\n",
    "by dividing $K$ loops into $\\frac{K}{tileK}$ outer loops and $tileK$\n",
    "inner loops. In this way, an entire block of data is loaded in each\n",
    "outer loop and accumulated in each inner loop.\n",
    "Figure :numref:`use_smem_store` shows the process of moving data from the\n",
    "global memory to the shared memory. Before each inner loop starts, the\n",
    "entire `tiles` in matrix $A$ and matrix $B$ is stored in the shared\n",
    "memory.\n",
    "\n",
    "Figure :numref:`use_smem_load` shows the process of moving data from the\n",
    "shared memory to the register. In each inner loop, data is loaded from\n",
    "the shared memory and computed. An advantage of this design is that each\n",
    "thread does not need to load all the data it requires from the global\n",
    "memory. Instead, the entire thread block loads the data required for all\n",
    "threads from the global memory and stores the data in the shared memory.\n",
    "During computational processes, each thread only needs to load the data\n",
    "it requires from the shared memory.\n",
    "\n",
    "![Writing data to the sharedmemory](../img/ch06/practise/use_smem_store.png)\n",
    ":label:`use_smem_store`\n",
    "\n",
    "![Loading data from the sharedmemory](../img/ch06/practise/use_smem_load.png)\n",
    ":label:`use_smem_load`\n",
    "\n",
    "For details about the complete code, see\n",
    "[gemm_use_smem.cu](https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm_use_smem.cu).\n",
    "\n",
    "The test results are as follows:\n",
    "\n",
    "```\n",
    "Max Error: 0.000092\n",
    "Average Time: 0.617 ms, Average Throughput: 13925.168 GFLOPS\n",
    "```\n",
    "\n",
    "Again, we use Nsight Compute to profile the kernel function and compare\n",
    "the results with the previous ones. The analysis shows some major\n",
    "improvements. Specifically, the number of `LDG` instructions decreases\n",
    "by 97%, which is consistent with this design. And the value of\n",
    "`SM Utilization` increases by 218%, which proves that using the shared\n",
    "memory can reduce the memory access latency and improve the memory\n",
    "utilization. Furthermore, the performance of other indicators such as\n",
    "`Pipe Fma Cycles Active` also improves significantly, demonstrating the\n",
    "benefits of the shared memory.\n",
    "\n",
    "## Reducing Register Usage\n",
    "\n",
    "In previous sections, the data blocks that store matrix $A$ in the\n",
    "shared memory are arranged in a row-first manner, and the shared memory\n",
    "is loaded by row. We can instead adopt a column-first manner in order to\n",
    "reduce loops and loop variables, thereby reducing the number of\n",
    "registers and improving performance.\n",
    "\n",
    "For details about the complete code, see\n",
    "[gemm_transpose_smem.cu](https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm_transpose_smem.cu).\n",
    "\n",
    "The test results are as follows:\n",
    "\n",
    "```\n",
    "Max Error: 0.000092\n",
    "Average Time: 0.610 ms, Average Throughput: 14083.116 GFLOPS\n",
    "```\n",
    "\n",
    "Analysis by Nsight Compute shows that `Occupancy` increases by 1.3%.\n",
    "This is because only 111 registers are used (17 fewer than used by the\n",
    "previous GPU kernel function). The benefit of reducing the number of\n",
    "registers varies depending on the GPU architecture. Observations have\n",
    "shown that the number of `STS` instructions increases and bank conflicts\n",
    "occur, meaning that using fewer registers may not have a positive impact\n",
    "on other GPU architectures.\n",
    "\n",
    "## Hiding Shared Memory Loading Latency\n",
    "\n",
    "To load data from the shared memory, a GPU uses the `LDS` instruction.\n",
    "After issuing this instruction, the GPU will execute the following\n",
    "instructions without waiting for the data to be loaded to the register\n",
    "unless the instructions require such data. In the previous section, each\n",
    "time this instruction is issued during $tileK$ inner loops, the\n",
    "mathematical operation that requires the loaded data is performed\n",
    "immediately. However, the compute unit has to wait for the data to be\n",
    "loaded from the shared memory, as shown in\n",
    "Figure :numref:`use_smem_pipeline`. Accessing the shared memory may take\n",
    "dozens of clock cycles, but computation instructions can often be\n",
    "completed within only a few clock cycles. In order to significantly\n",
    "accelerate memory access, we can hide the shared memory loading latency\n",
    "by optimizing the pipeline. Specifically, during $tileK$ inner loops,\n",
    "loading instructions that prepare data in the next loop can be loaded at\n",
    "the beginning of each loop, as shown in\n",
    "Figure :numref:`hide_smem_latency`. In this way, computation instructions in\n",
    "the current operation do not require the data in the next loop. As such,\n",
    "the execution of these computation instructions will not be blocked by\n",
    "the instructions that load the data for the next loop.\n",
    "\n",
    "![Pipeline of the previous GPU kernelfunction](../img/ch06/practise/use_smem_pipeline.png)\n",
    ":label:`use_smem_pipeline`\n",
    "\n",
    "![Pipeline that hides the shared memory loadinglatency](../img/ch06/practise/hide_smem_latency.png)\n",
    ":label:`hide_smem_latency`\n",
    "\n",
    "For details about the complete code, see\n",
    "[gemm_hide_smem_latency.cu](https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm_hide_smem_latency.cu).\n",
    "\n",
    "The test results are as follows:\n",
    "\n",
    "```\n",
    "Max Error: 0.000092\n",
    "Average Time: 0.585 ms, Average Throughput: 14686.179 GFLOPS\n",
    "```\n",
    "\n",
    "Analysis by Nsight Compute shows that the value of\n",
    "`Stall Short Scoreboard` decreases by 67% when compared with that of the\n",
    "previous GPU kernel function. As mentioned before, after GPU memory\n",
    "load/store instructions are issued, the GPU executes the next\n",
    "instruction without waiting for the data to be landed in the register.\n",
    "However, it will set a flag on the Scoreboard and reset the flag after\n",
    "the data is landed. If instructions that require such data need to be\n",
    "executed, the GPU will execute them only after the data is landed. The\n",
    "decrease of `Stall Short Scoreboard` demonstrates that hiding the access\n",
    "latency of the shared memory is an effective method to better utilize\n",
    "the GPU.\n",
    "\n",
    "## Hiding Global Memory Loading Latency\n",
    "\n",
    "To load data from the global memory, a GPU uses the textttLDG\n",
    "instruction, the behavior of which is similar to the `LDS` instruction\n",
    "used to load data from the shared memory as discussed in the previous\n",
    "section. At the beginning of each of the $\\frac{K}{tileK}$ outer loops,\n",
    "instructions that load the data tiles in matrix $A$ for the next loop\n",
    "are issued. Because this data is not required by any inner loop in a\n",
    "given outer loop, the computational processes in the inner loop will not\n",
    "wait for the read instruction to be completed, thereby hiding the global\n",
    "memory loading latency. We can also enable data in `buffer` to be\n",
    "written to `tile` in the last loop in the inner loop after $tileK - 1$\n",
    "loops are executed, further reducing the latency of writing data to\n",
    "`tile`. Figure :numref:`hide_global_latency` shows the optimized pipeline.\n",
    "\n",
    "![Pipeline that hides the global memory loadinglatency](../img/ch06/practise/hide_global_latency.png)\n",
    ":label:`hide_global_latency`\n",
    "\n",
    "For details about the complete code, see\n",
    "[gemm_final.cu](https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm_final.cu).\n",
    "\n",
    "The test results are as follows:\n",
    "\n",
    "```\n",
    "Max Error: 0.000092\n",
    "Average Time: 0.542 ms, Average Throughput: 15838.302 GFLOPS\n",
    "```\n",
    "\n",
    "Similar to the `Stall Short Scoreboard` results obtained in the previous\n",
    "section, analysis by Nsight Compute shows that the value of\n",
    "`Stall Long Scoreboard` (a global memory indicator) decreases by 67%.\n",
    "Such a significant decrease demonstrates that prefetching data can hide\n",
    "the global memory to reduce the loading latency.\n",
    "\n",
    "## Performance Optimization Principles\n",
    "\n",
    "So far, we have discussed various methods to enhance the performance of\n",
    "an accelerator. Even though other methods exist, the principles of\n",
    "performance optimization generally adhere to the following:\n",
    "\n",
    "-   Increasing parallelism through resource mapping: Multi-level\n",
    "    parallel resources (`blocks`, `warps`, and `threads`) are mapped to\n",
    "    the data needing computation and transfer to enhance program\n",
    "    parallelism.\n",
    "\n",
    "-   Reducing memory access latency through memory structure\n",
    "    optimization: Based on the recognition of data reuse within the same\n",
    "    `block` during computation, the reused data is stored in local\n",
    "    memory (like shared memory and registers) to increase locality.\n",
    "\n",
    "-   Reducing the instruction issue overhead through optimizing\n",
    "    instruction execution: The `#pragma unroll` function is used to\n",
    "    unroll loops in order to improve the degree of parallelism at the\n",
    "    instruction level and reduce logic judgment. The vectorized load\n",
    "    instruction is used to increase bandwidth. For the Ampere\n",
    "    architecture, the maximum vectorized load instruction is\n",
    "    `LDG.E.128`, and the data type for data loading is `float4`.\n",
    "\n",
    "-   Hiding load/store latency by optimizing the memory access pipeline:\n",
    "    In instances where the in-memory data undergoes modifications (such\n",
    "    as the movement of matrix data), we can optimize the memory access\n",
    "    pipeline. This way, the accelerator performs computations during the\n",
    "    intervals between data movement, thereby concealing the latency\n",
    "    associated with data movement.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}