{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b1f92e",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Programming Methods\n",
    ":label:`Programming Principles for Hardware Accelerators`\n",
    "\n",
    "The first two sections of this chapter primarily discuss the\n",
    "significance, ideas, and basic principles behind the design of hardware\n",
    "accelerators. Co-optimization of software and hardware, as an important\n",
    "guiding principle for building efficient AI systems, requires mutual\n",
    "influence and close coupling between software algorithms/stacks and\n",
    "hardware architectures in neural network applications. In order to fully\n",
    "leverage the advantages of accelerators, it is necessary to design a set\n",
    "of programming methods based on the hardware system architecture.\n",
    "\n",
    "## Method Classification\n",
    "\n",
    "Programming methods for hardware accelerators are categorized into three\n",
    "approaches: using high-level computation operators, harnessing\n",
    "primitives for specialized hardware units, and employing low-level\n",
    "assembly languages:\n",
    "\n",
    "1.  **High-level computation operators**: Hardware accelerators often\n",
    "    come equipped with high-level, hardware-accelerated implementations\n",
    "    of operators extensively used in numerical computing and deep\n",
    "    learning. For instance, NVIDIA provides cuBLAS (CUDA Basic Linear\n",
    "    Algebra Subprograms) and cuDNN (CUDA Deep Neural Network library).\n",
    "    These libraries offer developers an accessible way to harness the\n",
    "    power of NVIDIA GPUs without delving into low-level code. These\n",
    "    operators are optimized for efficiency and automatically exploit\n",
    "    specific GPU features, such as Tensor Cores.\n",
    "\n",
    "2.  **Primitives for task-specific hardware units:**: Hardware\n",
    "    accelerators typically feature task-specific hardware units (like\n",
    "    the Tensor Cores in NVIDIA GPUs) engineered to execute\n",
    "    mixed-precision matrix multiplication operations at high speed.\n",
    "    These units have associated programming primitives, such as CUDA's\n",
    "    Warp Matrix Multiply Accumulate (WMMA) and primitives for\n",
    "    loading/unloading tensors on the units.\n",
    "\n",
    "3.  **Low-level assembly languages**: Hardware accelerators also have\n",
    "    low-level assembly language interfaces. For instance, NVIDIA GPUs\n",
    "    offer the PTX ISA (Parallel Thread Execution Instruction Set\n",
    "    Architecture). It provides explicit control over all aspects of GPU\n",
    "    behavior, but it requires a deep understanding of the GPU\n",
    "    architecture and is more challenging to use correctly and\n",
    "    effectively than the high-level interfaces provided by cuBLAS and\n",
    "    cuDNN. PTX code is typically generated by a compiler from a\n",
    "    high-level language like CUDA C++.\n",
    "\n",
    "In essence, the above three methods operate at different levels of\n",
    "abstraction. High-level operators like cuBLAS and cuDNN provide\n",
    "easy-to-use interfaces to powerful hardware-accelerated operations,\n",
    "while the primitives provided by task-specific hardware units provide a\n",
    "more detailed interface to hardware operations, and low-level assembly\n",
    "languages like PTX ISA provide the most detailed, low-level control over\n",
    "accelerator behavior.\n",
    "\n",
    "## Programming Examples\n",
    "\n",
    "We exemplify different programming methods by implementing the General\n",
    "Matrix Multiplication (GEMM) with each approach. The implementation\n",
    "targets an NVIDIA Volta GPU. GEMM follows the equation\n",
    "$\\bf{C} = \\alpha \\bf{A}\\times \\bf{B} + \\beta \\bf{C}$, where\n",
    "$\\bf{A}\\in\\mathbb{R}^{M\\times K}, \\bf{B}\\in\\mathbb{R}^{K\\times N}, \\bf{C}\\in\\mathbb{R}^{M\\times N}$,\n",
    "and $\\alpha$ and $\\beta$ are parameters provided by users.\n",
    "\n",
    "### High-level Computation Operators\n",
    ":label:`sec-accelerator-use-cublas`\n",
    "\n",
    "Using an operator acceleration library directly is the most\n",
    "straightforward method. NVIDIA offers two types of operator libraries:\n",
    "cuBLAS and cuDNN. cuBLAS provides an interface for leveraging Tensor\n",
    "Cores to accelerate GEMM operations, while cuDNN offers an interface to\n",
    "hasten neural network operations. To utilize Tensor Cores via cuBLAS\n",
    "doing GEMM, we can use function `cublasGemmEx`, its signature is shown\n",
    "in Code `lst:cublasGemmEx`.\n",
    "\n",
    "**lst:cublasGemmEx**\n",
    "\n",
    "```cpp\n",
    "cublasStatus_t cublasGemmEx(cublasHandle_t handle, cublasOperation_t transa, cublasOperation_t transb, int m, int n, int k, const void *alpha, const void *A, cudaDataType_t Atype, int lda, const void *B, cudaDataType_t Btype, int ldb, const void *beta, void *C, cudaDataType_t Ctype, int ldc, cublasComputeType_t computeType, cublasGemmAlgo_t algo)\n",
    "```\n",
    "\n",
    "`handle` is the cuBLAS handle, which is created using the `cublasCreate`\n",
    "function. `transa` denotes whether the matrices $\\bf{A}$ and $\\bf{C}$\n",
    "are transposed, while `transb` denotes whether the matrix $\\bf{B}$ is\n",
    "transposed. `m`, `n`, and `k` are used to describe the shape of the\n",
    "matrices. `alpha` and `beta` are used to scale the matrix multiplication\n",
    "results. `A`, `B`, and `C` are pointers to the starting addresses of the\n",
    "matrices. `Atype`, `Btype`, and `Ctype` describe the data type of the\n",
    "matrices. For example, `CUDA_R_16F` indicates that the data is stored in\n",
    "real 16-bit floating point type. `lda`, `ldb`, and `ldc` represent the\n",
    "leading dimensions of the matrices. `computeType` is the data type used\n",
    "in computation. For instance, `CUBLAS_COMPUTE_16F` implies the use of\n",
    "Tensor Cores for computation in 16-bit floating point. Notably, if the\n",
    "input data type is 32-bit float, we can use\n",
    "`CUBLAS_COMPUTE_32F_FAST_16F` to perform the computation in 16-bit\n",
    "floating point and achieve acceleration using Tensor Cores. `algo` is\n",
    "the algorithm used in computation, and `CUBLAS_GEMM_DEFAULT` is commonly\n",
    "used to select the default algorithm.\n",
    "\n",
    "### Primitives for Hardware Units\n",
    "\n",
    "The second approach to accelerator programming involves the use of\n",
    "programming primitives, such as invoking the CUDA Warp Matrix Multiply\n",
    "Accumulate (WMMA) API on a device. This approach hinges on the\n",
    "collaborative design of software and hardware, meaning that the design\n",
    "of programming APIs at this level is architecture-dependent. For\n",
    "instance, in the Volta architecture, the control object of WMMA is a\n",
    "$16\\times16$ matrix block, processed by two Tensor Cores at a time. This\n",
    "notion is tightly linked to the integration of Tensor Cores into a SM.\n",
    "\n",
    "In the Volta architecture, NVIDIA offers three distinct sizes of WMMA\n",
    "multiply-accumulate computing interfaces for FP16 input data:\n",
    "$16\\times16\\times16$, $32\\times8\\times16$, and $8\\times32\\times16$.\n",
    "\n",
    "The basic control unit of the WMMA API is a fragment, which refers to a\n",
    "template class that specifies information such as the meaning of\n",
    "matrices (multiplier or accumulator), matrix shape\n",
    "(`WMMA_M, WMMA_N, or WMMA_K`), data type (FP16, FP32, etc.), and layout\n",
    "(`row_major or col_major`).\n",
    "Code `lst:frament` shows the fragment types.\n",
    "\n",
    "**lst:frament**\n",
    "\n",
    "```\n",
    "wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;\n",
    "wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> b_frag;\n",
    "wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> acc_frag;\n",
    "wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;\n",
    "```\n",
    "\n",
    "The data of the matrix block required by multiplication operations needs\n",
    "to be loaded to the register as a fragment. Fragments are initialized or\n",
    "cleared after multiply-accumulate operations performed by Tensor Cores,\n",
    "the fragments are stored back in global memory. NVIDIA provides the\n",
    "`wmma.load_matrix_sync() and wmma.store_matrix_sync()` interfaces to\n",
    "load or write the submatrix blocks. The `wmma.fill_fragment()` interface\n",
    "is used to initialize the data of the corresponding fragments, and the\n",
    "`wmma.mma_sync()` interface is used to perform multiply-accumulate\n",
    "operations on fragments.\n",
    "\n",
    "### Low-level Assembly Language Interface\n",
    "\n",
    "The PTX ISA offers another programming interface, for example, the\n",
    "`mma.sync.aligned.m8n8k4` instruction in the Volta architecture. This\n",
    "instruction uses the shape configuration of $M=8, N=8, K=4$ to perform\n",
    "multiply-add operations. The basic control unit of the API is the data\n",
    "element. The matrix size (modifier `.m8n8k4`), data format (modifier\n",
    "`.row` or `.col`) and data formats of input accumulator D, matrix A,\n",
    "matrix B, and output accumulator C (modifier `.f32` or `.f16`) need to\n",
    "be specified. NVIDIA's documentation provides information about\n",
    "using the PTX instruction set, helping programmers compile code based on\n",
    "the corresponding syntax rules, as shown in\n",
    "Code `lst:ptx`.\n",
    "\n",
    "**lst:ptx**\n",
    "\n",
    "```cpp\n",
    "half_t *a, *b;\n",
    "    float *C, *D;\n",
    "    unsigned const* A = reinterpret_cast<unsigned const*>(a);\n",
    "    unsigned const* B = reinterpret_cast<unsigned const*>(b);\n",
    "\n",
    "    asm volatile(\n",
    "        \"mma.sync.aligned.m8n8k4.row.row.f32.f16.f16.f32 \"\n",
    "        \"{%0,%1,%2,%3,%4,%5,%6,%7}, {%8,%9}, {%10,%11}, \"\n",
    "        \"{%12,%13,%14,%15,%16,%17,%18,%19};\\n\"\n",
    "        : \"=f\"(D[0]), \"=f\"(D[1]), \"=f\"(D[2]), \"=f\"(D[3]), \"=f\"(D[4]),\n",
    "        \"=f\"(D[5]), \"=f\"(D[6]), \"=f\"(D[7])\n",
    "        : \"r\"(A[0]), \"r\"(A[1]), \"r\"(B[0]), \"r\"(B[1]), \"f\"(C[0]),\n",
    "        \"f\"(C[1]), \"f\"(C[2]), \"f\"(C[3]), \"f\"(C[4]), \"f\"(C[5]),\n",
    "        \"f\"(C[6]), \"f\"(C[7]));\n",
    "```\n",
    "\n",
    "Data elements are directly used as the input (`unsigned` type is used\n",
    "for containing FP16 data elements). Moreover, NVIDIA provides the\n",
    "`ldmatrix` instruction to load data from the shared memory to fragments.\n",
    "\n",
    "A finer-grained instruction, `mma`, can form a warp-level WMMA API of\n",
    "more diversified shapes to control the mapping between threads and data\n",
    "in the warp. The PTX instructions offer greater flexibility than\n",
    "directly using CUDA C++ codes.\n",
    "\n",
    "[^1]: available at\n",
    "    <https://docs.nvidia.com/cuda/inline-ptx-assembly/index.html>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}