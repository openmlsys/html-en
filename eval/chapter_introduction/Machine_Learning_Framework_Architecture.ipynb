{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8d873d",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Machine Learning Framework Architecture\n",
    "\n",
    "Figure :numref:`intro/framework-architecture` shows the basic\n",
    "architecture of a typical, complete machine learning framework.\n",
    "\n",
    "![Architecture of a machine learningframework](../img/intro/framework-architecture.png)\n",
    ":label:`intro/framework-architecture`\n",
    "\n",
    "1.  **Programming interfaces:** A machine learning framework needs to\n",
    "    provide programming interfaces, usually those of high-level\n",
    "    programming languages (like Python), to cater for the diversified\n",
    "    backgrounds of machine learning developers. At the same time, the\n",
    "    framework also needs to support a system implementation that is\n",
    "    mainly based on low-level programming languages (e.g., C and C++) so\n",
    "    that operating system features (e.g., thread management and network\n",
    "    communication) and various hardware accelerators can be utilized\n",
    "    efficiently for optimized performance.\n",
    "\n",
    "2.  **Computational graph:** Machine learning applications, though\n",
    "    implemented through different programming interfaces, need to share\n",
    "    the same backend when the applications run. The computational graph\n",
    "    technology is key to realizing this backend. A computational graph,\n",
    "    which defines a user's machine learning application, includes many\n",
    "    graph nodes that represent computational operations. These nodes are\n",
    "    connected by edges, which represent computational dependencies.\n",
    "\n",
    "3.  **Compiler frontend:** Once a computational graph is built, the\n",
    "    machine learning framework analyzes and optimizes it (or the\n",
    "    corresponding application) through the compiler frontend. The\n",
    "    compiler frontend provides key functions such as intermediate\n",
    "    representation, automatic differentiation, type derivation, and\n",
    "    static analysis.\n",
    "\n",
    "4.  **Compiler backend and runtime:** After analyzing and optimizing the\n",
    "    computational graph, the machine learning framework uses the\n",
    "    compiler backend and runtime to optimize different types of\n",
    "    underlying hardware. In addition to optimizing the selection or\n",
    "    scheduling sequence of operators, common optimization technologies\n",
    "    usually analyze the L2/L3 cache size and the instruction pipeline\n",
    "    length to match hardware specifications.\n",
    "\n",
    "5.  **Heterogeneous processors:** A machine learning application is\n",
    "    co-executed by central processing units (CPUs) and hardware\n",
    "    accelerators (such as NVIDIA GPUs, Huawei Ascend processors, and\n",
    "    Google TPUs). During the execution, non-matrix operations (e.g.,\n",
    "    complex data preprocessing and computational graph scheduling) are\n",
    "    handled by CPUs, whereas matrix operations and certain frequently\n",
    "    used machine learning operators (e.g., Transformer operators and\n",
    "    convolution operators) are performed by hardware accelerators.\n",
    "\n",
    "6.  **Data processing:** A machine learning application needs to perform\n",
    "    complex preprocessing on raw data and manage a large number of\n",
    "    training, validation, and test datasets. The data processing module\n",
    "    (e.g., the tf.data module of TensorFlow, or the DataLoader module of\n",
    "    PyTorch) is responsible for such data-centered operations.\n",
    "\n",
    "7.  **Model deployment:** In addition to model training, model\n",
    "    deployment is another key function needed in a machine learning\n",
    "    framework. Model compression technologies --- such as model\n",
    "    conversion, quantization, and distillation --- enable us to run\n",
    "    models on hardware with limited memory. It is also necessary to\n",
    "    optimize model operators for specific hardware inference platforms\n",
    "    (e.g., NVIDIA Orin). Furthermore, in order to ensure the security of\n",
    "    a model (e.g., to deny unauthorized user reads), model obfuscation\n",
    "    must be considered in the framework's design.\n",
    "\n",
    "8.  **Distributed training:** A machine learning model is usually\n",
    "    trained in parallel on distributed compute nodes. Common parallel\n",
    "    training methods include data parallelism, model parallelism, hybrid\n",
    "    parallelism, and pipeline parallelism, all of which are usually\n",
    "    implemented through the remote procedure call (RPC), collective\n",
    "    communication, or parameter server.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}