{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6cfc88a",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Multi-agent Reinforcement Learning System\n",
    "\n",
    "The preceding examples help explain the role of reinforcement learning\n",
    "in multi-agent problems. At present, cutting-edge multi-agent\n",
    "reinforcement learning algorithms are capable of solving large-scale\n",
    "complex multi-agent problems. For example, in games like StarCraft II\n",
    "and Dota 2, AlphaStar  (an agent developed by DeepMind) and OpenAI Five \n",
    "(an agent developed by OpenAI) already surpass the top human players.\n",
    "Chinese companies such as Tencent and Inspir.ai have also proposed their\n",
    "multi-agent reinforcement learning solutions TStarBot-X  and StarCraft\n",
    "Commander (SCC)  for StarCraft II. For such a highly complex gaming\n",
    "environment, the entire training process --- which may be divided into\n",
    "multiple phases --- raises extremely high requirements on distributed\n",
    "computing systems. Take AlphaStar as an example. It combines supervised\n",
    "learning and reinforcement learning in agent training. In order to\n",
    "quickly gain good capabilities during the early stage of training,\n",
    "supervised learning often resorts to using a large amount of annotated\n",
    "data provided by professional human players. Once this is achieved, the\n",
    "training then switches to the reinforcement learning process, and the\n",
    "fictitious self-play algorithm mentioned earlier comes into play (i.e.,\n",
    "self-game). To obtain the best-performing agent among the numerous\n",
    "candidates, the algorithm needs to explore the entire strategy space. In\n",
    "this way, instead of training one individual strategy, it trains a\n",
    "league (i.e., strategy cluster). And by filtering the league in a manner\n",
    "similar to evolutionary algorithms, it finds the best-performing\n",
    "strategy. As shown in\n",
    "Figure :numref:`ch011/ch11-marl-train`, each agent needs to play games\n",
    "with exploiters and other agents. An *exploiter* represents the best\n",
    "response strategy confronting a specific agent strategy --- such\n",
    "confrontation helps improve the anti-exploitation capability of the\n",
    "agent strategy. The act of training and filtering numerous agent\n",
    "strategies is called *league training* (or *population-based training*),\n",
    "which aims to improve strategy population diversity through distributed\n",
    "training in order to attain higher model performance. In practice, this\n",
    "type of method relies on a distributed system to implement multi-agent\n",
    "training and gaming, reflecting the dependency of multi-agent\n",
    "reinforcement learning on distributed computing.\n",
    "\n",
    "![Multi-agent reinforcement learning training in the form of aleague](../img/ch11/ch11-marl-train.pdf)\n",
    ":label:`ch011/ch11-marl-train`\n",
    "\n",
    "Now, let's look at the difficulties involved in building a multi-agent\n",
    "reinforcement learning system in the following sections.\n",
    "\n",
    "## Curse of Multi-agent\n",
    "\n",
    "This difficulty refers to the complexity caused by multiple agents. The\n",
    "most direct change from a single-agent system to a multi-agent one is\n",
    "that the number of agents changes from 1 to more than 1. For an\n",
    "$N$-agent system where each agent is independent of others, this change\n",
    "will lead to an exponential increase in the complexity of the strategy\n",
    "representation space, that is, $\\tilde{O}(e^N)$. Take a single-agent\n",
    "system with a discrete strategy space as an example. If the size of the\n",
    "state space is $S$, that of the action space is $A$, and the game step\n",
    "is $H$, then the size of the discrete strategy space is $O(HSA)$. If we\n",
    "extend the game to an $N$-player game, the joint distribution space of\n",
    "all player strategies in the most general case --- all players have a\n",
    "symmetric action space (size is $A$) and do not share any structure\n",
    "information --- will be $O(HSA^N)$ in size. This is because the strategy\n",
    "space of each independent player is multiplied to form this joint space,\n",
    "specifically, $\\mathcal{A}=\\mathcal{A}_1\\times\\dots\\mathcal{A}_N$. As a\n",
    "direct consequence, the algorithm search complexity increases.\n",
    "\n",
    "To address this issue, the original single-agent system needs to be\n",
    "extended to a multi-agent system for strategy optimization. This means\n",
    "that each parallel module in the single-agent distributed system needs\n",
    "to be extended for each agent in the multi-agent system. In complex\n",
    "cases, there are still many other factors to take into account, for\n",
    "example, communications and heterogeneity between agents --- sometimes\n",
    "different agents are represented by models that are not completely\n",
    "symmetrical and may use different algorithms for optimization.\n",
    "\n",
    "## Complex Game Types\n",
    "\n",
    "From the perspective of game theory, multi-agent systems are associated\n",
    "with complex game types, for example, games can be directly classified\n",
    "as competitive, cooperative, or mixed. In competitive games, the most\n",
    "typical one is a two-player zero-sum game, such as the\n",
    "rock-paper-scissors game discussed in the previous section. In such\n",
    "games, the Nash equilibrium strategy is generally a mixed strategy. That\n",
    "is, equilibrium cannot be achieved through a single pure strategy;\n",
    "however, pure-strategy Nash equilibrium does exist in a few zero-sum\n",
    "games. In cooperative games, multiple agents need to cooperate in order\n",
    "to improve the overall reward. Related research typically adopts the\n",
    "value decomposition approach to assign the rewards obtained by all\n",
    "agents to individual agents. Typical algorithms include\n",
    "value-decomposition network (VDN) , counterfactual multi-agent (COMA) ,\n",
    "and QMIX .\n",
    "\n",
    "In mixed games, some agents may cooperate with each other while others\n",
    "or sets of agents compete with each other. Usually non-zero-sum and\n",
    "non-pure-cooperative games are mixed games. An example is the prisoner's\n",
    "dilemma (see Table :numref:`ch11-marl-prison` for its reward values), in which two\n",
    "prisoners (players) each have two actions, silence and betrayal. The\n",
    "absolute value of the reward is the number of years that each prisoner\n",
    "will be sentenced to. Because the sum of the reward values is not a\n",
    "constant, prisoner's dilemma is a non-zero-sum game. And given that one\n",
    "prisoner may choose silence while the other chooses betrayal (in that\n",
    "case, the former gets a reward of --3 and the latter gets 0), it cannot\n",
    "be considered a pure competitive or pure cooperative game. In a\n",
    "cooperation strategy, both prisoners choose silence and each gets a\n",
    "reward of --1. Although this strategy seems better than others, it is\n",
    "not the game's Nash equilibrium strategy, which assumes that all player\n",
    "strategies are separate and cannot form a joint distribution,\n",
    "prohibiting information communication and potential cooperation between\n",
    "players. As such, the Nash equilibrium strategy of the prisoner's\n",
    "dilemma is that both players choose to betray.\n",
    "\n",
    "In consideration of such games, single-agent reinforcement learning\n",
    "cannot be directly used to optimize the strategy of each agent in a\n",
    "multi-agent system. Single-agent reinforcement learning is generally a\n",
    "process of finding an extremum, whereas solving the Nash equilibrium\n",
    "strategy of a multi-agent system is to find the maximum-minimum (i.e.,\n",
    "saddle points). The two types of systems also differ from an\n",
    "optimization perspective. Complex relationships need to be represented\n",
    "by a more generalized system --- this poses another challenge to the\n",
    "construction of a multi-agent system. There are also other types of\n",
    "games in multi-agent systems, including single-round or multi-round\n",
    "games, simultaneous decision or sequential decision, etc. Each type\n",
    "corresponds to different algorithms, but the existing multi-agent\n",
    "systems are targeted at a specific game type or algorithm. We are in\n",
    "urgent need of generalized multi-agent reinforcement learning systems,\n",
    "especially distributed systems.\n",
    "\n",
    ":Rewards in the prisoner's dilemma\n",
    "\n",
    "|          |   Silence   |   Betrayal |\n",
    "|----------| ------------| ------------ |\n",
    "| Silence  | (--1, --1)  |  (--3, 0) |\n",
    "| Betrayal |   (0, --3)  |  (--2, --2) |\n",
    ":label:`ch11-marl-prison`\n",
    "\n",
    "\n",
    "## Algorithm Heterogeneity\n",
    "\n",
    "From the simple multi-agent algorithms described earlier, such as\n",
    "self-play and fictitious self-play, we can conclude that multi-agent\n",
    "algorithms sometimes involve multiple rounds of the single-agent\n",
    "reinforcement learning process and that algorithms vary according to\n",
    "game types. For cooperative games, many algorithms are based on the idea\n",
    "of credit assignment. Central to these algorithms is the ability to\n",
    "properly assign the rewards obtained by multiple agents to individual\n",
    "agents. According to the execution mode employed, such algorithms fall\n",
    "into three categories: centralized training centralized execution,\n",
    "centralized training decentralized execution, and decentralized training\n",
    "decentralized execution. These categories describe the uniformity of\n",
    "different agents' training and execution processes. For competitive\n",
    "games, various approximation methods (including *fictitious self-play*,\n",
    "*double oracle*, and *mirror descent*) are used to compute the Nash\n",
    "equilibrium. A single agent's attempt to obtain a single optimal\n",
    "strategy through reinforcement learning is regarded as an *action*, and\n",
    "the Nash equilibrium needs to be approximated on the meta-problem\n",
    "composed of these actions. Due to the significant differences between\n",
    "how existing algorithms handle similar problems, building a unified\n",
    "multi-agent reinforcement learning system is difficult.\n",
    "\n",
    "## Hybrid Methods\n",
    "\n",
    "In work such as AlphaStar , reinforcement learning algorithms alone\n",
    "cannot obtain a good strategy in a multi-agent system; instead, we need\n",
    "to employ other learning methods, such as imitation learning. For\n",
    "example, we can create labeled training samples from the gaming records\n",
    "of top human players in order to pre-train agents. Due to the complexity\n",
    "of these large-scale games, such a method allows us to quickly improve\n",
    "the performance of agents in the early stage of training. In terms of\n",
    "the learning system as a whole, a combination of different learning\n",
    "paradigms is required, for example, reasonably switching between\n",
    "imitation learning and reinforcement learning. This means that we cannot\n",
    "consider the large-scale multi-agent system as solely a reinforcement\n",
    "learning system; instead, it is one that requires cooperation between\n",
    "many other learning and coordination mechanisms.\n",
    "\n",
    "Figure :numref:`ch011/ch11-marl-sys` shows an example of a distributed\n",
    "multi-agent reinforcement learning system. Only two agents are shown for\n",
    "simplicity, but the system can be extended to multiple agents. Each\n",
    "agent has multiple actors for sampling and multiple learners for model\n",
    "updating. These actors and learners can be processed in parallel to\n",
    "accelerate the training process. For details, see the A3C and IMPALA\n",
    "architectures described earlier. Trained models are uniformly stored and\n",
    "managed for symmetrical agents --- if agents are asymmetrical, their\n",
    "models need to be stored separately. Models in the memory are scored by\n",
    "the model evaluator, which is a prerequisite for the model selector to\n",
    "work. Based on the output of the model evaluator --- or meta-learner,\n",
    "such as *policy space response oracle* (PSRO) --- and equilibrium\n",
    "solver, the model selector selects a model and distributes it to the\n",
    "actors of each agent. This process is known as *league-based\n",
    "management*. In terms of interaction with the environment, the\n",
    "distributed system uses an inference server to perform centralized\n",
    "inference on the model in each parallel process. To be specific, the\n",
    "inference server sends actions (based on observations) to the\n",
    "environment, which then performs parallel processing of these actions\n",
    "before returning observations, with the interaction trajectories\n",
    "collected and sent by the inference server to each agent for model\n",
    "training. While this example describes a distributed multi-agent system\n",
    "in general terms, there may be many different designs for different game\n",
    "types and algorithm structures in real-world applications.\n",
    "\n",
    "![Distributed multi-agent reinforcement learningsystem](../img/ch11/ch11-marl-sys.png)\n",
    ":label:`ch011/ch11-marl-sys`\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}