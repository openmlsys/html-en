{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3876da72",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Multi-agent Reinforcement Learning\n",
    "\n",
    "Previous sections discussed reinforcement learning involving only one\n",
    "agent. However, researchers are becoming increasingly interested in\n",
    "multiagent reinforcement learning. Consider the framework of\n",
    "single-agent reinforcement learning shown in Figure\n",
    ":numref:`ch011/ch11-rl`. This framework considers the impact of\n",
    "only a single agent's action on the environment, and the reward feedback\n",
    "from the environment applies only to this agent. If we extend the\n",
    "single-agent mode to multiple agents, we have at least two multiagent\n",
    "reinforcement learning frameworks, as shown in Figure\n",
    ":numref:`ch011/ch11-marl`. Figure\n",
    ":numref:`ch011/ch11-marl`(a) shows a scenario where multiple\n",
    "agents perform actions at the same time. The agents are unable to\n",
    "observe actions of other agents, and their actions have an overall\n",
    "impact on the environment. Each agent receives an individual reward for\n",
    "its actions. Figure :numref:`ch011/ch11-marl`(b) shows a scenario where multiple\n",
    "agents perform actions in sequence. Each agent can observe the actions\n",
    "of its previous agents. Their actions have an overall impact on the\n",
    "environment. Each agent receives an individual or team reward. Aside\n",
    "from these two frameworks, other frameworks may involve a more complex\n",
    "mechanism of observations, communications, cooperation, and competition\n",
    "among agents. The simplest situation is to assume that the agent\n",
    "observations are the environment states. However, this is the least\n",
    "possible in the real world. In practice, agents usually have different\n",
    "observations on the environment.\n",
    "\n",
    "![Two possible multiagent reinforcement learning frameworks: (a)Synchronous multiagent decision-making; (b) Asynchronous multiagentdecision-making](../img/ch11/ch11-marl.pdf)\n",
    ":label:`ch011/ch11-marl`\n",
    "\n",
    "## Multi-agent RL\n",
    "\n",
    "Based on the Markov decision process used in single-agent reinforcement\n",
    "learning, we can define that used in multiagent reinforcement learning\n",
    "as a tuple\n",
    "$(\\mathcal{S}, N, \\boldsymbol{\\mathcal{A}}, \\mathbf{R}, \\mathcal{T}, \\gamma)$.\n",
    "In the tuple, $N$ indicates the number of agents, and $\\mathcal{S}$ and\n",
    "$\\boldsymbol{\\mathcal{A}}=(\\mathcal{A}_1, \\mathcal{A}_2, ..., \\mathcal{A}_N)$\n",
    "are the environment state space and the multiagent action space,\n",
    "respectively, where $A_i$ is the action space of the $i$th agent.\n",
    "$\\mathbf{R}=(R_1, R_2, ..., R_N)$ is the multiagent reward function.\n",
    "$\\mathbf{R}(s,\\mathbf{a})$:\n",
    "$\\mathcal{S}\\times \\boldsymbol{\\mathcal{A}}\\rightarrow \\mathbb{R}^N$\n",
    "denotes the reward vector with respect to the state $s\\in\\mathcal{S}$\n",
    "and multiagent action $\\mathbf{a}\\in\\boldsymbol{\\mathcal{A}}$, and $R_i$\n",
    "is the reward for the $i$th agent. The probability of transitioning from\n",
    "the current state and action to the next state is defined as\n",
    "$\\mathcal{T}(s^\\prime|s,\\mathbf{a})$:\n",
    "$\\mathcal{S}\\times\\boldsymbol{\\mathcal{A}}\\times\\mathcal{S}\\rightarrow \\mathbb{R}_+$.\n",
    "$\\gamma\\in (0,1)$ is the reward discount factor[^1]. In addition to\n",
    "maximizing the expected cumulative reward\n",
    "$\\mathbb{E}[\\sum_t \\gamma^t r^i_t], i\\in[N]$ for each agent, multiagent\n",
    "reinforcement learning involves other objectives such as reaching Nash\n",
    "equilibrium or maximizing the team reward --- these do not form part of\n",
    "single-agent reinforcement learning.\n",
    "\n",
    "We can therefore conclude that multiagent reinforcement learning is more\n",
    "complex than single-agent reinforcement learning, and that its\n",
    "complexity is not simply the accumulation of each agent's decision\n",
    "complexity. Closely related to a classical research topic named Game\n",
    "theory, the research of multiagent systems has a long history --- even\n",
    "before reinforcement learning became popular. There was significant\n",
    "research into such systems and many open theoretical problems existed. A\n",
    "typical one is that Nash equilibrium is unsolvable in a two-player\n",
    "non-zero-sum game[^2]. We will not delve too deeply into such problems\n",
    "due to limited space. Instead, we will provide a simple example to\n",
    "explain why a multiagent learning problem cannot be directly solved\n",
    "using a single-agent reinforcement learning algorithm.\n",
    "\n",
    "## Game Example\n",
    "\n",
    "Consider the rock-paper-scissors game. In this game, the win-lose\n",
    "relationship is scissors \\< rock \\< paper \\< scissors\\... `<` means that\n",
    "the latter pure strategy wins over the previous one, and a reward of --1\n",
    "or +1 is given to the two players, respectively. If both players choose\n",
    "the same pure strategy, they are rewarded 0. The payoff table of the\n",
    "game is provided in Table :numref:`ch11-marl`. The horizontal and vertical headings\n",
    "indicate the strategies of Player 1 and Player 2, respectively. The\n",
    "arrays in the table are the players' rewards for their actions.\n",
    "\n",
    ":Payoff table of the rock-paper-scissors game\n",
    "\n",
    "|  Reward  |  Scissors  |    Rock    |    Paper |\n",
    "|----------| -----------| -----------| ----------- |\n",
    "| Scissors |    (0,0)   |  (--1, +1) |  (+1, --1) |\n",
    "|   Rock   |  (+1, --1) |    (0,0)   |  (--1, +1) |\n",
    "|  Paper   |  (--1, +1) |  (+1, --1) |    (0,0) |\n",
    ":label:`ch11-marl`\n",
    "\n",
    "Due to the antisymmetric nature of this matrix, the Nash equilibrium\n",
    "strategy is the same for both players, with a strategy distribution of\n",
    "$(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$. This means that both players\n",
    "have a $\\frac{1}{3}$ probability of choosing paper, rock, or scissors.\n",
    "If we treat the Nash equilibrium strategy as the objective of multiagent\n",
    "reinforcement learning, we can conclude that this strategy cannot be\n",
    "obtained simply through single-agent reinforcement learning. Assume that\n",
    "we randomly initialize two players for any two pure strategies. For\n",
    "example, Player 1 chooses scissors, and Player 2 chooses rock. Also\n",
    "assume that the strategy of Player 2 is fixed. As such, the strategy of\n",
    "Player 2 can be considered as a part of the environment. This allows us\n",
    "to use single-agent reinforcement learning to improve the strategy of\n",
    "Player 1 in order to maximize its reward. In this case, Player 1\n",
    "converges to the pure strategy of paper. If we then fix this strategy\n",
    "for Player 1 to train Player 2, Player 2 converges to the pure strategy\n",
    "of scissors. In this way, Player 1 and Player 2 enter a cycle of three\n",
    "strategies, but neither of them can obtain the correct Nash equilibrium\n",
    "strategy.\n",
    "\n",
    "## Self-play\n",
    "\n",
    "The learning method used in the preceding example is called *self-play*,\n",
    "as shown in Figure :numref:`ch011/ch11-marl-sp`. It is one of the most basic among\n",
    "the multiagent reinforcement learning methods. In self-play, given the\n",
    "fixed strategy of Player 1, the strategy of Player 2 is optimized by\n",
    "maximizing its own reward using single-agent learning methods. The\n",
    "strategy, referred to as best response strategy, is then fixed for\n",
    "Player 2 to optimize the strategy of Player 1. In this manner, the cycle\n",
    "repeats indefinitely. In some cases, however, self-play may fail to\n",
    "converge to the objective we expect. Due to the possible existence of\n",
    "such a cycle, we need training methods that are more complex and methods\n",
    "that are designed for multiagent learning to achieve our objective.\n",
    "\n",
    "![Self-playalgorithm](../img/ch11/ch11-marl-sp.png)\n",
    ":label:`ch011/ch11-marl-sp`\n",
    "\n",
    "Generally, multiagent reinforcement learning is more complex than\n",
    "single-agent reinforcement learning. In self-play, a single-agent\n",
    "reinforcement learning process may be considered as a subtask of\n",
    "multi-agent reinforcement learning. In the game discussed above, when\n",
    "the strategy of Player 1 is fixed, Player 1 plus the game environment\n",
    "constitute the learning environment of Player 2, which can maximize its\n",
    "reward using single-agent reinforcement learning. Likewise, when the\n",
    "strategy of Player 2 is fixed, Player 1 can perform single-agent\n",
    "reinforcement learning. The cycle repeats indefinitely. This is why\n",
    "single-agent reinforcement learning can be considered as subtasks of\n",
    "multiagent reinforcement learning. Another learning method is\n",
    "*fictitious self-play*, as shown in Figure\n",
    ":numref:`ch011/ch11-marl-fsp`, whereby an agent needs to choose\n",
    "an optimal strategy based on its opponent's historical average\n",
    "strategies, and vice versa. In this manner, players can converge to Nash\n",
    "equilibrium strategy in games like rock-paper-scissors.\n",
    "\n",
    "![Fictitious self-playalgorithm](../img/ch11/ch11-marl-fsp.pdf)\n",
    ":label:`ch011/ch11-marl-fsp`\n",
    "\n",
    "[^1]: Assume that the agents use the same reward discount factor.\n",
    "\n",
    "[^2]: This is regarded as a Polynomial Parity Argument, Directed (PPAD)\n",
    "    version problem. For details, see Settling the Complexity of\n",
    "    Computing Two-Player Nash Equilibria. Xi Chen, et al.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}