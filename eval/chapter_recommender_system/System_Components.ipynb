{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd34a4ce",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# System Components\n",
    "\n",
    "The subsequent sections delve into the functionalities and features of\n",
    "each module within the recommender system's workflow.\n",
    "\n",
    "## Message Queue\n",
    "\n",
    "Message queues enable distributed systems to exchange data\n",
    "asynchronously in the form of messages. In a recommender system, each\n",
    "model is often encapsulated as a microservice, and message queues\n",
    "facilitate the data communication among these microservices. Typical\n",
    "message queue implementations include RabbitMQ, Kafka, and Pulsar.\n",
    "\n",
    "Specifically, a message queue aggregates logs (which include user\n",
    "feedback on recommended results) sent from the client to the server. The\n",
    "data processing module retrieves raw logs from the message queue,\n",
    "sanitizes and transforms them into user features and item features,\n",
    "stores these features in feature repositories, and deposits the derived\n",
    "training samples into another message queue for the training server's\n",
    "utilization.\n",
    "\n",
    "Message queues offer multiple advantages, including enabling message\n",
    "producers (such as client reporting modules) and message consumers (like\n",
    "server data processing modules) to produce and consume data at different\n",
    "rates. For instance, during peak usage times of the recommendation\n",
    "service, the client side might generate a vast amount of feedback. If\n",
    "the data processing module is expected to receive data directly, a large\n",
    "portion of it could be discarded due to the data generation pace\n",
    "surpassing the processing speed. Configuring the data processing module\n",
    "based on peak feedback volume would result in resource waste during\n",
    "off-peak times. Message queues can buffer user logs during peak times,\n",
    "allowing the data processing module to handle them during off-peak\n",
    "times. This ensures user feedback is processed consistently and\n",
    "cost-effectively.\n",
    "\n",
    "## Feature Store\n",
    "\n",
    "Feature stores serve as repositories where features are stored and\n",
    "organized for use in model training and inference services. Typical\n",
    "feature store implementations include those developed by commercial\n",
    "enterprises, such as Amazon SageMaker and Databricks, as well as\n",
    "open-source solutions like Hopsworks and Feast.\n",
    "\n",
    "To generate features, a recommender system has a data processing\n",
    "component that retrieves raw logs from the message queues. These logs\n",
    "might contain gender information, for example, *Male*, *Female*, or\n",
    "*Unknown*. Since these raw features cannot be directly incorporated into\n",
    "a recommendation model, the data processing module performs a simple\n",
    "mapping conversion on the raw features: *Male* $\\rightarrow$ 0, *Female*\n",
    "$\\rightarrow$ 1, and *Unknown* $\\rightarrow$ 2. The transformed features\n",
    "are then stored in the feature store, where they can be accessed by the\n",
    "data processing module or the recommendation model.\n",
    "\n",
    "Figure :numref:`feature store` displays a typical format of a feature\n",
    "store. When the training or inference module needs to access a specific\n",
    "user's features, all necessary features can be retrieved from the\n",
    "feature store using just the user ID.\n",
    "\n",
    "![Example of a featurestore](../img/ch_recommender/feature_store.png)\n",
    ":label:`feature store`\n",
    "\n",
    "One significant advantage of utilizing feature stores is the potential\n",
    "for reusing the data processing module's outputs, thereby reducing the\n",
    "volume of redundant data that needs to be stored. Essentially, each\n",
    "module doesn't need to process raw data independently or maintain a\n",
    "storage system for potentially useful features. A more crucial advantage\n",
    "of feature stores is that they ensure a consistent view of features\n",
    "across all system modules. Consider an extreme scenario where the values\n",
    "for *Male* and *Female* are assigned as 1 and 0 in the training module's\n",
    "database, but as 0 and 1 in the inference module's database. In such a\n",
    "situation, the model inference results could be disastrous.\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "Neural networks, particularly dense ones, are integral to the foundation\n",
    "of recommendation models, owing to their proficiency in discerning\n",
    "intricate and implicit relationships among features. This capability\n",
    "enables the suggestion of items potentially appealing to the users.\n",
    "Multilayer perceptrons (MLPs), a simple form of dense neural networks,\n",
    "have demonstrated their potency in these systems. Renowned companies\n",
    "such as Google   and Meta   incorporate MLPs in their recommendation\n",
    "models, underscoring their efficacy.\n",
    "\n",
    "The compact size of an MLP, a mere few megabytes, ensures manageable\n",
    "storage requirements within a recommendation model. Nevertheless, it\n",
    "should be noted that the extensive matrix multiplication operations\n",
    "inherent to MLPs are computationally demanding and therefore necessitate\n",
    "robust computing power.\n",
    "\n",
    "While MLPs have produced impressive outcomes, the exploration of\n",
    "innovative deep neural network applications in recommendation systems\n",
    "continues unabated. An assortment of sophisticated and elaborate network\n",
    "structures has emerged in recent years, testifying to the relentless\n",
    "drive for improvement. Notably, there have been attempts to utilize\n",
    "Transformer models for recommendation tasks  , hinting at the evolving\n",
    "trajectory of this field.\n",
    "\n",
    "Looking forward, it is anticipated that the size of dense neural\n",
    "networks within recommendation models will inevitably expand, possibly\n",
    "escalating to several gigabytes or even terabytes. This evolution is\n",
    "poised to bring about significant changes, both in terms of the systems'\n",
    "effectiveness and their operational demands.\n",
    "\n",
    "## Embedding Table\n",
    "\n",
    "Embedding tables constitute a crucial component of recommendation\n",
    "systems. They primarily serve to transform discrete feature data that\n",
    "cannot be directly calculated---for instance, user IDs, item IDs,\n",
    "genders, and item categories---into vectors within a high-dimensional\n",
    "space. The structure of an embedding table in a recommendation model\n",
    "bears resemblance to its counterpart in a natural language processing\n",
    "(NLP) model, though there are subtle distinctions.\n",
    "\n",
    "![Structure of a recommendation model](../img/ch_recommender/recommendation_model.png)\n",
    ":label:`recommendation models`\n",
    "\n",
    "In NLP models, the majority of parameters are found within deep neural\n",
    "networks. In contrast, embedding tables carry the bulk of parameters in\n",
    "recommendation models, as illustrated in Figure\n",
    ":numref:`recommendation models`. This discrepancy can be\n",
    "attributed to the plethora of discrete features in recommendation\n",
    "systems, each necessitating an individual embedding item for every\n",
    "possible value.\n",
    "\n",
    "For instance, the gender feature, which can take on the values *Female*,\n",
    "*Male*, or *Unknown*, would require three separate embedding items.\n",
    "Assuming that each item is a 64-dimensional single-precision\n",
    "floating-point vector, a recommendation system catering to 100 million\n",
    "users would result in a user embedding table (where each embedding item\n",
    "corresponds to a user) of $4*64*10^8~=23.8$ GB in size.\n",
    "\n",
    "Moreover, there would also be a requirement for embedding tables for\n",
    "items (with each table entry corresponding to an item) as well as for\n",
    "various user and item features. Taken together, the combined size of\n",
    "these tables could potentially stretch into hundreds of gigabytes, or\n",
    "even tens of terabytes.\n",
    "\n",
    "In contrast, as mentioned earlier, the MLP models that are frequently\n",
    "employed in recommendation systems are relatively small in size. For\n",
    "instance, in a Deep Learning Recommendation Model (DLRM)   trained on\n",
    "the Ali-CCP dataset  , the embedding tables surpass 1.44 GB in size,\n",
    "while the dense neural network only measures approximately 100 KB. This\n",
    "disparity underscores the dominant role of embedding tables in shaping\n",
    "the scale and computational demands of recommendation models.\n",
    "\n",
    "## Parameter Server\n",
    "\n",
    "Recommendation systems commonly employ a parameter server architecture,\n",
    "effectively partitioning computational and storage responsibilities.\n",
    "This is advantageous given that while embedding tables consume a\n",
    "significant portion of a recommendation model's storage space, their\n",
    "computation tends to be sparse.\n",
    "\n",
    "This sparsity results from the piecemeal computation of data in small\n",
    "batches throughout both the training and inference phases. Within the\n",
    "computation of a single batch, only the relevant embedding items are\n",
    "accessed. For example, in a recommender system serving 100 million users\n",
    "and processing 1000 user requests at a time, only about\n",
    "$\\frac{1}{100000}$ of the total embedding items are accessed\n",
    "concurrently.\n",
    "\n",
    "Thus, servers conducting the computations for generating recommendation\n",
    "results---during either the training or inference stages---do not need\n",
    "to store the full set of embedding tables. This separation of concerns\n",
    "offered by the parameter server architecture greatly improves the\n",
    "efficiency of storage and computation resource management in\n",
    "recommendation systems.\n",
    "\n",
    "![Role of parameter servers in a recommender system](../img/ch_recommender/parameter_server_in_recommendation.png)\n",
    ":label:`parameter server in recommendation`\n",
    "\n",
    "The parameter servers in a recommender system have a dual role. They\n",
    "coordinate the training process and also support model inference. During\n",
    "the model inference process, inference servers need to access model\n",
    "parameters stored on parameter servers to compute recommendation\n",
    "results. To minimize delay in the inference process, at least one copy\n",
    "of the model parameters is stored on the parameter servers located in\n",
    "the same data center as the inference server (commonly referred to as an\n",
    "inference data center), as illustrated in Figure\n",
    ":numref:`parameter server in recommendation`.\n",
    "\n",
    "This architectural design also aids in failure recovery. If a data\n",
    "center unexpectedly goes offline and becomes inaccessible, user requests\n",
    "can be rerouted to other operational inference data centers.\n",
    "\n",
    "## Training Server\n",
    "\n",
    "Within a recommender system, a training server is responsible for\n",
    "pulling a batch of data from the message queue and the corresponding\n",
    "embedding items and deep neural network parameters from the parameter\n",
    "servers. This enables the server to generate the recommendation result,\n",
    "calculate the loss, and carry out backpropagation to compute the\n",
    "gradients.\n",
    "\n",
    "The parameter servers, in turn, gather these gradients from all training\n",
    "servers. Following this, they aggregate the gathered gradients to\n",
    "compute the new parameters. This cycle of operations signifies the\n",
    "completion of one round of model training.\n",
    "\n",
    "Given that the parameter servers need to compile the gradients from all\n",
    "training servers, these servers are usually housed in the same data\n",
    "center, also known as the training data center. This setup serves to\n",
    "mitigate the 'straggler problem', where network delays can severely\n",
    "impact training efficiency.\n",
    "\n",
    "## Inference Server\n",
    "\n",
    "Inference servers in a recommender system play the role of receiving\n",
    "users' recommendation requests from the client, pulling necessary model\n",
    "parameters from the parameter servers based on these requests, and\n",
    "obtaining user features and item features from feature stores. Using\n",
    "this information, they calculate the recommendation results.\n",
    "\n",
    "To simplify, this section considers a scenario with only one inference\n",
    "server processing user requests. However, in a large-scale recommender\n",
    "system, the recommendation result is typically produced by a\n",
    "recommendation pipeline composed of multiple models, each running on\n",
    "separate inference servers.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}