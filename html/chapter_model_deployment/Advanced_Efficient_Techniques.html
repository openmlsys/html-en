<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>9.4. Advanced Efficient Techniques &#8212; Machine Learning Systems: Design and Implementation 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.5. Model Inference" href="Model_Inference.html" />
    <link rel="prev" title="9.3. Model Compression" href="Model_Compression.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">9. </span>Model Deployment</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">9.4. </span>Advanced Efficient Techniques</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_model_deployment/Advanced_Efficient_Techniques.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-en">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://openmlsys.github.io/">
                  <i class="fas fa-external-link-alt"></i>
                  中文版
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="Machine Learning Systems: Design and Implementation"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">1. Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">2. Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/Machine_Learning_Applications.html">2.1. Machine Learning Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/Design_Objectives_of_Machine_Learning_Frameworks.html">2.2. Design Objectives of Machine Learning Frameworks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/Machine_Learning_Framework_Architecture.html">2.3. Machine Learning Framework Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/Application_Scenarios_of_Machine_Learning_Systems.html">2.4. Application Scenarios of Machine Learning Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/Book_Organization_and_Intended_Audience.html">2.5. Book Organization and Intended Audience</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_basic/index.html">3. Part I Framework Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_model/index.html">4. Programming Model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_model/Overview.html">4.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_model/Machine_Learning_Workflow.html">4.2. Machine Learning Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_model/Neural_Network_Programming.html">4.3. Neural Network Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_model/Functional_Programming.html">4.4. Functional Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_model/Bridging_Python_and_C_C%2B%2B_Functions.html">4.5. Bridging Python and C/C++ Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_model/Chapter_Summary.html">4.6. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_compiler_frontend/index.html">5. AI Compiler Frontend</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Overview_of_AI_Compilers.html">5.1. Overview of AI Compilers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Overview_of_AI_Compiler_Frontends.html">5.2. Overview of AI Compiler Frontends</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Intermediate_Representation.html">5.3. Intermediate Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Automatic_Differentiation.html">5.4. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Type_Systems_and_Static_Analysis.html">5.5. Type Systems and Static Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Frontend_Compilation_Optimization.html">5.6. Frontend Compilation Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Chapter_Summary.html">5.7. Chapter Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Further_Reading.html">5.8. Further Reading</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_compiler_backend/index.html">6. AI Compiler Backend</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Overview.html">6.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Graph_Optimization.html">6.2. Graph Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Operator_Selection.html">6.3. Operator Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Memory_Allocation.html">6.4. Memory Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Operator_Compiler.html">6.5. Operator Compiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Chapter_Summary.html">6.6. Chapter Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Further_Reading.html">6.7. Further Reading</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_accelerator/index.html">7. Hardware Accelerator</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/Overview.html">7.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/Components_of_Hardware_Accelerators.html">7.2. Components of Hardware Accelerators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/Programming_Methods.html">7.3. Programming Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/Performance_Optimization_Methods.html">7.4. Performance Optimization Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/Chapter_Summary.html">7.5. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed/index.html">8. Distributed Training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Overview.html">8.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Parallelism_Methods.html">8.2. Parallelism Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Pipeline_Parallelism_with_Micro-Batching.html">8.3. Pipeline Parallelism with Micro-Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Architecture_of_Machine_Learning_Clusters.html">8.4. Architecture of Machine Learning Clusters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Collective_Communication.html">8.5. Collective Communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Parameter_Server.html">8.6. Parameter Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Federated_Learning.html">8.7. Federated Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Training_Large_Language_Models.html">8.8. Training Large Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Chapter_Summary.html">8.9. Chapter Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Further_Reading.html">8.10. Further Reading</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">9. Model Deployment</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Overview.html">9.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="Conversion_to_Inference_Model_and_Model_Optimization.html">9.2. Conversion to Inference Model and Model Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="Model_Compression.html">9.3. Model Compression</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">9.4. Advanced Efficient Techniques</a></li>
<li class="toctree-l2"><a class="reference internal" href="Model_Inference.html">9.5. Model Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="Security_Protection_of_Models.html">9.6. Security Protection of Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="Chapter_Summary.html">9.7. Chapter Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="Further_Reading.html">9.8. Further Reading</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">10. Part II Application Scenarios</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender_system/index.html">11. Recommender System</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/Overview.html">11.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/System_Components.html">11.2. System Components</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/Recommendation_Pipeline.html">11.3. Recommendation Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/Model_Update.html">11.4. Model Update</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/Supporting_Real-time_Machine_Learning.html">11.5. Supporting Real-time Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/Chapter_Summary.html">11.6. Chapter Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/Further_Reading.html">11.7. Further Reading</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">12. Reinforcement Learning System</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Overview.html">12.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Introduction_to_Reinforcement_Learning.html">12.2. Introduction to Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Single-Node_Reinforcement_Learning_System.html">12.3. Single-Node Reinforcement Learning System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Distributed_Reinforcement_Learning_System.html">12.4. Distributed Reinforcement Learning System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Multi-agent_Reinforcement_Learning.html">12.5. Multi-agent Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Multi-agent_Reinforcement_Learning_System.html">12.6. Multi-agent Reinforcement Learning System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Chapter_Summary.html">12.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_robot/index.html">13. Robotic System</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_robot/Overview_of_Robotic_Systems.html">13.1. Overview of Robotic Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_robot/Robot_Operating_System.html">13.2. Robot Operating System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_robot/Case_Study.html">13.3. Case Study: Using ROS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_robot/Modern_Robot_Learning.html">13.4. Modern Robot Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_robot/Chapter_Summary.html">13.5. Chapter Summary</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="Machine Learning Systems: Design and Implementation"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">1. Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">2. Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/Machine_Learning_Applications.html">2.1. Machine Learning Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/Design_Objectives_of_Machine_Learning_Frameworks.html">2.2. Design Objectives of Machine Learning Frameworks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/Machine_Learning_Framework_Architecture.html">2.3. Machine Learning Framework Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/Application_Scenarios_of_Machine_Learning_Systems.html">2.4. Application Scenarios of Machine Learning Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/Book_Organization_and_Intended_Audience.html">2.5. Book Organization and Intended Audience</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_basic/index.html">3. Part I Framework Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_model/index.html">4. Programming Model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_model/Overview.html">4.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_model/Machine_Learning_Workflow.html">4.2. Machine Learning Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_model/Neural_Network_Programming.html">4.3. Neural Network Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_model/Functional_Programming.html">4.4. Functional Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_model/Bridging_Python_and_C_C%2B%2B_Functions.html">4.5. Bridging Python and C/C++ Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_model/Chapter_Summary.html">4.6. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_compiler_frontend/index.html">5. AI Compiler Frontend</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Overview_of_AI_Compilers.html">5.1. Overview of AI Compilers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Overview_of_AI_Compiler_Frontends.html">5.2. Overview of AI Compiler Frontends</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Intermediate_Representation.html">5.3. Intermediate Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Automatic_Differentiation.html">5.4. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Type_Systems_and_Static_Analysis.html">5.5. Type Systems and Static Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Frontend_Compilation_Optimization.html">5.6. Frontend Compilation Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Chapter_Summary.html">5.7. Chapter Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend/Further_Reading.html">5.8. Further Reading</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_compiler_backend/index.html">6. AI Compiler Backend</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Overview.html">6.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Graph_Optimization.html">6.2. Graph Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Operator_Selection.html">6.3. Operator Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Memory_Allocation.html">6.4. Memory Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Operator_Compiler.html">6.5. Operator Compiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Chapter_Summary.html">6.6. Chapter Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend/Further_Reading.html">6.7. Further Reading</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_accelerator/index.html">7. Hardware Accelerator</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/Overview.html">7.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/Components_of_Hardware_Accelerators.html">7.2. Components of Hardware Accelerators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/Programming_Methods.html">7.3. Programming Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/Performance_Optimization_Methods.html">7.4. Performance Optimization Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/Chapter_Summary.html">7.5. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed/index.html">8. Distributed Training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Overview.html">8.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Parallelism_Methods.html">8.2. Parallelism Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Pipeline_Parallelism_with_Micro-Batching.html">8.3. Pipeline Parallelism with Micro-Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Architecture_of_Machine_Learning_Clusters.html">8.4. Architecture of Machine Learning Clusters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Collective_Communication.html">8.5. Collective Communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Parameter_Server.html">8.6. Parameter Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Federated_Learning.html">8.7. Federated Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Training_Large_Language_Models.html">8.8. Training Large Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Chapter_Summary.html">8.9. Chapter Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed/Further_Reading.html">8.10. Further Reading</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">9. Model Deployment</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Overview.html">9.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="Conversion_to_Inference_Model_and_Model_Optimization.html">9.2. Conversion to Inference Model and Model Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="Model_Compression.html">9.3. Model Compression</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">9.4. Advanced Efficient Techniques</a></li>
<li class="toctree-l2"><a class="reference internal" href="Model_Inference.html">9.5. Model Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="Security_Protection_of_Models.html">9.6. Security Protection of Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="Chapter_Summary.html">9.7. Chapter Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="Further_Reading.html">9.8. Further Reading</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">10. Part II Application Scenarios</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender_system/index.html">11. Recommender System</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/Overview.html">11.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/System_Components.html">11.2. System Components</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/Recommendation_Pipeline.html">11.3. Recommendation Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/Model_Update.html">11.4. Model Update</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/Supporting_Real-time_Machine_Learning.html">11.5. Supporting Real-time Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/Chapter_Summary.html">11.6. Chapter Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/Further_Reading.html">11.7. Further Reading</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">12. Reinforcement Learning System</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Overview.html">12.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Introduction_to_Reinforcement_Learning.html">12.2. Introduction to Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Single-Node_Reinforcement_Learning_System.html">12.3. Single-Node Reinforcement Learning System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Distributed_Reinforcement_Learning_System.html">12.4. Distributed Reinforcement Learning System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Multi-agent_Reinforcement_Learning.html">12.5. Multi-agent Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Multi-agent_Reinforcement_Learning_System.html">12.6. Multi-agent Reinforcement Learning System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/Chapter_Summary.html">12.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_robot/index.html">13. Robotic System</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_robot/Overview_of_Robotic_Systems.html">13.1. Overview of Robotic Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_robot/Robot_Operating_System.html">13.2. Robot Operating System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_robot/Case_Study.html">13.3. Case Study: Using ROS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_robot/Modern_Robot_Learning.html">13.4. Modern Robot Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_robot/Chapter_Summary.html">13.5. Chapter Summary</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="advanced-efficient-techniques">
<h1><span class="section-number">9.4. </span>Advanced Efficient Techniques<a class="headerlink" href="#advanced-efficient-techniques" title="Permalink to this heading">¶</a></h1>
<p>In addition to standard model compression methods, some advanced
approaches are being developed to accelerate the decoding process of the
large models. These methods include generating specific tokens using
smaller models and the ability to generate multiple tokens in a single
step, resulting in accelerating the decoding process. Furthermore, there
are techniques that utilize the memory hierarchy for high throughput
computation, aiming to decrease memory I/O, and as a result, be more
efficient.</p>
<div class="section" id="speculative-decoding">
<h2><span class="section-number">9.4.1. </span>Speculative Decoding<a class="headerlink" href="#speculative-decoding" title="Permalink to this heading">¶</a></h2>
<p>Speculative decoding is a strategy to speed up the decoding process,
based on insights provided by Leviathan et al. [&#64;leviathan2023fast].</p>
<ol class="arabic simple">
<li><p>Complex modeling tasks frequently encompass simpler subtasks that can
be effectively approximated using more efficient models.</p></li>
<li><p>By combining speculative execution with a unique sampling approach,
it is possible to accelerate exact decoding from larger models. This
is achieved by processing them with the outputs from the
approximation models in parallel.</p></li>
</ol>
<p>Figure <code class="xref std std-numref docutils literal notranslate"><span class="pre">ch-deploy/sd</span></code> is a brief overview of Speculative
Decoding. It involves initially generating a series of tokens using a
draft model, which is a smaller and less complex model. These generated
tokens are then verified in parallel with the target model, which is a
larger model. The tokens that are finally executed in the output are
those that are accepted by the target model from the initial draft
tokens. Additionally, if rejection occurs, one more token is resampled
and generated from the adjusted distribution. If there is no rejection,
an extra token is generated by the target model using the draft tokens
as context.</p>
<figure id="fig:ch-deploy/sd"><figcaption><p>Speculative Decoding Overview</p>
</figcaption></figure><p>To elaborate, the process begins with the draft model generating a
series of <span class="math notranslate nohighlight">\(\gamma\)</span> tokens, denoted as
<span class="math notranslate nohighlight">\(x_1, x_2, ..., x_{\gamma}\)</span>. Subsequently, it preserves the
distributions <span class="math notranslate nohighlight">\(q_{1}(x), q_{2}(x), ..., q_{\gamma}(x)\)</span> of these
tokens for future verification by the target model. These <span class="math notranslate nohighlight">\(\gamma\)</span>
tokens are then inputted into the target model in parallel to calculate
the logits for the respective token combinations
<span class="math notranslate nohighlight">\(p_{1}(x), p_{2}(x), ..., p_{\gamma+1}(x)\)</span>, derived from
<span class="math notranslate nohighlight">\(M_{\text{target}}(\text{prefix} + [x_1 + ... + x_{\gamma}])\)</span>. If
the condition <span class="math notranslate nohighlight">\(q(x) &lt; p(x)\)</span> is met, the token is retained. In
contrast, if not met, the token faces a rejection chance of
<span class="math notranslate nohighlight">\(1 - \frac{p(x)}{q(x)}\)</span>, following which it is reselected from an
adjusted distribution:</p>
<div class="math notranslate nohighlight" id="equation-chapter-model-deployment-advanced-efficient-techniques-0">
<span class="eqno">(9.4.1)<a class="headerlink" href="#equation-chapter-model-deployment-advanced-efficient-techniques-0" title="Permalink to this equation">¶</a></span>\[p'(x) = norm(max(0, p(x) - q(x)))\]</div>
<p>:eqlabel:<code class="docutils literal notranslate"><span class="pre">equ:sd_adjusted</span></code></p>
<p>In the paper [&#64;leviathan2023fast], Leviathan et al. have proved the
correctness of this adjusted distribution for resampling.</p>
<p>Under the assumption that the execution time for a single step of the
Target model is denoted as <span class="math notranslate nohighlight">\(T\)</span>, and that of the draft model as
<span class="math notranslate nohighlight">\(cT\)</span>, where <span class="math notranslate nohighlight">\(0&lt;c\leq1\)</span>. The standard procedure using the
target model to generate <span class="math notranslate nohighlight">\(\gamma + 1\)</span> tokens would require a total
time of <span class="math notranslate nohighlight">\(\gamma T + T\)</span>. In contrast, with speculative decoding,
where <span class="math notranslate nohighlight">\(\gamma + 1\)</span> tokens are produced (<span class="math notranslate nohighlight">\(\gamma\)</span> by the
draft model and one additional by the target model concurrently during
the parallel verification), the time required would be
<span class="math notranslate nohighlight">\(\gamma cT + T\)</span>. If all <span class="math notranslate nohighlight">\(\gamma\)</span> draft tokens are accepted
by the target model and <span class="math notranslate nohighlight">\(c\)</span> is small enough to make
<span class="math notranslate nohighlight">\(cT &lt;&lt; T\)</span>, speculative decoding has the potential to significantly
reduce latency during the decoding process.</p>
<p>To further explain, if we denote <span class="math notranslate nohighlight">\(\alpha = E(\beta)\)</span> where
<span class="math notranslate nohighlight">\(\beta\)</span> is the acceptance rate with a given prefix and
<span class="math notranslate nohighlight">\(E(\beta)\)</span> is a natural measure of how well the draft model can
approximate the target model assuming <span class="math notranslate nohighlight">\(\beta\)</span>s are i.i.d., the
expected number of tokens generated by the speculative process is
<span class="math notranslate nohighlight">\(\frac{1-\alpha^{\gamma+1}}{1-\alpha}\)</span> [&#64;leviathan2023fast].
According to the speculative decoding time for one superstep
<span class="math notranslate nohighlight">\(\gamma cT + T\)</span>, the expected time for generating one token with
speculative decoding is
<span class="math notranslate nohighlight">\(\frac{(c\gamma+1)(1-\alpha)}{1-\alpha^{\gamma+1}}T\)</span>. By choosing
a good <span class="math notranslate nohighlight">\(\gamma\)</span> and a well-aligned efficient draft model meaning
big <span class="math notranslate nohighlight">\(\alpha\)</span> and small <span class="math notranslate nohighlight">\(c\)</span>, the result is desired.</p>
<p>Nevertheless, as the value of <span class="math notranslate nohighlight">\(\gamma\)</span> continues to rise, it
becomes progressively more difficult for a draft model to generate draft
tokens with a high acceptance rate by the target model, especially as
the likelihood of acceptance typically diminishes when <span class="math notranslate nohighlight">\(\gamma\)</span>
exceeds a certain value. In the worst-case scenario, if all draft tokens
generated by the draft model are rejected by the target model, then only
the one token that is resampled from the adjusted distribution will be
decoded following the speculative process. In this situation, the time
spent on generating <span class="math notranslate nohighlight">\(\gamma\)</span> tokens with the draft model
represented as <span class="math notranslate nohighlight">\(\gamma cT\)</span> effectively becomes a complete waste of
time when compared to generating a single token directly with the target
model; in addition, the draft model is consuming the GPU memory.</p>
<p>Therefore, finding the best <span class="math notranslate nohighlight">\(\gamma\)</span> or having a well-designed
draft model that is effectively accepted by the target model is of
importance. There are some strategies that can be employed to address
this issue effectively. For example:</p>
<p><strong>Self-Derived Drafts from Target Models</strong></p>
<p>Is it possible to utilize the target model directly as the draft model,
rather than employing a separate smaller model, which could lead to
increased GPU memory usage? The answer is yes. Similar to the original
approach, the modification involves switching the draft model into the
target model itself, followed by self-verifying these draft tokens. The
advantages of this method are:</p>
<ol class="arabic simple">
<li><p>Since the draft model is almost the same as the target model, it is
sufficiently robust to maintain a stable acceptance rate.</p></li>
<li><p>Only need to keep one model in the GPU memory.</p></li>
</ol>
<p>The challenge now lies in the ability to generate multiple future tokens
in a single decoding step. To achieve this, the concept involves
appending additional concurrent layers to the existing output layer of
the model. Stern et al. first proposed this method in
[&#64;stern2018blockwise].</p>
<p>The training of these extra layers can either start from scratch with
the target model or involve fine-tuning a pre-trained model. This
approach forms the basis of the Medusa [&#64;medusa]. Medusa’s architecture
includes extra “Medusa heads” attached after the last hidden layer. This
design enables the model to generate a range of token candidates in just
one decoding step. Subsequently, these candidates undergo a
self-verification process, and only the accepted tokens are executed.</p>
<p>Other methodologies, such as implementing Knowledge Distillation between
draft and target models, employing multiple draft models instead of just
one, and replacing draft models with retrieval datasets proposed by
researchers are still being investigated to determine their
effectiveness and reliability.</p>
<p>Speculative decoding is an effective technique that uses smaller models
to reduce the overhead caused by larger models. By developing a
well-trained and aligned draft model, the efficiency of the decoding
process can be significantly improved.</p>
</div>
<div class="section" id="flashattention">
<h2><span class="section-number">9.4.2. </span>FlashAttention<a class="headerlink" href="#flashattention" title="Permalink to this heading">¶</a></h2>
<p>FlashAttention is an advanced optimization technique utilizing the
memory hierarchy aimed at enhancing the efficiency of attention
computations in transformer models in terms of memory usage and speed.</p>
<p>Dao et al. were the first to suggest this approach, as indicated in
[&#64;dao2022flashattention]. They noted the absence of <em>IO-awareness</em> – the
consideration of I/O interactions across GPU memory layers – in the
classic Scaled Dot-Product Attention algorithm. To address this, they
introduced FlashAttention, an enhanced version of the attention
algorithm designed to minimize the intensive access to the GPU’s high
bandwidth memory (HBM). This innovation led to significant gains in both
computational speed and throughput.</p>
<p>Figure <code class="xref std std-numref docutils literal notranslate"><span class="pre">ch-deploy/memory</span></code> shows the memory hierarchy with
corresponding bandwidths. The main goal of FlashAttention is to avoid
reading and writing the large attention matrix to and from HBM. And
perform computation in SRAM as much as possible.</p>
<p>The standard Scaled Dot-Product Attention [&#64;attention] formula is</p>
<div class="math notranslate nohighlight" id="equation-chapter-model-deployment-advanced-efficient-techniques-1">
<span class="eqno">(9.4.2)<a class="headerlink" href="#equation-chapter-model-deployment-advanced-efficient-techniques-1" title="Permalink to this equation">¶</a></span>\[\textbf{A} = Softmax(\frac{\textbf{QK}^T}{\sqrt{d_k}})\textbf{V}\]</div>
<p>:eqlabel:<code class="docutils literal notranslate"><span class="pre">equ:std_attn</span></code></p>
<p>As <span class="math notranslate nohighlight">\(d_k\)</span> is a scalar, we can simplify it into three parts:</p>
<div class="math notranslate nohighlight" id="equation-chapter-model-deployment-advanced-efficient-techniques-2">
<span class="eqno">(9.4.3)<a class="headerlink" href="#equation-chapter-model-deployment-advanced-efficient-techniques-2" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
    \textbf{S} = \textbf{QK}^T\\
    \textbf{P} = Softmax(\textbf{S})\\
    \textbf{O} = \textbf{PV}
\end{aligned}\end{split}\]</div>
<p>:eqlabel:<code class="docutils literal notranslate"><span class="pre">equ:attn_sep</span></code></p>
<p>The matrices <strong>K</strong>, <strong>Q</strong>, <strong>V</strong> are all stored in HBM. The standard
implementation of attention follows these steps:</p>
<ol class="arabic simple">
<li><p>Load <strong>K, Q</strong> from HBM, compute <strong>:math:`S` = :math:`QK^T`</strong>, and
write <strong>S</strong> to the HBM.</p></li>
<li><p>Read <strong>S</strong> from HBM, compute <strong>P</strong> = <span class="math notranslate nohighlight">\(Softmax\)</span>S, and write
<strong>P</strong> to HBM.</p></li>
<li><p>Load <strong>P</strong> and <strong>V</strong> from HBM, compute <strong>O</strong> = <strong>PV</strong>, and write
<strong>O</strong> to HBM. Finally, return <strong>O</strong>.</p></li>
</ol>
<p>The standard implementation of attention involves frequent I/O
interactions with HBM for large matrices reads/writes, leading to
reduced speed due to the intensive memory access requirements. Moreover,
it stores large intermediate matrices in HBM for backward propagation.</p>
<p>To handle such issues, FlashAttention divides the input components <strong>Q,
K</strong>, and <strong>V</strong> into blocks. These blocks are then transferred from
slower HBM to faster SRAM. Once in SRAM, the attention output is
computed with respect to these blocks. Two strategies involved are
called <strong>tiling</strong> and <strong>recomputation</strong>.</p>
<p><strong>Tiling</strong>: Assuming a vector <span class="math notranslate nohighlight">\(x\in \mathbb{R}^D\)</span>, the basic
Softmax can be calculated as:</p>
<div class="math notranslate nohighlight" id="equation-chapter-model-deployment-advanced-efficient-techniques-3">
<span class="eqno">(9.4.4)<a class="headerlink" href="#equation-chapter-model-deployment-advanced-efficient-techniques-3" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
m(x) = \max\limits_{i} x_i\\
l_{1}(x) = [e^{x_{1} - m(x)},\, ...\,,e^{x_{D} - m(x)}]\\
s_{1}(x) = \sum_{i} l_{1}(x)_i\\
Softmax(x) = \frac{l_{1}(x)}{s_{1}(x)}
\end{aligned}\end{split}\]</div>
<p>Attention can be computed by blocks, so large Softmax can be decomposed
into separated parts. To elaborate, assuming a vector
<span class="math notranslate nohighlight">\(x \in\mathbb{R}^{2D}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-model-deployment-advanced-efficient-techniques-4">
<span class="eqno">(9.4.5)<a class="headerlink" href="#equation-chapter-model-deployment-advanced-efficient-techniques-4" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
x = [x_{1}, \,x_{2}], \quad x_{1}, \, x_{2} \in\mathbb{R}^D\\
m(x) = \max(m(x_{1}), \,m(x_{2}))\\
l(x) = [e^{m(x_{1})-m_(x)}l_{1}(x_1),\, ... \, ,e^{m(x_2)-m(x)}l_{1}(x_2)]\\
s(x) = e^{m(x_{1})-m(x)}s_{1}(x_1) + e^{m(x_2)-m(x)}s_{1}(x_2)\\
Softmax(x) = \frac{l(x)}{s(x)}
\end{aligned}\end{split}\]</div>
<p>Figure <code class="xref std std-numref docutils literal notranslate"><span class="pre">ch-deploy/flashattn</span></code> shows a brief overview of
FlashAttention with two blocks. Following decomposition, Softmax
calculations can be executed block by block. Therefore, <strong>K, Q</strong> and
<strong>V</strong> are initially divided into blocks. Subsequently, compute the
Softmax values together with the respective <span class="math notranslate nohighlight">\(s(x)\)</span> and
<span class="math notranslate nohighlight">\(m(x)\)</span>. Ultimately, aggregate <strong>O</strong> blocks, the outcomes of the
block-wise Softmax values with the multiplication of corresponding <strong>V</strong>
block vectors. To enhance the efficiency of these steps, it’s necessary
to load all the required matrix blocks from the HBM to the on-chip SRAM
for the current step’s computation. All the calculations take place
on-chip, that is, within the SRAM. To ensure that all required blocks
are sufficiently proper to fit within the on-chip SRAM, which has a
capacity of 20MB, careful consideration must be given to setting the
size of these blocks. For <strong>K, Q</strong> and <strong>V</strong>
<span class="math notranslate nohighlight">\(\in\mathbb{R}^{N \times d}\)</span>, the block size is set to
<span class="math notranslate nohighlight">\(\lfloor \frac{M}{4d} \rfloor\)</span> where M is the SRAM size and the
output block size is set to be
<span class="math notranslate nohighlight">\(min(\lfloor \frac{M}{4d} \rfloor, d)\)</span> [&#64;dao2022flashattention].
Post-computation of each block, the resulting output block along with
the corresponding <span class="math notranslate nohighlight">\(s(x)\)</span> and <span class="math notranslate nohighlight">\(m(x)\)</span> are transferred back to
the HBM. These blocks are sufficiently small for reads/writes to avoid
causing significant latency; in addition, all related computations are
implemented in one CUDA kernel using <strong>kernel fusion</strong>. This avoids
repeatedly reading and writing from and to HBM.</p>
<figure id="fig:ch-deploy/memory"><figcaption><p>Memory Hierarchy Overview</p>
</figcaption></figure><figure id="fig:ch-deploy/flashattn"><figcaption><p>FlashAttention Overview with Two Blocks</p>
</figcaption></figure><p><strong>Recomputation</strong>:</p>
<p>Standard attention requires <span class="math notranslate nohighlight">\(O(N^2)\)</span> memory to store intermediate
matrices <strong>S</strong> and <strong>P</strong> for gradient computation w.r.t. <strong>Q, K, V</strong> in
the backward pass. For FlashAttention, <strong>S</strong> and <strong>P</strong> can be recomputed
with the HBM-stored <span class="math notranslate nohighlight">\(s(x)\)</span>, <span class="math notranslate nohighlight">\(m(x)\)</span> and <strong>O</strong> in SRAM easily.
Therefore, only <span class="math notranslate nohighlight">\(O(N)\)</span> memory is required. Furthermore,
FlashAttention has fewer HBM accesses than Standard Attention which
results in faster runtime [&#64;dao2022flashattention].</p>
<p>The standard FlashAttention implementation doesn’t eliminate the
redundant computation of zero elements within the attention mechanism.
To address this, a mask is incorporated in FlashAttention to focus
computation exclusively on non-zero elements. Termed as Block-Sparse
FlashAttention, this approach is also discussed in
[&#64;dao2022flashattention]. By using sparsity, Block-Sparse FlashAttention
effectively reduces the larger component of the I/O complexity, leading
to a direct improvement in performance.</p>
<p>However, FlashAttention has not been fully optimized. Dao noted that its
inefficiency stems from suboptimal work distribution among various
thread blocks and warps on the GPU. This leads to either low occupancy
or unnecessary shared memory reads and writes. Thus, Dao proposed
<strong>FlashAttention-2</strong> [&#64;dao2023flashattention2] which has better
parallelism and work partitioning.</p>
<p>FlashAttention-2 includes several tweaks to reduce the non-matmul
operations.</p>
<ol class="arabic simple">
<li><p>Remain output <strong>O</strong> blocks un-scaled until the very end of the loop.</p></li>
<li><p>Instead of saving both <span class="math notranslate nohighlight">\(s(x)\)</span> and <span class="math notranslate nohighlight">\(m(x)\)</span> in HBM, save
<span class="math notranslate nohighlight">\(logsumexp_{i} = m_{i} + log(s_{i})\)</span> which is enough for
backward pass.</p></li>
<li><p>For blocks where column indices are greater than row indices, which
occupy about half of the blocks in large sequences, computation is
skipped. It leads to a 1.7-1.8X speedup compared to those without
this skip.</p></li>
<li><p>Only use the row-wise <span class="math notranslate nohighlight">\(logsumexp\)</span> instead of both the row-wise
max <span class="math notranslate nohighlight">\(m(x)\)</span> and row-wise sum <span class="math notranslate nohighlight">\(s(x)\)</span> of exponentials in the
softmax.</p></li>
</ol>
<p>For parallelism, In the original version of FlashAttention, parallel
processing was done over the batch size and number of heads, with one
thread block processing one attention head. There are as many thread
blocks as the product of the batch size and the number of heads. This
works well on an A100 GPU, which has 108 Streaming Multiprocessors
(SMs), as long as the number of thread blocks is large enough to engage
most of the SMs, like 80 or more.</p>
<p>However, for long sequences, this isn’t as efficient because of the
smaller number of thread blocks. FlashAttention-2 introduces additional
parallelization over the sequence length dimension, which significantly
speeds up the process in these cases by improving GPU occupancy, i.e.
the fraction of GPU resources being used.</p>
<p>In the forward pass, the method schedules different parts of the
sequence length on different thread blocks that operate independently.
The backward pass also incorporates parallelization over the sequence
length. To update the gradients of the query matrix <strong>dQ</strong>, it uses
atomic additions to synchronize updates between different thread blocks.</p>
<p>Within each thread block, work partitioning for each wrap is also of
importance. Usually, 4 to 8 warps are allocated to each thread block. To
handle this condition, FlashAttention-2 introduces significant
improvements in both the forward and backward passes of the algorithm.
In the forward pass, unlike FlashAttention which splits <strong>K</strong> and <strong>V</strong>
across 4 warps (the “split-K” scheme) leading to inefficient shared
memory operations, FlashAttention-2 splits <strong>Q</strong> across the warps while
keeping <strong>K</strong> and <strong>V</strong> accessible to all. This change eliminates the
need for inter-warp communication and reduces shared memory
reads/writes, resulting in a faster runtime. Each warp directly
multiplies its slice of <strong>Q</strong> with <strong>K</strong> and then with <strong>V</strong>,
simplifying the computation of the output slice. In the backward pass,
FlashAttention-2 continues to avoid the “split-K” scheme, aligning the
warps in a way that minimizes shared memory operations. Despite
requiring some synchronization due to complex dependencies among inputs
and gradients, this approach still leads to a speedup by reducing the
shared memory reads/writes.</p>
<p>FlashAttention has gained significant attention in the industry for its
remarkable performance, offering accelerated attention computations in
both forward and backward passes while also reducing memory I/O
complexity. An enhanced version, FlashAttention-2, achieves a notable 2X
speedup over the standard FlashAttention [&#64;dao2022flashattention].
Moreover, continuous optimization efforts are being made, promising an
even more potent version of FlashAttention in the future.</p>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">9.4. Advanced Efficient Techniques</a><ul>
<li><a class="reference internal" href="#speculative-decoding">9.4.1. Speculative Decoding</a></li>
<li><a class="reference internal" href="#flashattention">9.4.2. FlashAttention</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="Model_Compression.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>9.3. Model Compression</div>
         </div>
     </a>
     <a id="button-next" href="Model_Inference.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>9.5. Model Inference</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>