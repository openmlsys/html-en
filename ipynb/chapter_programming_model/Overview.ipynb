{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccdc1a8a",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Overview\n",
    "\n",
    "With the advent of machine learning systems, the design of user-friendly\n",
    "and high-performance APIs has become a paramount concern for system\n",
    "designers. In the early stages of machine learning frameworks (as\n",
    "depicted in Figure :numref:`ch03/framework_development_history`), developers often\n",
    "opted for high-level programming languages like Lua (Torch) and Python\n",
    "(Theano) to write machine learning programs. These frameworks offered\n",
    "essential functions, including model definition and automatic\n",
    "differentiation, which are integral to machine learning. They were\n",
    "particularly well-suited for creating small-scale machine learning\n",
    "applications targeted toward scientific research purposes.\n",
    "\n",
    "<figure id=\"fig:ch03/framework_development_history\">\n",
    "<embed src=\"../img/ch03/framework_development_history.pdf\" />\n",
    "<figcaption> Evolution of Machine Learning Programming Frameworks: A\n",
    "Historical Perspective</figcaption>\n",
    "</figure>\n",
    "\n",
    "The rapid advancement of deep neural networks (DNNs) since 2011 has\n",
    "sparked groundbreaking achievements in various AI application domains,\n",
    "such as computer vision, speech recognition, and natural language\n",
    "processing. However, training DNNs requires substantial computational\n",
    "power. Unfortunately, earlier frameworks like Torch (primarily using\n",
    "Lua) and Theano (mainly using Python) were unable to fully harness this\n",
    "computing power. On the other hand, general-purpose APIs like CUDA C for\n",
    "computational accelerators such as NVIDIA GPUs have become increasingly\n",
    "mature, and multi-thread libraries like POSIX Threads built on CPU\n",
    "multi-core technology have gained popularity among developers.\n",
    "Consequently, many machine learning users sought to develop\n",
    "high-performance deep learning applications utilizing C/C++. These\n",
    "requirements led to the emergence of frameworks like Caffe, which\n",
    "employed C/C++ as their core APIs.\n",
    "\n",
    "However, customization of machine learning models is often necessary to\n",
    "suit specific deployment scenarios, data types, identification tasks,\n",
    "and so on. This customization typically falls on the shoulders of AI\n",
    "application developers, who may come from diverse backgrounds and may\n",
    "not fully leverage the capabilities of C/C++. This became a significant\n",
    "bottleneck that hindered the widespread adoption of programming\n",
    "frameworks like Caffe, which heavily relied on C/C++.\n",
    "\n",
    "In late 2015, Google introduced TensorFlow, which revolutionized the\n",
    "landscape. In contrast to Torch, TensorFlow adopted a design where the\n",
    "frontend and backend were relatively independent. The frontend,\n",
    "presented to users, utilized the high-level programming language Python,\n",
    "while the high-performance backend was implemented in C/C++. TensorFlow\n",
    "provided numerous Python-based frontend APIs, gaining wide acceptance\n",
    "among data scientists and machine learning researchers. It seamlessly\n",
    "integrated into Python-dominated big data ecosystems, benefiting from\n",
    "various big data development libraries such as NumPy, Pandas, SciPy,\n",
    "Matplotlib, and PySpark. Python's exceptional interoperability with\n",
    "C/C++, as demonstrated in multiple Python libraries, further enhanced\n",
    "TensorFlow's appeal. Consequently, TensorFlow combined the flexibility\n",
    "and ecosystem of Python with high-performance capabilities offered by\n",
    "its C/C++ backend. This design philosophy was inherited by subsequent\n",
    "frameworks like PyTorch, MindSpore, and PaddlePaddle.\n",
    "\n",
    "Subsequently, as observed globally, prominent enterprises started\n",
    "favoring open-source machine learning frameworks, leading to the\n",
    "emergence of Keras and TensorLayerX. These high-level libraries\n",
    "significantly expedited the development of machine learning\n",
    "applications. They provided Python APIs that allowed quick importing of\n",
    "existing models, and these high-level APIs were decoupled from the\n",
    "intricate implementation details of specific machine learning\n",
    "frameworks. As a result, Keras and TensorLayerX could be utilized across\n",
    "different machine learning frameworks.\n",
    "\n",
    "While deep neural networks continued to evolve, new challenges surfaced\n",
    "regarding the APIs of machine learning frameworks. Around 2020, novel\n",
    "frameworks like MindSpore and JAX emerged to tackle these challenges.\n",
    "MindSpore, in addition to inheriting the hybrid interfaces (Python and\n",
    "C/C++) from TensorFlow and PyTorch, expanded the scope of machine\n",
    "learning programming models. This expansion facilitated efficient\n",
    "support for a diverse range of AI backend chips, including NVIDIA GPU,\n",
    "Huawei Ascend , and ARM. Consequently, machine learning applications can\n",
    "be swiftly deployed across a wide array of heterogeneous devices.\n",
    "\n",
    "Simultaneously, the proliferation of ultra-large datasets and\n",
    "ultra-large DNNs necessitated distributed execution as a fundamental\n",
    "design requirement for machine learning programming frameworks. However,\n",
    "implementing distributed execution in TensorFlow and PyTorch required\n",
    "developers to write substantial amounts of code for allocating datasets\n",
    "and DNNs across distributed nodes. Yet, many AI developers are not\n",
    "well-versed in distributed programming. In this regard, JAX and\n",
    "MindSpore significantly improves the situation by enabling the seamless\n",
    "execution of programs on a single node across various other nodes.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}