{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ac6771",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Neural Network Programming\n",
    "\n",
    "To implement an AI model, a machine learning framework often takes a\n",
    "neural-network-centric programming interface. Regardless of their\n",
    "structures, neural networks are comprised of three elements: (1) Nodes\n",
    "serve as computational units that carry out the processing of a neural\n",
    "network, (2) Node Weights are variables updated by gradients during the\n",
    "training process, and (3) Node Connections specify how data (for\n",
    "example, activation and gradients) are transmitted within a neural\n",
    "network.\n",
    "\n",
    "## Neural Network Layers\n",
    "\n",
    "In order to simplify the construction of a neural network, many machine\n",
    "learning frameworks utilize a layer-oriented approach. This method\n",
    "organizes nodes, their weights, and their connections into cohesive\n",
    "neural network layers.\n",
    "\n",
    "To illustrate this, we can examine the use of fully connected layers, a\n",
    "type of neural network layer. A distinguishing characteristic of fully\n",
    "connected layers is that every node in one layer is linked to every node\n",
    "in the succeeding layer. This method facilitates an extensive linear\n",
    "transformation of the feature space. By doing so, data can be transposed\n",
    "from a high-dimensional space to a lower-dimensional one, and\n",
    "conversely.\n",
    "\n",
    "As shown in Figure :numref:`ch03/fc_layer_1`, the fully connected process transforms\n",
    "*n* data points from the input into an *m* sized feature space. This is\n",
    "followed by a further transformation into a *p* sized feature space.\n",
    "It's important to highlight that the quantity of parameters in a fully\n",
    "connected layer grows substantially --- from n$\\times$m during the\n",
    "initial transformation to m$\\times$p in the subsequent one.\n",
    "\n",
    "<figure id=\"fig:ch03/fc_layer_1\">\n",
    "<img src=\"../img/ch03/fc_layer_1.png\" style=\"width:60.0%\" />\n",
    "<figcaption>Fully connected layer illustration</figcaption>\n",
    "</figure>\n",
    "\n",
    "Several types of neural network layers are widely used in various\n",
    "applications, including fully connected, convolution, pooling,\n",
    "recurrent, attention, batch normalization, and dropout layers. When\n",
    "dealing with problems related to time series association in sequential\n",
    "data, recurrent neural layers are commonly employed. However, recurrent\n",
    "neural layers encounter difficulties with vanishing or exploding\n",
    "gradients as the sequence length increases during the training process.\n",
    "The Long Short-Term Memory (LSTM) model was developed as a solution to\n",
    "this problem, enabling the capturing of long-term dependencies in\n",
    "sequential data. Code `ch02/code2.3.1` shows some examples of NN Layers in Pytorch:\n",
    "\n",
    "**ch02/code2.3.1**\n",
    "\n",
    "```python\n",
    "fc_layer = nn.Linear(16, 5) # A fully connected layer with 16 input features and 5 output features\n",
    "relu_layer = nn.ReLU() # A ReLU activation layer\n",
    "conv_layer = nn.Conv2d(3, 16, 3, padding=1) # A convolutional layer with 3 input channels, 16 output channels, and a 3x3 kernel\n",
    "dropout_layer = nn.Dropout(0.2) # A dropout layer with 20% dropout rate\n",
    "batch_norm_layer = nn.BatchNorm2d(16) # A batch normalization layer with 16 channels\n",
    "layers = nn.Sequential(conv_layer, batch_norm_layer, relu_layer, fc_layer, dropout_layer) # A sequential container to combine layers\n",
    "```\n",
    "\n",
    "In tasks related to natural language processing, the\n",
    "Sequence-to-Sequence (Seq2Seq) architecture applies recurrent neural\n",
    "layers in an encoder-decoder framework. Often, the decoder component of\n",
    "Seq2Seq integrates the attention mechanism, allowing the model to\n",
    "concentrate on pertinent segments of the input sequence. This\n",
    "amalgamation contributed to the inception of the *Transformer* model, a\n",
    "pivotal element in the architecture of the Bidirectional Encoder\n",
    "Representations from Transformers (BERT) and Generative Pre-trained\n",
    "Transformers (GPT) models. Both BERT and GPT have propelled significant\n",
    "progress in diverse language-related tasks.\n",
    "\n",
    "## Neural Network Implementation\n",
    "\n",
    "With an increase in the number of network layers, the manual management\n",
    "of training variables becomes progressively complex. Thankfully, most\n",
    "machine learning frameworks provide user-friendly APIs that encapsulate\n",
    "neural network layers into a base class, which is then inherited by all\n",
    "other layers. Notable examples include `mindspore.nn.Cell` in MindSpore\n",
    "and `torch.nn.Module` in PyTorch. Code\n",
    "`ch02/code2.3.2` gives a MLP Implementation using Pytorch.\n",
    "\n",
    "**ch02/code2.3.2**\n",
    "\n",
    "```python\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "```\n",
    "\n",
    "Figure :numref:`ch03/model_build` demonstrates the intricate process of\n",
    "constructing a neural network. The base class plays a pivotal role in\n",
    "initializing training parameters, managing their status, and outlining\n",
    "the computation process. Conversely, the neural network model implements\n",
    "functions to administer the network layers and their associated\n",
    "parameters. Both MindSpore's Cell and PyTorch's Module efficiently serve\n",
    "these functions. Notably, Cell and Module function not just as model\n",
    "abstraction methods but also as base classes for all networks.\n",
    "\n",
    "Existing model abstraction strategies can be divided into two\n",
    "categories. The first involves the abstraction of two methods: Layer\n",
    "(which oversees parameter construction and forward computation for an\n",
    "individual neural network layer) and Model (which manages the\n",
    "connection, combination of neural network layers, and administration of\n",
    "layer parameters). The second category combines Layer and Model into a\n",
    "single method, representing both an individual neural network layer and\n",
    "a model composed of multiple layers. Cell and Module implementations\n",
    "fall into this second category.\n",
    "\n",
    "<figure id=\"fig:ch03/model_build\">\n",
    "<embed src=\"../img/ch03/model_build.pdf\" style=\"width:90.0%\" />\n",
    "<figcaption>Comprehensive neural network construction\n",
    "process</figcaption>\n",
    "</figure>\n",
    "\n",
    "Figure :numref:`ch03/cell_abstract` portrays a universal method for\n",
    "designing the abstraction of a neural network layer. The constructor\n",
    "uses the `OrderedDict` class from the Python `collections` module to\n",
    "store initialized neural network layers and their corresponding\n",
    "parameters. This results in an ordered output, which is more compatible\n",
    "with stacked deep learning models compared to an unordered `Dict`. The\n",
    "management of neural network layers and parameters is conducted within\n",
    "the `__setattr__` method. Upon detecting that an attribute pertains to a\n",
    "neural network layer or represents a layer parameter, `__setattr__`\n",
    "records the attribute appropriately.\n",
    "\n",
    "In the neural network model, the computation process is vital. This\n",
    "process is defined by reloading the `__call__` method during the\n",
    "implementation of neural network layers. To acquire the training\n",
    "parameters, the base class traverses all network layers. All retrieved\n",
    "training parameters are then conveyed to the optimizer through the\n",
    "assigned interface that returns such parameters. This text, however,\n",
    "only touches on a few significant methods.\n",
    "\n",
    "Concerning custom methods, it is often required to implement techniques\n",
    "for inserting/deleting parameters, adding/removing neural network\n",
    "layers, and retrieving neural network model information.\n",
    "\n",
    "<figure id=\"fig:ch03/cell_abstract\">\n",
    "<img src=\"../img/ch03/cell_abstract.png\" style=\"width:90.0%\" />\n",
    "<figcaption>Abstraction technique of neural network base\n",
    "classes</figcaption>\n",
    "</figure>\n",
    "\n",
    "In order to preserve simplicity, we provide a condensed overview of the\n",
    "base class implementation for neural network interface layers. In\n",
    "practical applications, users are typically unable to directly reload\n",
    "the `__call__` method responsible for computation. Instead, an operation\n",
    "method is usually defined outside of `__call__`, which users can invoke\n",
    "to utilize `__call__`.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}