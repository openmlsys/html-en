{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02dd2251",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Overview of AI Compilers\n",
    "\n",
    "Like classical compilers, AI compilers also convert user-written code\n",
    "into efficient machine-executable code. In the following, we delve into\n",
    "the intricacies of AI compilers, discussing various concepts inherent to\n",
    "general-purpose compilers such as ahead of time (AOT), just in time\n",
    "(JIT), intermediate representations (IRs), pass-based optimization,\n",
    "abstract syntax tree, side effects, and closures. Our focus will be\n",
    "primarily on the distinctive design and functionality of AI compilers as\n",
    "compared to classical compilers, rather than offering definitions of\n",
    "these concepts, as these can be found in numerous other compiler-related\n",
    "textbooks.\n",
    "\n",
    "The design of AI compilers is significantly influenced by classical\n",
    "compilers like the Low Level Virtual Machine (LLVM). Thus, gaining an\n",
    "understanding of the basic architecture of the LLVM compiler, depicted\n",
    "in Figure :numref:`ch04/llvm-basic`, will be beneficial.\n",
    "\n",
    "![Basic architecture of the LLVMcompiler](../img/ch04/LLVM_basic_architecture.png)\n",
    ":label:`ch04/llvm-basicwidth=\"\\\\linewidth\"`\n",
    "\n",
    "The LLVM compiler consists of three components: the frontend,\n",
    "intermediate representations, and the backend. The frontend converts\n",
    "high-level languages into IRs. The backend then transforms these IRs\n",
    "into machine instructions executable on the target hardware. As their\n",
    "name implies, IRs serve as a transition phase from the frontend to the\n",
    "backend, where necessary optimizations can take place. The architecture\n",
    "of the LLVM compiler ensures that IRs are reusable and compatible with\n",
    "any newly introduced frontend or hardware. While IRs can exist on one or\n",
    "more levels, LLVM typically uses a one-level structure, meaning the\n",
    "frontend and backend optimizations share the same set of IRs.\n",
    "\n",
    "AI compilers, on the other hand, commonly employ a multi-level IR\n",
    "structure. An example is the multi-level IR (MLIR) design adopted by\n",
    "TensorFlow, as depicted in Figure\n",
    ":numref:`ch04/TF-IR`.\n",
    "TensorFlow's MLIR comprises three levels of IRs: the TensorFlow graph\n",
    "IR, the XLA HLO IR, and hardware-specific LLVM IR or TPU IR. The\n",
    "subsequent sections briefly outline these levels and their corresponding\n",
    "compilation optimization processes.\n",
    "\n",
    "![TensorFlow's multi-level IRdesign](../img/ch04/TensorFlow-IR.png)\n",
    ":label:`ch04/TF-IRwidth=\"\\\\linewidth\"`\n",
    "\n",
    "The process of optimization in computational graphs is known as graph\n",
    "compilation optimization. The first level of IR, the graph IR, carries\n",
    "out optimization and operations (e.g., graph optimization and graph\n",
    "segmentation) for an entire graph. While this complete-graph IR is\n",
    "suitable for static graph execution, it proves challenging for\n",
    "hardware-specific optimization due to the absence of hardware\n",
    "information. To address this, hardware-specific generic compilation\n",
    "optimization is applied at the mid-level of IRs. Platforms like XLA,\n",
    "Tensor RT, and MindSpore's graph kernel fusion enhance the execution\n",
    "performance of various neural networks on specific hardware by executing\n",
    "operator fusion and other optimizations for different hardware types.\n",
    "\n",
    "The final level of IR deals exclusively with a certain type of hardware\n",
    "accelerator and often comes bundled with a hardware vendor's compiler.\n",
    "For instance, the TBE compiler, paired with the Ascend hardware, is\n",
    "based on HalideIR as its efficient execution operators are generated\n",
    "based on TVM's HalideIR.\n",
    "\n",
    "The multi-level IR design grants IRs enhanced flexibility and\n",
    "facilitates more efficient pass-based optimization for each specific IR\n",
    "level. However, this design has limitations. First, achieving fully\n",
    "compatible IR transformation across different levels is challenging due\n",
    "to the substantial engineering effort required and potential information\n",
    "loss during the transformation. Optimization carried out at one IR level\n",
    "might eliminate some information, and the implications of this removal\n",
    "must be evaluated at the next level. As a result, IR transformation\n",
    "imposes stricter constraints on the sequence in which optimization\n",
    "occurs. Second, the decision of at which of two adjacent levels to\n",
    "perform certain IR optimizations presents a dilemma for framework\n",
    "developers. Lastly, because different IR levels can define different\n",
    "operator granularities, some accuracy might be compromised.\n",
    "\n",
    "To mitigate these drawbacks, the AI compiler in the MindSpore machine\n",
    "learning framework uses a unified IR design known as MindIR. Figure\n",
    ":numref:`ch04/msflow`\n",
    "illustrates the internal execution process of MindSpore's AI compiler.\n",
    "In this process, the compiler frontend handles graph compilation and\n",
    "hardware-agnostic optimization, while the compiler backend conducts\n",
    "tasks like hardware-specific optimization and operator selection.\n",
    "\n",
    "![Working process of MindSpore's AIcompiler](../img/ch04/compiler_process.png)\n",
    ":label:`ch04/msflowwidth=\"\\\\linewidth\"`\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}