{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f896e128",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "In the following, we describe the key methodologies applied in automatic\n",
    "differentiation.\n",
    "\n",
    "## Types of Differentiation Methods\n",
    "\n",
    "Differentiation constitutes a collection of methodologies enabling the\n",
    "efficient and precise evaluation of derivatives within computer\n",
    "programs. Since the 1960s and 1970s, it has been extensively utilized\n",
    "across multiple sectors including fluid mechanics, astronomy, and\n",
    "mathematical finance . Its theories and implementation have been\n",
    "rigorously studied over time.\n",
    "\n",
    "With the advancement of deep learning, which has shown remarkable\n",
    "progress across an expanding range of machine learning tasks in recent\n",
    "years, automatic differentiation has found wide-spread application in\n",
    "the field of machine learning. Given that many optimization algorithms\n",
    "employed in machine learning models necessitate derivatives of the\n",
    "models, automatic differentiation has emerged as an integral component\n",
    "within mainstream machine learning frameworks such as TensorFlow and\n",
    "PyTorch.\n",
    "\n",
    "There are four primary methods to evaluate derivatives in computer\n",
    "programs, each of which is explained in the following sections.\n",
    "\n",
    "### Manual Differentiation\n",
    "\n",
    "Manual differentiation involves the direct computation of the derivative\n",
    "expression of a function, a task which hinges upon the input values\n",
    "specified within a program. Although this method could seem appealing\n",
    "due to its simplicity and directness, it is worth noting that it comes\n",
    "with its share of limitations.\n",
    "\n",
    "A primary drawback of manual differentiation is the need to re-derive\n",
    "and re-implement the derivative every time a function changes, which can\n",
    "be laborious and time-consuming. This is especially true for complex\n",
    "functions or when working on large-scale projects where the function\n",
    "might undergo frequent updates.\n",
    "\n",
    "Moreover, manual differentiation can be prone to human errors. The\n",
    "process of deriving complex functions often involves intricate chains of\n",
    "mathematical reasoning. A slight oversight or error in any of these\n",
    "steps can lead to an incorrect derivative, which, in turn, can greatly\n",
    "affect the outcome of the computation. This susceptibility to mistakes\n",
    "can add a layer of uncertainty to the reliability of this method.\n",
    "\n",
    "Furthermore, in cases where high-order derivatives or partial\n",
    "derivatives with respect to many variables are needed, manual\n",
    "differentiation quickly becomes unfeasible due to the increase in\n",
    "complexity. The difficulty of computing these derivatives correctly\n",
    "grows exponentially with the number of variables and the order of the\n",
    "derivative.\n",
    "\n",
    "### Numerical Differentiation\n",
    "\n",
    "Numerical differentiation is an approach that logically stems from the\n",
    "fundamental definition of a derivative and employs the method of\n",
    "difference approximation. The basic formula for numerical\n",
    "differentiation can be described as follows:\n",
    "\n",
    "$$f^{'}(x)=\\lim_{h \\to 0}\\frac{f(x+h)-f(x)}{h}$$\n",
    "\n",
    "In this equation, for a sufficiently small value of the step size $h$,\n",
    "the difference quotient $\\frac{f(x+h)-f(x)}{h}$ is used as an\n",
    "approximation of the derivative. The inherent error in this\n",
    "approximation is referred to as the truncation error, which\n",
    "theoretically diminishes as the value of $h$ approaches zero. This\n",
    "suggests that a smaller step size would yield a more accurate\n",
    "approximation.\n",
    "\n",
    "However, the scenario in practice is not always so straightforward due\n",
    "to the phenomenon of round-off error. This error arises from the finite\n",
    "precision of floating-point arithmetic operations in digital computer\n",
    "systems. As the value of $h$ decreases, the round-off error conversely\n",
    "increases, adding a degree of uncertainty to the computation.\n",
    "\n",
    "This creates a complex interplay between truncation error and round-off\n",
    "error. When the value of $h$ is large, the truncation error dominates,\n",
    "whereas when $h$ is small, the round-off error is more significant.\n",
    "Consequently, the total error of numerical differentiation achieves a\n",
    "minimum at an optimal $h$ value that balances these two types of errors.\n",
    "\n",
    "In a nutshell, while numerical differentiation offers the advantage of\n",
    "relative simplicity in implementation, it suffers from certain\n",
    "limitations with regard to accuracy. The complexities arising from the\n",
    "interplay between truncation and round-off errors make it less reliable\n",
    "for certain tasks, particularly when high precision is required.\n",
    "Therefore, for many practical applications, more sophisticated\n",
    "techniques of automatic differentiation are preferred.\n",
    "\n",
    "### Symbolic Differentiation\n",
    "\n",
    "Symbolic differentiation involves the use of computer programs to\n",
    "automatically calculate derivatives. This is accomplished by recursively\n",
    "transforming function expressions in accordance with specific\n",
    "differentiation rules. These rules can be summarized as follows:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x}(f(x)+g(x))\\rightsquigarrow\\frac{\\partial}{\\partial x}f(x)+\\frac{\\partial }{\\partial x}g(x)$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x}(f(x)g(x))\\rightsquigarrow(\\frac{\\partial}{\\partial x}f(x))g(x)+f(x)(\\frac{\\partial}{\\partial x}g(x))$$\n",
    "\n",
    "Symbolic differentiation has been integrated into many modern algebraic\n",
    "systems such as Mathematica, as well as machine learning frameworks like\n",
    "Theano. It successfully addresses the issues related to hard-coding\n",
    "derivatives inherent in manual differentiation, thus automating the\n",
    "differentiation process and minimizing human error.\n",
    "\n",
    "Despite these advantages, symbolic differentiation has its own set of\n",
    "challenges. One of its primary limitations is its strict adherence to\n",
    "transforming and expanding expressions recursively, without the ability\n",
    "to reuse previous results of transformations. This can lead to a\n",
    "phenomenon known as expression swell , which results in highly complex\n",
    "and expanded expressions that can significantly slow down computation\n",
    "and increase memory usage.\n",
    "\n",
    "In addition, symbolic differentiation requires that the expressions to\n",
    "be differentiated are defined in closed form. This constraint largely\n",
    "restricts the use of control flow statements such as loops and\n",
    "conditional branches, which are common in programming. This lack of\n",
    "flexibility can significantly limit the design and expressivity of\n",
    "neural networks within machine learning frameworks, as these often\n",
    "require intricate control flow structures for more advanced operations.\n",
    "\n",
    "### Automatic Differentiation\n",
    "\n",
    "Automatic differentiation cleverly amalgamates the strategies of\n",
    "numerical differentiation and symbolic differentiation to offer an\n",
    "efficient and precise mechanism for derivative evaluation. It breaks\n",
    "down the arithmetic operations in a program into a finite set of\n",
    "elementary operations, for each of which the rules of derivative\n",
    "evaluation are already known. Upon determining the derivative of each\n",
    "elementary operation, the chain rule is applied to synthesize these\n",
    "individual results, ultimately yielding the derivative of the entire\n",
    "program.\n",
    "\n",
    "The fundamental strength of automatic differentiation lies in its\n",
    "ability to sidestep the primary drawbacks of both numerical and symbolic\n",
    "differentiation. Unlike numerical differentiation, which suffers from\n",
    "precision issues due to truncation and round-off errors, automatic\n",
    "differentiation facilitates accurate derivative evaluations.\n",
    "Furthermore, it mitigates the issue of expression swell, a significant\n",
    "concern in symbolic differentiation, by decomposing the program into a\n",
    "series of elementary expressions. Symbolic differentiation rules are\n",
    "only applied to these simplified expressions, and the derivative results\n",
    "are reused to enhance efficiency.\n",
    "\n",
    "Automatic differentiation also surpasses symbolic differentiation in its\n",
    "capability to handle control flow statements. It has the ability to\n",
    "process branching, looping, and recursion, enhancing its flexibility and\n",
    "applicability to complex computational scenarios.\n",
    "\n",
    "In contemporary applications, automatic differentiation has found\n",
    "widespread use in deep learning frameworks for the evaluation of\n",
    "derivatives, given its blend of accuracy and efficiency. The subsequent\n",
    "sections delve into the mechanics and implementation aspects of\n",
    "automatic differentiation, elucidating its role as a crucial tool in\n",
    "computational mathematics and machine learning.\n",
    "\n",
    "## Forward Mode and Reverse Mode\n",
    "\n",
    "Automatic differentiation can be categorized into two modes, forward and\n",
    "reverse, based on the sequence in which the chain rule is applied.\n",
    "Consider a composite function $y=a(b(c(x)))$. The formula to calculate\n",
    "its gradient, $\\frac{\\partial y}{\\partial x}$, is given as:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial a}\\frac{\\partial a}{\\partial b}\\frac{\\partial b}{\\partial c}\\frac{\\partial c}{\\partial x}$$\n",
    "\n",
    "In the forward mode of automatic differentiation, the computation of the\n",
    "gradient originates from the inputs, as shown in the following\n",
    "formulation:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x}=(\\frac{\\partial y}{\\partial a}(\\frac{\\partial a}{\\partial b}(\\frac{\\partial b}{\\partial c}\\frac{\\partial c}{\\partial x})))$$\n",
    "\n",
    "Conversely, in the reverse mode, the computation of the gradient begins\n",
    "from the outputs, represented by the equation:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x}=(((\\frac{\\partial y}{\\partial a}\\frac{\\partial a}{\\partial b})\\frac{\\partial b}{\\partial c})\\frac{\\partial c}{\\partial x})$$\n",
    "\n",
    "To illustrate the computation methods of the two modes, let us consider\n",
    "the following function and aim to evaluate its derivative,\n",
    "$\\frac{\\partial y}{\\partial x_1}$ at the point $(x_1, x_2)=(2,5)$:\n",
    "$$y=f(x_1,x_2)=ln(x_1)+{x_1}{x_2}-sin(x_2)$$\n",
    "\n",
    "Figure :numref:`ch04/ch04-calculation_graph` represents the\n",
    "computational graph of this function, providing a visual demonstration\n",
    "of how automatic differentiation processes the function in both forward\n",
    "and reverse modes. This distinction between forward and reverse modes is\n",
    "particularly important when dealing with functions of multiple\n",
    "variables, with each mode having specific use cases and efficiency\n",
    "implications.\n",
    "\n",
    "![Computational graph of the examplefunction](../img/ch04/AD-example_graph.png)\n",
    ":label:`ch04/ch04-calculation_graph`\n",
    "\n",
    "### Forward Mode\n",
    "\n",
    "![Illustration of forward-mode automaticdifferentiation](../img/ch04/AD-forward_example.png)\n",
    ":label:`ch04/ch04-forward-mode-compute-function`\n",
    "\n",
    "Figure :numref:`ch04/ch04-forward-mode-compute-function` elucidates thecomputation process within the forward mode. The sequence of elementaryoperations, derived from the source program, is displayed on the left.Following the chain rule and using established derivative evaluationrules, we sequentially compute each intermediate variable ${\\dot{v}_i}=\\frac{\\partial v_i}{\\partial x_1}$ from top to bottom, as depicted on the right. \n",
    "Consequently, this leads to the computation ofthe final variable ${\\dot{v}_5}=\\frac{\\partial y}{\\partial x_1}$. In the process of derivative evaluation of a function, we obtain a setof partial derivatives of any output with respect to any input of thisfunction. \n",
    "For a function $f:{\\mathbf{R}^n}\\to \\mathbf{R}^m$, where $n$ is the number of independent input variables $x_i$, and $m$ is thenumber of independent output variables $y_i$, the derivative resultscorrespond to the following Jacobian matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{J}_{f}=    \\begin{bmatrix}        \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\        \n",
    "\\vdots & \\ddots & \\vdots \\\\        \n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each forward pass of function $f$ results in partial derivatives of alloutputs with respect to a single input, represented by the vectorsbelow. This corresponds to one column of the Jacobian matrix. Therefore,executing $n$ forward passes gives us the full Jacobian matrix.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}        \\frac{\\partial y_1}{\\partial x_i} \\\\        \n",
    "\\vdots \\\\        \n",
    "\\frac{\\partial y_m}{\\partial x_i}    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The forward mode allows us to compute Jacobian-vector products byinitializing $\\dot{\\mathbf{x}}=\\mathbf{r}$ to generate the results for asingle column. As the derivative evaluation rules for elementaryoperations are pre-determined, we know the Jacobian matrix for all theelementary operations. Consequently, by leveraging the chain rule toevaluate the derivatives of $f$ propagated from inputs to outputs, wesecure one column in the Jacobian matrix of the entire network.\n",
    "\n",
    "$$\n",
    "\\mathbf{J}_{f}\\mathbf{r}=    \\begin{bmatrix}        \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\        \n",
    "\\vdots & \\ddots & \\vdots \\\\        \n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}    \\end{bmatrix}    \\begin{bmatrix}        r_1 \\\\        \n",
    "\\vdots \\\\        \n",
    "r_n    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Reverse Mode\n",
    "\n",
    "Figure :numref:`ch04/ch04-backward-mode-compute` illustrates theautomatic differentiation process in the reverse mode. The sequence ofelementary operations, derived from the source program, is displayed onthe left. Beginning from $\\bar{v}_5=\\bar{y}=\\frac{\\partial y}{\\partial y}=1$, we sequentiallycompute each intermediate variable ${\\bar{v}_i}=\\frac{\\partial y_j}{\\partial v_i}$ from bottom to top,\n",
    "leveraging the chain rule and established derivative evaluation rules\n",
    "(as depicted on the right). Thus, we can compute the final variables\n",
    "${\\bar{x}_1}=\\frac{\\partial y}{\\partial x_1}$ and\n",
    "${\\bar{x}_2}=\\frac{\\partial y}{\\partial x_2}$.\n",
    "\n",
    "![Illustration of reverse-mode automaticdifferentiation](../img/ch04/AD-backward_example.png)\n",
    ":label:`ch04/ch04-backward-mode-compute`\n",
    "\n",
    "Every reverse pass of function $f$ produces partial derivatives of asingle output with respect to all inputs, represented by the followingvectors. This corresponds to a single row of the Jacobian matrix.Consequently, executing $m$ reverse passes gives us the full Jacobianmatrix.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}        \\frac{\\partial y_j}{\\partial x_1} & \\cdots & \\frac{\\partial y_j}{\\partial x_n}    \\end{bmatrix}$$Similarly, we can compute vector-Jacobian products to obtain the resultsfor a single row.$$\\mathbf{r}^{T}\\mathbf{J}_{f}=    \\begin{bmatrix}        r_1 & \\cdots & r_m    \\end{bmatrix}    \\begin{bmatrix}        \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\        \n",
    "\\vdots & \\ddots & \\vdots \\\\        \n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The quantity of columns and rows in a Jacobian matrix directly\n",
    "influences the number of forward and reverse passes needed to solve it\n",
    "for a given function $f$. This characteristic is particularly\n",
    "significant when determining the most efficient method of automatic\n",
    "differentiation.\n",
    "\n",
    "When the function has significantly fewer inputs than outputs\n",
    "$(f:{\\mathbf{R}^n}\\to \\mathbf{R}^m, n << m)$, the forward mode proves to\n",
    "be more efficient. Conversely, when the function has considerably more\n",
    "inputs than outputs $(f:{\\mathbf{R}^n}\\to \\mathbf{R}^m, n >> m)$, the\n",
    "reverse mode becomes advantageous. \n",
    "\n",
    "For an extreme case where the function maps from $n$ inputs to a single\n",
    "output $f:{\\mathbf{R}^n}\\to \\mathbf{R}$, we can evaluate all the\n",
    "derivatives of the output with respect to the inputs\n",
    "$(\\frac{\\partial y}{\\partial x_1},\\cdots,\\frac{\\partial y}{\\partial n})$\n",
    "using a single reverse pass or $n$ forward passes. This is a situation\n",
    "akin to derivative evaluation for a multi-input, single-output network,\n",
    "a structure frequently encountered in machine learning.\n",
    "\n",
    "Due to this feature, reverse-mode automatic differentiation forms the\n",
    "basis for the backpropagation algorithm, a key technique for training\n",
    "neural networks. By enabling efficient computation of gradients,\n",
    "especially in scenarios with high-dimensional input data and scalar\n",
    "output (common in many machine learning applications), reverse-mode\n",
    "automatic differentiation has become indispensable in the field.\n",
    "\n",
    "However, the reverse mode does come with certain limitations. For\n",
    "instance, once a source program is decomposed into a sequence of\n",
    "elementary operations in the forward mode, inputs can be obtained\n",
    "synchronously during the execution of these operations. This is possible\n",
    "because the sequence of derivative evaluations aligns with the sequence\n",
    "of operation execution. In contrast, in the reverse mode, the sequence\n",
    "for derivative evaluation is the inverse of the execution sequence of\n",
    "the source program, leading to a two-phased computation process. The\n",
    "initial phase entails executing the source program and storing the\n",
    "intermediate results in memory, while the subsequent phase involves\n",
    "retrieving these intermediate results to evaluate the derivatives. Due\n",
    "to the additional steps involved, the reverse mode requires more memory.\n",
    "\n",
    "## Implementing Automatic Differentiation\n",
    "\n",
    "This section explores typical design patterns for implementing automatic\n",
    "differentiation in machine learning frameworks. These design patterns\n",
    "can be broadly classified into three categories: elemental libraries,\n",
    "operator overloading, and source transformation.\n",
    "\n",
    "### Elemental Libraries\n",
    "\n",
    "Elemental libraries encapsulate elementary expressions and their\n",
    "differential expressions as library functions. When coding, users must\n",
    "manually decompose a program into a set of elementary expressions and\n",
    "replace them with corresponding library functions. Take the program\n",
    "$a=(x+y)/z$ as an example; it needs to be manually decomposed as\n",
    "follows:\n",
    "\n",
    "```\n",
    "t = x + y\n",
    "    a = t / z\n",
    "```\n",
    "\n",
    "Subsequently, users replace the decomposed elementary expressions with\n",
    "appropriate library functions:\n",
    "\n",
    "```\n",
    "// The parameters include variables x, y, and t and their derivative variables dx, dy, and dt.\n",
    "    call ADAdd(x, dx, y, dy, t, dt)\n",
    "    // The parameters include variables t, z, and a and their derivative variables dt, dz, and da.\n",
    "    call ADDiv(t, dt, z, dz, a, da)\n",
    "```\n",
    "\n",
    "The library functions, ADAdd and ADDiv, use the chain rule to define the\n",
    "Add and Div differential expressions, respectively. This is illustrated\n",
    "in Code `lst:diff`.\n",
    "\n",
    "**lst:diff**\n",
    "\n",
    "```\n",
    "def ADAdd(x, dx, y, dy, z, dz):\n",
    "    z = x + y\n",
    "    dz = dy + dx\n",
    "    \n",
    "    def ADDiv(x, dx, y, dy, z, dz):\n",
    "    z = x / y\n",
    "    dz = dx / y + (x / (y * y)) * dy\n",
    "```\n",
    "\n",
    "Elemental libraries constitute a simple and straightforward way of\n",
    "implementing automatic differentiation for programming languages.\n",
    "However, this approach requires users to manually decompose a program\n",
    "into elementary expressions before calling library functions for\n",
    "programming. Furthermore, it is not possible to use the native\n",
    "expressions found in programming languages.\n",
    "\n",
    "### Operator Overloading\n",
    "\n",
    "Leveraging the polymorphism characteristic inherent in modern\n",
    "programming languages, the Operator Overloading design pattern redefines\n",
    "the semantics of elementary operations and successfully encapsulates\n",
    "their differentiation rules. During the execution phase, it methodically\n",
    "documents the type, inputs, and outputs of every elementary operation\n",
    "within a data structure known as a 'tape'. These tapes have the ability\n",
    "to generate a trace, serving as a pathway for applying the chain rule.\n",
    "This makes it possible to aggregate elementary operations either in a\n",
    "forward or backward direction to facilitate differentiation. As depicted\n",
    "in Code `lst:OO`,\n",
    "we utilize the AutoDiff code from automatic differentiation libraries as\n",
    "a case in point to overload the basic arithmetic operators in\n",
    "programming languages.\n",
    "\n",
    "**lst:OO**\n",
    "\n",
    "```cpp\n",
    "namespace AutoDiff\n",
    "    {\n",
    "        public abstract class Term\n",
    "        {\n",
    "            // To overload and call operators (`+`, `*`, and `/`),\n",
    "            // TermBuilder records the types, inputs, and outputs of operations in tapes.\n",
    "            public static Term operator+(Term left, Term right)\n",
    "            {\n",
    "                return TermBuilder.Sum(left, right);\n",
    "            }\n",
    "            public static Term operator*(Term left, Term right)\n",
    "            {\n",
    "                return TermBuilder.Product(left, right);\n",
    "            }\n",
    "            public static Term operator/(Term numerator, Term denominator)\n",
    "            {\n",
    "                return TermBuilder.Product(numerator, TermBuilder.Power(denominator, -1));\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Tape data structures include the following basic elements:\n",
    "        // 1) Arithmetic results of operations\n",
    "        // 2) Derivative evaluation results corresponding to arithmetic results of operations\n",
    "        // 3) Inputs of operations\n",
    "        // In addition, functions Eval and Diff are used to define the computation and differentiation rules of the arithmetic operations.\n",
    "        internal abstract class TapeElement\n",
    "        {\n",
    "            public double Value;\n",
    "            public double Adjoint;\n",
    "            public InputEdges Inputs;\n",
    "            \n",
    "            public abstract void Eval();\n",
    "            public abstract void Diff();\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "Operator overloading carries the advantage of tracing the program\n",
    "through function calls and control flows, resulting in an implementation\n",
    "process that is both simple and straightforward. However, the\n",
    "requirement to trace the program during runtime introduces certain\n",
    "challenges. Specifically, operator overloading is necessitated to\n",
    "execute reverse-mode differentiation along the trace, which can\n",
    "potentially cause a drop in performance, particularly for elementary\n",
    "operations that are executed swiftly. Furthermore, due to the\n",
    "constraints of runtime, operator overloading is unable to conduct\n",
    "compile-time graph optimization prior to execution, and the unfolding of\n",
    "control flows must be based on the information available at runtime.\n",
    "Despite these challenges, operator overloading is extensively employed\n",
    "in the PyTorch framework for automatic differentiation due to its\n",
    "inherent simplicity and adaptability.\n",
    "\n",
    "### Source Transformation\n",
    "\n",
    "Source transformation is a design pattern that enriches programming\n",
    "languages and scrutinizes a program's source code or its Abstract Syntax\n",
    "Tree (AST) to automatically deconstruct the program into a set of\n",
    "differentiable elementary operations, each with predefined\n",
    "differentiation rules. The chain rule is then employed to amalgamate the\n",
    "differential expressions of the elementary operations, resulting in a\n",
    "novel program expression that conducts the differentiation. Source\n",
    "Transformation is integral to machine learning frameworks such as\n",
    "TensorFlow and MindSpore.\n",
    "\n",
    "Unlike operator overloading, which functions within programming\n",
    "languages, source transformation necessitates parsers and tools that\n",
    "manipulate IRs. It also requires transformation rules for function calls\n",
    "and control flow statements, such as loops and conditions. The principal\n",
    "advantage of source transformation is that the automatic differentiation\n",
    "transformation occurs only once per program, thus eliminating runtime\n",
    "overhead. Additionally, the complete differentiation program is\n",
    "available during compilation, enabling ahead-of-time optimization using\n",
    "compilers.\n",
    "\n",
    "However, source transformation presents a higher implementation\n",
    "complexity compared to the other approaches. It must support a wider\n",
    "array of data types and operations, and it necessitates preprocessors,\n",
    "compilers, or interpreters of extended languages, along with a more\n",
    "robust type-checking system. Even though source transformation does not\n",
    "manage automatic differentiation transformation at runtime, it still\n",
    "must ensure that certain intermediate variables from the forward pass\n",
    "are accessible by the adjoint in reverse mode. Two modes are available\n",
    "to facilitate this:\n",
    "\n",
    "-   **Tape-based mode**: This mode requires a global tape that ensures\n",
    "    the accessibility of intermediate variables. In this method, the\n",
    "    primitive function is augmented so that intermediate variables are\n",
    "    written to functions in the tape during the forward pass, and the\n",
    "    adjoint program reads these intermediate variables from the tape\n",
    "    during the backward pass. The tape used in source transformation\n",
    "    primarily stores the intermediate variables, while the tape used in\n",
    "    operator overloading additionally stores the executed operation\n",
    "    types. Given that the tape is a data structure constructed at\n",
    "    runtime, custom compiler optimizations are required. Moreover, tape\n",
    "    read and write operations must be differentiable to support\n",
    "    higher-order differentiation, which involves multiple applications\n",
    "    of reverse mode. As most tape-based tools do not differentiate tape\n",
    "    read and write operations, such tools do not support\n",
    "    reverse-over-reverse automatic differentiation.\n",
    "\n",
    "-   **Closure-based mode**: This mode was proposed to mitigate some of\n",
    "    the limitations observed in the tape-based mode. Within functional\n",
    "    programming, closures can capture the execution environment of a\n",
    "    statement and identify the non-local use of intermediate variables.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}