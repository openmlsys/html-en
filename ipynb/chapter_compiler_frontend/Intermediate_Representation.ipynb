{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e1a6ec",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Intermediate Representation\n",
    "\n",
    "In this section, we begin by introducing basic IR concepts and the types\n",
    "of IR employed in classical compilers. Next, we address the new\n",
    "requirements and challenges that arise in the IR design for machine\n",
    "learning frameworks. To conclude this section, we examine the types of\n",
    "IRs utilized by well-known machine learning frameworks and delve into\n",
    "their implementation.\n",
    "\n",
    "## Definition of Intermediate Representations\n",
    "\n",
    "An IR is a data structure or a form of code that a compiler utilizes to\n",
    "represent source code. Almost all compilers need IRs to model the\n",
    "program code that requires analysis, transformation, and optimization.\n",
    "The representational capability of an IR is crucial during the\n",
    "compilation process. It must accurately depict source code without\n",
    "information loss, ensure the completeness of the source-to-target code\n",
    "compilation, and guarantee the effectiveness and performance of code\n",
    "optimization.\n",
    "\n",
    "As illustrated in Figure :numref:`ch04/ch04-IR`, IRs facilitate the representation of\n",
    "multiple source program languages from the frontend and enable the\n",
    "backend to connect to various target machines. Located between the\n",
    "frontend and backend is an optimizer, which allows for the addition of\n",
    "new optimization processes directly into the frontend and backend. These\n",
    "processes use existing IRs as input and generate new IRs as output. By\n",
    "analyzing and optimizing IRs, the optimizer enhances the extensibility\n",
    "of the compilation process and minimizes the impact that might be\n",
    "introduced during an optimization process on the frontend and backend.\n",
    "\n",
    "![Compiler's optimizationprocess](../img/ch04/IR-IR_structure.png)\n",
    ":label:`ch04/ch04-IR`\n",
    "\n",
    "With the ongoing evolution of compiler techniques, the development of\n",
    "IRs has progressed through three stages. In the initial stage, IRs were\n",
    "confined within a compiler and exclusively used by compiler developers.\n",
    "During the middle stage, when specific compilers became open source, IRs\n",
    "started being made publicly available, primarily for use by the users of\n",
    "compilers and related compilation tools. In the current stage, IRs are\n",
    "advancing toward facilitating an ecosystem of ecosystems (through a\n",
    "unified IR approach), encouraging increasing stakeholders (for example,\n",
    "hardware accelerator designers, machine learning framework users, and\n",
    "more) to participate in advertising AI computing.\n",
    "\n",
    "## Types of Intermediate Representations\n",
    "\n",
    "We will discuss various types of IR structures used by classical\n",
    "compilers. Understanding these IR structures is essential for analyzing\n",
    "source programs and generating optimized compiled code. Table\n",
    ":numref:`ch06/ch06-categorize` offers an overview of the\n",
    "different IR types. It is important to design IR structures carefully,\n",
    "considering the specific requirements of the compiler's design.\n",
    "\n",
    ":Types of IRs\n",
    "\n",
    "| IR Structure  | Characteristics                       | Examples |\n",
    "| --------------| --------------------------------------| ----------------------------------------------\n",
    "| Linear IR     | Based on linear code                  | Stack machine code, three-address code |\n",
    "| Graphical IR  | Based on graphs                       | Abstract syntax tree, directed acyclic graph |\n",
    "| Hybrid IR     | Based on both graphs and linear code  |LLVM IR |\n",
    ":label:`ch06/ch06-categorize`\n",
    "\n",
    "\n",
    "### Linear Intermediate Representation\n",
    "\n",
    "Linear IRs are widely used in compiler design, resembling assembly code\n",
    "for abstract machines. They represent the code to be compiled as a\n",
    "sequentially ordered series of operations. This ordering is important in\n",
    "practical terms. Linear IRs are popular because most processors utilize\n",
    "linear assembly languages.\n",
    "\n",
    "Two common types of linear IRs are stack machine code and three-address\n",
    "code . Stack machine code, a form of single-address code, offers a\n",
    "straightforward and compact representation. Instructions in stack\n",
    "machine code typically consist solely of an opcode that specifies an\n",
    "operation, with operands stored on a stack. Most instructions retrieve\n",
    "operands from the stack and push the results of their operations back\n",
    "onto it. On the other hand, three-address code (3AC) emulates the\n",
    "instruction format used in modern RISC machines. It employs a set of\n",
    "quadruples, each containing an operator and three addresses (two\n",
    "operands and one target). Figure\n",
    ":numref:`ch04/ch04-linearIR` illustrates the stack machine code\n",
    "and three-address code representations for the expression $a-b*5$.\n",
    "\n",
    "![Stack machine code and three-addresscode](../img/ch04/IR-linear_IR.png)\n",
    ":label:`ch04/ch04-linearIR`\n",
    "\n",
    "### Graphical Intermediate Representation\n",
    "\n",
    "Graphical IRs store information about the compilation process in the\n",
    "form of graphs. These graphs utilize nodes, edges, lists, trees, and\n",
    "other elements to collectively represent an algorithm. Although all\n",
    "graphical IRs consist of nodes and edges, they differ in terms of\n",
    "abstraction levels and graph structures. Common examples of graphical\n",
    "IRs include abstract syntax trees (ASTs), directed acyclic graphs\n",
    "(DAGs), and control-flow graphs (CFGs).\n",
    "\n",
    "An AST is a tree-structured IR that closely resembles the structure of\n",
    "the source code. Figure :numref:`ch04/ch04-AST_DAG` depicts the AST for the expression\n",
    "$a5+a5b$. It is worth noting that the AST contains two identical copies\n",
    "of $a5$, which introduces redundancy. To address this redundancy, the\n",
    "DAG offers a simplified representation where identical subtrees can be\n",
    "shared by multiple parent nodes. By reusing subtrees, the DAG reduces\n",
    "the cost of the evaluation process, especially when the compiler can\n",
    "verify that the value of $a$ remains constant.\n",
    "\n",
    "![AST and DAG](../img/ch04/IR-ASTDAG.png)\n",
    ":label:`ch04/ch04-AST_DAG`\n",
    "\n",
    "### Hybrid Intermediate Representation\n",
    "\n",
    "Hybrid IRs combine both linear IR and graphical IR elements. An example\n",
    "of a hybrid IR is LLVM IR , which is illustrated in Figure\n",
    ":numref:`ch04/ch04-LLVM_IR`. LLVM is an open-source compiler\n",
    "framework with the goal of providing unified IRs for different frontends\n",
    "and backends.\n",
    "\n",
    "In LLVM IR, linear IRs are used to construct basic blocks, while\n",
    "graphical IRs represent the control flow between these blocks. Each\n",
    "instruction within a basic block is presented as a static single\n",
    "assignment (SSA) . SSA requires each variable to be defined before use,\n",
    "with values assigned to them only once. Multiple SSA instructions form a\n",
    "linear list within a basic block.\n",
    "\n",
    "In the control flow graph (CFG), each node represents a basic block, and\n",
    "control transfer between these blocks is implemented through edges. This\n",
    "combination of linear IR for basic blocks and graphical IR for control\n",
    "flow allows for a flexible and efficient representation in LLVM IR.\n",
    "\n",
    "![LLVM IR](../img/ch04/IR-LLVMIR.png)\n",
    ":label:`ch04/ch04-LLVM_IR`\n",
    "\n",
    "## Intermediate Representation in Machine Learning Frameworks\n",
    "\n",
    "Classical IRs (such as LLVM IR) primarily target programming languages\n",
    "for general-purpose computation tasks, which falls short of satisfying\n",
    "the unique requirements of machine-learning-related computation. When\n",
    "designing IRs tailored for machine learning frameworks, certain vital\n",
    "factors warrant attention:\n",
    "\n",
    "-   **Tensor Representation**. Given the predominance of tensor data in\n",
    "    machine learning frameworks, it's imperative that the IRs can\n",
    "    effectively handle tensor representation.\n",
    "\n",
    "-   **Automatic Differentiation**. A core aspect of machine learning\n",
    "    involves evaluating derivatives of neural networks and optimizers\n",
    "    through automatic differentiation. Accordingly, IRs must prioritize\n",
    "    simplicity, performance, and scalability of higher-order\n",
    "    differentials for automatic differentiation.\n",
    "\n",
    "-   **Computational Graph Mode**. Machine learning frameworks like\n",
    "    TensorFlow, PyTorch, and MindSpore operate on two computational\n",
    "    graph modes: static and dynamic. The static mode, with pre-defined\n",
    "    computational graphs, enhances optimization but compromises on\n",
    "    flexibility. Conversely, the dynamic mode trades running speed for\n",
    "    flexibility and easier debugging by executing operators immediately\n",
    "    in the computational graph. IRs should therefore support both modes,\n",
    "    enabling users to choose the one best suited for their tasks while\n",
    "    building algorithm models.\n",
    "\n",
    "-   **Support for Higher-order Functions and Closures**. Essential in\n",
    "    functional programming, higher-order functions take or return\n",
    "    functions, while closures bundle code blocks with references to the\n",
    "    surrounding environment, facilitating access to an outer function's\n",
    "    scope from an inner function. Such support reduces redundant code,\n",
    "    improves abstraction, and enhances the flexibility and simplicity of\n",
    "    framework representations.\n",
    "\n",
    "-   **Compilation Optimization**. Machine learning frameworks lean on\n",
    "    compilation optimizations, including hardware-agnostic,\n",
    "    hardware-specific, and deployment- or inference-related\n",
    "    optimizations. These rely significantly on IRs implementations.\n",
    "\n",
    "-   **Just-in-Time (JIT) Compilation**. For expedited compilation and\n",
    "    execution in machine learning frameworks, JIT compilation is\n",
    "    frequently utilized. Optimization of JIT compilation, including loop\n",
    "    unrolling, fusion, and inlining, plays a crucial role in optimizing\n",
    "    parts of data flow graphs in IRs. A flawed IR design could\n",
    "    potentially hamper JIT compilation performance in machine learning\n",
    "    frameworks, thereby impacting the program's running capabilities.\n",
    "\n",
    "Considering these factors, developers persistently refine classical IRs\n",
    "and introduce new IRs specifically tailored for machine learning\n",
    "frameworks. In the following section, we will delve into the IRs\n",
    "employed by various machine learning frameworks.\n",
    "\n",
    "### Intermediate Representation in PyTorch\n",
    "\n",
    "PyTorch is a dynamic, Python-oriented machine learning framework.\n",
    "Renowned for its usability and flexibility, PyTorch simplifies the\n",
    "process of writing and debugging machine learning programs. It\n",
    "introduces TorchScript, a method used for constructing serializable and\n",
    "optimizable models during the saving and loading of neural networks.\n",
    "\n",
    "Particularly, TorchScript IR employs JIT compilation to convert Python\n",
    "code into target model files. All TorchScript programs can be saved\n",
    "within the Python process and later loaded into processes devoid of\n",
    "Python dependencies.\n",
    "\n",
    "Aligning with the imperative programming paradigm, PyTorch incorporates\n",
    "the TorchScript IR, composed primarily of Single Static Assignment\n",
    "(SSA)-based linear IRs, to represent Python code. This representation\n",
    "can be achieved through either the Tracing or Scripting method of JIT\n",
    "compilation. TorchScript IR not only amplifies model deployment\n",
    "capabilities but also bolsters compilation performance. Additionally,\n",
    "TorchScript IR greatly improves the model visualization within the\n",
    "PyTorch framework.\n",
    "\n",
    "Code `lst:torchscript` illustrates the use of the Scripting method\n",
    "to print a TorchScript IR graph.\n",
    "\n",
    "**lst:torchscript**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def test_func(input):\n",
    "    rv = 10.0\n",
    "    for i in range(5):\n",
    "    rv = rv + input\n",
    "    rv = rv/2\n",
    "    return rv\n",
    "    \n",
    "    print(test_func.graph)\n",
    "```\n",
    "\n",
    "Code `lst:torchscriptir` shows the structure of this IR graph.\n",
    "\n",
    "**lst:torchscriptir**\n",
    "\n",
    "```\n",
    "graph(%input.1 : Tensor):\n",
    "    %9 : int = prim::Constant[value=1]()\n",
    "    %5 : bool = prim::Constant[value=1]() # test.py:6:1\n",
    "    %rv.1 : float = prim::Constant[value=10.]() # test.py:5:6\n",
    "    %2 : int = prim::Constant[value=5]() # test.py:6:16\n",
    "    %14 : int = prim::Constant[value=2]() # test.py:8:10\n",
    "    %rv : float = prim::Loop(%2, %5, %rv.1) # test.py:6:1\n",
    "    block0(%i : int, %rv.9 : float):\n",
    "    %rv.3 : Tensor = aten::add(%input.1, %rv.9, %9) # <string>:5:9\n",
    "    %12 : float = aten::FloatImplicit(%rv.3) # test.py:7:2\n",
    "    %rv.6 : float = aten::div(%12, %14) # test.py:8:7\n",
    "    -> (%5, %rv.6)\n",
    "    return (%rv)\n",
    "```\n",
    "\n",
    "### Intermediate Representation in JAX\n",
    "\n",
    "The JAX framework facilitates both static and dynamic computational\n",
    "graphs and employs the Jax Program Representation (Jaxpr) IR. This IR\n",
    "ensures that the output, not reliant on global variables, depends solely\n",
    "on the input, with both input and output encapsulating typed\n",
    "information. Functionality-wise, Jaxpr IR supports an array of features\n",
    "such as loops, branching, recursion, closure function differentiation,\n",
    "third-order differentiation, as well as backpropagation and forward\n",
    "propagation in automatic differentiation.\n",
    "\n",
    "Jaxpr IR utilizes the A-normal Form (ANF), a form of functional\n",
    "expression, demonstrated in\n",
    "Code `lst:ANF`\n",
    "via the ANF grammar.\n",
    "\n",
    "**lst:ANF**\n",
    "\n",
    "```\n",
    "<aexp> ::=  NUMBER | STRING | VAR | BOOLEAN | PRIMOP\n",
    "    |  (lambda (VAR ...) <exp>)\n",
    "    <cexp> ::=  (<aexp> <aexp> ...)\n",
    "    | (if <aexp> <exp> <exp>)\n",
    "    <exp> ::=  (let ([VAR <cexp>]) <exp>) | <cexp> | <aexp>\n",
    "```\n",
    "\n",
    "The ANF segregates expressions into atomic expressions (aexp) and\n",
    "compound expressions (cexp). Atomic expressions represent constants,\n",
    "variables, primitives, and anonymous functions, while compound\n",
    "expressions, comprising several atomic expressions, can be viewed as\n",
    "invocations of anonymous or primitive functions. The first input in a\n",
    "cexp represents the invoked function, and all subsequent inputs\n",
    "symbolize the invoked parameters.\n",
    "\n",
    "Code `lst:JaxCode` displays the Jaxpr corresponding to a function.\n",
    "\n",
    "**lst:JaxCode**\n",
    "\n",
    "```python\n",
    "from jax import make_jaxpr\n",
    "    import jax.numpy as jnp\n",
    "    \n",
    "    def test_func(x, y):\n",
    "    ret = x + jnp.sin(y) * 3\n",
    "    return jnp.sum(ret)\n",
    "    \n",
    "    print(make_jaxpr(test_func)(jnp.zeros(8), jnp.ones(8)))\n",
    "```\n",
    "\n",
    "The structure of this Jaxpr is shown in\n",
    "Code `lst:JaxPr`.\n",
    "\n",
    "**lst:JaxPr**\n",
    "\n",
    "```\n",
    "{ lambda ; a:f32[8] b:f32[8]. let\n",
    "        c:f32[8] = sin b\n",
    "        d:f32[8] = mul c 3.0\n",
    "        e:f32[8] = add a d\n",
    "        f:f32[] = reduce_sum[axes=(0,)] e\n",
    "        in (f,) }\n",
    "```\n",
    "\n",
    "### Intermediate Representation in TensorFlow\n",
    "\n",
    "TensorFlow utilizes dataflow programming to execute numerical\n",
    "computations through dataflow graphs. TensorFlow's static graph\n",
    "mechanism progresses through a series of abstractions and analyses when\n",
    "running a program, transforming it from higher-level to lower-level IRs,\n",
    "a process referred to as \\\"lowering\\\".\n",
    "\n",
    "To cater to diverse hardware platforms, TensorFlow employs a range of IR\n",
    "designs. As illustrated in\n",
    "Figure :numref:`ch04/ch04-tensorflow_ecosystem`, the blue boxes denote\n",
    "graph-based IRs while the green ones indicate SSA-based IRs. During the\n",
    "IR transformation, each level optimizes the IR independently, precluding\n",
    "communication with other levels. This absence of awareness about\n",
    "optimizations performed at other levels necessitates optimal\n",
    "implementation at each level, often leading to repetitive tasks and\n",
    "sub-optimal efficiency. Notably, transitioning from graph-based IRs to\n",
    "SSA-based IRs involves a qualitative transformation that incurs\n",
    "significant costs. The inability to reuse the same optimization code\n",
    "across levels also hampers development efficiency.\n",
    "\n",
    "Multi-level IRs present a mixed bag of advantages and disadvantages. On\n",
    "the plus side, they offer flexible representations, pass-based\n",
    "optimization at varying levels, and efficient optimization algorithms.\n",
    "On the downside, they pose challenges due to their inherent\n",
    "characteristics: The transformation between different IRs often\n",
    "complicates full compatibility implementation, thereby increasing\n",
    "engineering workload and potentially leading to information loss. This\n",
    "might make lower-level optimization challenging if information at a\n",
    "higher level has been optimized. To mitigate such information loss, we\n",
    "can impose stricter constraints on the optimization sequence.\n",
    "Additionally, choosing the level for implementing certain optimizations\n",
    "that can be performed at two adjacent levels can be a conundrum for\n",
    "framework developers. Finally, defining distinct operator granularities\n",
    "at different levels might impact accuracy to a certain degree.\n",
    "\n",
    "![TensorFlow's IRdesign](../img/ch04/IR-MLIR.png)\n",
    ":label:`ch04/ch04-tensorflow_ecosystem`\n",
    "\n",
    "### Multi-Level Intermediate Representation\n",
    "\n",
    "Multi-Level Intermediate Representation (MLIR) serves as a unified\n",
    "platform for IRs rather than being a specific type of IR. Leveraging the\n",
    "infrastructure provided by MLIR, developers can define IRs to suit their\n",
    "needs. Thus, MLIR can be interpreted as a \\\"compiler of compilers\\\". It\n",
    "expands beyond the TensorFlow framework and can be used to construct IRs\n",
    "linking other languages to backend platforms (such as LLVM).\n",
    "\n",
    "Despite the design of MLIR being heavily influenced by LLVM, MLIR\n",
    "fosters a more open ecosystem. Given that MLIR does not confine\n",
    "developers to a set group of operation or abstraction types, it offers\n",
    "more latitude to define IRs and solve specific problems. To facilitate\n",
    "this extensibility, MLIR introduces the concept of \\\"dialects\\\". These\n",
    "provide a grouping mechanism for abstraction under a unique namespace.\n",
    "Each dialect lays out a production and associates an operation to an IR,\n",
    "thus producing an MLIR-typed IR. Within MLIR, the \\\"operation\\\" is the\n",
    "fundamental unit of abstraction and computation. Operations can carry\n",
    "application-specific semantics and encapsulate all the core IR\n",
    "structures in LLVM, including instructions, functions, modules, etc.\n",
    "\n",
    "The MLIR assembly for an operation is illustrated as follows:\n",
    "\n",
    "```\n",
    "%tensor = \"toy.transpose\"(%tensor) {inplace = true} : (tensor<2x3xf64>) -> tensor<3x2xf64> loc(\"example/file/path\":12:1)\n",
    "```\n",
    "\n",
    "This MLIR operation can be dissected as follows:\n",
    "\n",
    "-   %tensor: The identifier for the result defined by this operation\n",
    "    (prefixed with a $\\%$ to prevent naming conflicts). An operation may\n",
    "    define no results or multiple results, represented as SSA values.\n",
    "\n",
    "-   \\\"toy.transpose\\\": The operation name. It is usually a unique\n",
    "    string, with the dialect's namespace prefixing the \".\". This refers\n",
    "    to the transpose operation within the toy dialect.\n",
    "\n",
    "-   (%tensor): A list that can contain zero or more input operands (or\n",
    "    arguments), which are SSA values defined by other operations or that\n",
    "    refer to block arguments.\n",
    "\n",
    "-   inplace = true: A dictionary that may contain zero or more\n",
    "    attributes. These are constant special operands. Here, a boolean\n",
    "    attribute named `inplace` with a constant value of `true` is\n",
    "    defined.\n",
    "\n",
    "-   (tensor\\<2x3xf64\\>)-\\>tensor\\<3x2xf64\\>: This represents the\n",
    "    operation type in a functional form, specifying the input before the\n",
    "    arrow and output after. The data types and shapes of the input and\n",
    "    output are contained within the parentheses. For instance,\n",
    "    $<2x3xf64>$ represents a tensor with a shape of `(2, 3)` and data\n",
    "    type `float64`.\n",
    "\n",
    "-   loc(\\\"example/file/path\\\":12:1): This refers to the source code\n",
    "    location from where this operation originated.\n",
    "\n",
    "As each level's IR design adheres to this assembly, it simplifies\n",
    "transformation across levels, boosting the efficiency of IR\n",
    "transformation. Moreover, different levels can interact to optimize the\n",
    "IRs, enabling optimization to be performed at the most suitable level,\n",
    "thereby negating the need for optimal performance at each level. By\n",
    "transforming them into the IR at the most appropriate level, other IRs\n",
    "can be optimized, enhancing both optimization and development\n",
    "efficiency. TensorFlow can also employ MLIR to perform multi-layer\n",
    "transformation from graph-based IRs to\n",
    "\n",
    "### Intermediate Representation in MindSpore\n",
    "\n",
    "MindSpore adopts graph-based functional IRs, known as MindSpore IR\n",
    "(abbreviated to MindIR). MindIR employs a unified IR approach instead of\n",
    "a multi-level IR structure, outlining the network's logical structure\n",
    "and operator attributes. This approach obliterates model disparities\n",
    "across different backends, facilitating connections to various target\n",
    "machines.\n",
    "\n",
    "MindIR primarily caters to the automatic differential transformation. It\n",
    "implements a transformation method grounded in functional programming\n",
    "frameworks, thereby making it similar to ANF (A-Normal Form) functional\n",
    "semantics. Its defining characteristics include:\n",
    "\n",
    "1.  **Graph-based Representation**. MindSpore represents programs as\n",
    "    graphs which are conducive to optimization. MindSpore treats\n",
    "    functions as essential elements of a machine learning program,\n",
    "    allowing for recursive invocation, parameter passing, or returning\n",
    "    from other functions. This ability paves the way for representing a\n",
    "    range of control flow structures.\n",
    "\n",
    "2.  **Purely Functional**. In a purely functional context, the function\n",
    "    outcomes depend solely on parameters. Side effects are potential\n",
    "    issues when a function relies on or affects external states, such as\n",
    "    global variables. These can lead to incorrect results if code\n",
    "    execution sequence isn't strictly maintained. These side effects can\n",
    "    also impact automatic differentiation, necessitating the requirement\n",
    "    for pure functions. MindIR has the capability to transform\n",
    "    representations with side effects into purely functional\n",
    "    representations, ensuring correct code execution sequence while\n",
    "    upholding ANF functional semantics and enabling a higher degree of\n",
    "    automatic differentiation freedom.\n",
    "\n",
    "3.  **Closure Representation**. Reverse mode automatic differentiation\n",
    "    requires the storage of basic operation intermediate results in\n",
    "    closures for a combined connection. Closures, the combination of a\n",
    "    code block bundled with references to its surrounding environment,\n",
    "    become particularly crucial. In MindIR, the code block takes the\n",
    "    shape of a function diagram, with the surrounding environment\n",
    "    interpreted as the function invocation context.\n",
    "\n",
    "4.  **Strongly Typed**. Each node requires a specific type for achieving\n",
    "    optimal performance. This is particularly crucial in machine\n",
    "    learning frameworks where operator execution can be time-consuming.\n",
    "    Detecting errors at the earliest can help save valuable time.\n",
    "    MindIR's type and shape inference capabilities thus center on the\n",
    "    support for function invocation and higher-order functions.\n",
    "\n",
    "Figure :numref:`ch04/ch04-MindIR` outlines the MindIR grammar based on\n",
    "MindSpore framework's characteristics. ANode corresponds to an atomic\n",
    "expression in ANF, ValueNode represents the constant value,\n",
    "ParameterNode signifies the function's formal parameter, and CNode\n",
    "(corresponding to a compound expression in ANF) indicates function\n",
    "invocation.\n",
    "\n",
    "![MindIR grammar](../img/ch04/IR-MindIR.png)\n",
    ":label:`ch04/ch04-MindIR`\n",
    "\n",
    "The example provided below in Code 1 offers a deeper analysis of MindIR.\n",
    "\n",
    "**lst:MindSporeCode**\n",
    "\n",
    "```\n",
    "def func(x, y):\n",
    "    return x / y\n",
    "    \n",
    "    @ms_function\n",
    "    def test_f(x, y):\n",
    "    a = x - 1\n",
    "    b = a + y\n",
    "    c = b * func(a, b)\n",
    "    return c\n",
    "```\n",
    "\n",
    "The ANF expression corresponding to this function is demonstrated in\n",
    "Code `lst:MindIR`.\n",
    "\n",
    "**lst:MindIR**\n",
    "\n",
    "```\n",
    "lambda (x, y)\n",
    "    let a = x - 1 in\n",
    "    let b = a + y in\n",
    "    let func = lambda (x, y)\n",
    "    let ret = x / y in\n",
    "    ret end in\n",
    "    let %1 = func(a, b) in\n",
    "    let c = b * %1 in\n",
    "    c end\n",
    "```\n",
    "\n",
    "In ANF, each expression is encapsulated as a variable utilizing the\n",
    "`let` expression, with dependencies on the expression's output\n",
    "represented via variable references. In contrast, MindIR packages each\n",
    "expression as a node, portraying dependencies through directed edges\n",
    "connecting the nodes.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}