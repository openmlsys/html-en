{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c2880b",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Parallelism Methods\n",
    "\n",
    "This section explores the prevalent methods for implementing distributed\n",
    "training systems, discussing the design goals and a detailed examination\n",
    "of each parallelism approach.\n",
    "\n",
    "## Classification of Methods\n",
    "\n",
    "Distributed training amalgamates multiple single-node training systems\n",
    "into a parallel structure to expedite the training process without\n",
    "sacrificing model accuracy. A single-node training system, depicted in\n",
    "Figure :numref:`ch010/ch10-single-node`, processes training datasets\n",
    "split into small batches, termed as mini-batches. Here, a mini-batch of\n",
    "*data* is input into the model, guided by a training *program*, which\n",
    "generates gradients to enhance model accuracy. Typically, this program\n",
    "executes a deep neural network. To illustrate the execution of a neural\n",
    "network, we employ a computational graph, comprising connected\n",
    "operators. Each operator executes a layer of the neural network, storing\n",
    "parameters to be updated during the training.\n",
    "\n",
    "![Single-node trainingsystem](../img/ch10/ch10-single-node.png)\n",
    ":label:`ch010/ch10-single-node`\n",
    "\n",
    "The execution of a computational graph involves two phases: *forward*\n",
    "and *backward* computation. In the forward phase, data is fed into the\n",
    "initial operator, which calculates and generates the data required by\n",
    "the downstream operator. This process is continued sequentially through\n",
    "all operators until the last one concludes its computation. The backward\n",
    "phase initiates from the last operator, computing gradients and updating\n",
    "local parameters accordingly. The process culminates at the first\n",
    "operator. Upon completion of these two phases for a given mini-batch,\n",
    "the system loads the next mini-batch to update the model.\n",
    "\n",
    "Considering a model training job, partitioning the *data* and *program*\n",
    "can facilitate parallel acceleration. Table\n",
    "`ch010/ch10-parallel-methods` compiles various partition\n",
    "methods. Single-node training systems enable a \\\"single program, single\n",
    "data\\\" paradigm. For parallel computing across multiple devices, data is\n",
    "partitioned and the program is replicated for simultaneous execution,\n",
    "creating a \\\"single program, multiple data\\\" or *data parallelism*\n",
    "paradigm. Another approach involves partitioning the program,\n",
    "distributing its operators across devices---termed as \\\"multiple\n",
    "programs, single data\\\" or *model parallelism*. When training\n",
    "exceptionally large AI models, both data and program are partitioned to\n",
    "optimize the degree of parallelism (DOP), yielding a \\\"multiple program,\n",
    "multiple data\\\" or *hybrid parallelism* paradigm.\n",
    "\n",
    ":Parallelism methods\n",
    "|  Classification  |      Single Data      |   Multiple Data |\n",
    "|------------------|-----------------------|-------------------- |\n",
    "|  Single program  | single-node execution |  data parallelism |\n",
    "| Multiple program |   model parallelism   | hybrid parallelism |\n",
    ":label:`ch010/ch10-parallel-methods`\n",
    "\n",
    "\n",
    "## Data Parallelism\n",
    "\n",
    "Data parallelism is used when a single node cannot provide sufficient\n",
    "computing power. This is the most common parallelism approach adopted by\n",
    "AI frameworks. Specific implementations include TensorFlow\n",
    "DistributedStrategy, PyTorch Distributed, and Horovod\n",
    "DistributedOptimizer. Given a data-parallel system, assume that the\n",
    "training batch size is $N$, and that there are $M$ devices available for\n",
    "parallel acceleration. To achieve data parallelism, the batch size is\n",
    "partitioned into $M$ partitions, with each device getting $N/M$ training\n",
    "samples. Sharing a replica of the training program, each device executes\n",
    "and calculates a gradient separately over its own data partition. Each\n",
    "device (indexed $i$) calculates a gradient $G_i$ based on local training\n",
    "samples. To ensure that training program parameters are coherent, local\n",
    "gradients $G_i$ on different devices are aggregated to calculate an\n",
    "average gradient $(\\sum_{i=1}^{N} G_i) / N$. To complete the training on\n",
    "this mini-batch, the training program updates model parameters based on\n",
    "the average gradient.\n",
    "\n",
    "Figure :numref:`ch010/ch10-data-parallel` shows a data-parallel training\n",
    "system composed of two devices. For a batch size of 64, each device is\n",
    "assigned 32 training samples and shares the same neural network\n",
    "parameters (or program replicas). The local training samples are passed\n",
    "through the operators in the program replica in sequence for forward and\n",
    "backward computation. During backward computation, the program replicas\n",
    "generate local gradients. Corresponding local gradients on different\n",
    "devices (e.g., gradient 1 on device 1 and gradient 1 on device 2) are\n",
    "aggregated (typically by AllReduce, a collective communication\n",
    "operation) to calculate an average gradient.\n",
    "\n",
    "![Data-parallelsystem](../img/ch10/ch10-data-parallel.png)\n",
    ":label:`ch010/ch10-data-parallel`\n",
    "\n",
    "## Model Parallelism\n",
    "\n",
    "Model parallelism is useful when memory constraints make it impossible\n",
    "to train a model on a single device. For example, the memory on a single\n",
    "device will be insufficient for a model that contains a large operator\n",
    "(such as the compute-intensive fully connected layer for classification\n",
    "purpose). In such cases, we can partition this large operator for\n",
    "parallel execution. Assume that the operator has $P$ parameters and the\n",
    "system consists of $N$ devices. To minimize the workload on each device\n",
    "given the limited memory capacity, we can evenly assign the parameters\n",
    "across the devices ($P/N$ = number of parameters per device). This\n",
    "partitioning method is called **intra-operator parallelism**, which is a\n",
    "typical application of model parallelism.\n",
    "\n",
    "Figure :numref:`ch010/ch10-model-parallel-intra-op` shows an example of\n",
    "intra-operator parallelism implemented by two devices. The neural\n",
    "network in this example consists of two operators. To complete forward\n",
    "and backward computation, operator 1 and operator 2 require 16 GB and 1\n",
    "GB of memory, respectively. However, in this example, the maximum amount\n",
    "of memory a single device can provide is only 10 GB. To train this\n",
    "network, parallelism is implemented on operator 1. Specifically, the\n",
    "parameters of operator 1 are evenly partitioned into two partitions\n",
    "between device 1 and device 2, meaning that device 1 runs program\n",
    "partition 1 while device 2 runs program partition 2. The network\n",
    "training process starts with feeding a mini-batch of training data to\n",
    "operator 1. Because the parameters of operator 1 are shared between two\n",
    "devices, the data is broadcast to the two devices. Each device completes\n",
    "forward computation based on the local partition of parameters. The\n",
    "local computation results on the devices are aggregated before being\n",
    "sent to downstream operator 2. In backward computation, the data of\n",
    "operator 2 is broadcast to device 1 and device 2, so that each device\n",
    "completes backward computation based on the local partition of\n",
    "operator 1. The local computation results on the devices are aggregated\n",
    "and returned to complete the backward computation process.\n",
    "\n",
    "![Model-parallel system: intra-operatorparallelism](../img/ch10/ch10-model-parallel-intra-op.png)\n",
    ":label:`ch010/ch10-model-parallel-intra-op`\n",
    "\n",
    "In some cases, the overall model --- rather than specific operators ---\n",
    "requires more memory than a single device can provide. Given $N$\n",
    "operators and $M$ devices, we can evenly assign the operators across $M$\n",
    "devices. As such, each device needs to run forward and backward\n",
    "computation of only $N/M$ operators, thereby reducing the memory\n",
    "overhead of each device. This application of model parallelism is called\n",
    "*inter-operator parallelism*.\n",
    "\n",
    "Figure :numref:`ch010/ch10-model-parallel-inter-op` shows an example of\n",
    "inter-operator parallelism implemented by two devices. The neural\n",
    "network in this example has two operators, each requiring 10 GB of\n",
    "memory for computation (20 GB in total). Because the maximum memory a\n",
    "single device can provide in this example is 10 GB, we can place\n",
    "operator 1 on device 1 and operator 2 on device 2. In forward\n",
    "computation, the output of operator 1 is sent to device 2, which uses\n",
    "this output as input to complete forward computation of operator 2. In\n",
    "backward computation, device 2 sends the backward computation result of\n",
    "operator 2 to device 1 for backward computation of operator 1,\n",
    "completing the training on a mini-batch.\n",
    "\n",
    "![Model-parallel system: inter-operatorparallelism](../img/ch10/ch10-model-parallel-inter-op.png)\n",
    ":label:`ch010/ch10-model-parallel-inter-op`\n",
    "\n",
    "## Hybrid Parallelism\n",
    "\n",
    "In training large AI models, the computing power and memory constraints\n",
    "often go hand in hand. The solution to overcoming these constraints is\n",
    "to adopt a hybrid of data parallelism and model parallelism, that is,\n",
    "hybrid parallelism. Figure\n",
    ":numref:`ch010/ch10-hybrid-parallel` shows an example of hybrid\n",
    "parallelism implemented by four devices. In this example, inter-operator\n",
    "parallelism is adopted to reduce memory overheads by allocating operator\n",
    "1 to device 1 and operator 2 to device 2. Device 3 and device 4 are\n",
    "added to the system to achieve data parallelism, thereby improving the\n",
    "computing power of the system. Specifically, the training data is\n",
    "partitioned to data partitions 1 and 2, and the model (consisting of\n",
    "operators 1 and 2) is replicated on devices 3 and 4 respectively. This\n",
    "makes it possible for the program replicas to run in parallel. During\n",
    "forward computation, devices 1 and 3 run the replicas of operator 1\n",
    "simultaneously and send their respective computation results to devices\n",
    "2 and 4 to compute the replicas of operator 2. During backward\n",
    "computation, devices 2 and 4 compute gradients simultaneously, and the\n",
    "local gradients are averaged by using the AllReduce operation. The\n",
    "averaged gradient is back-propagated to the replicas of operator 1 on\n",
    "devices 1 and 3, and the backward computation process ends.\n",
    "\n",
    "![Hybrid-parallelsystem](../img/ch10/ch10-hybrid-parallel.png)\n",
    ":label:`ch010/ch10-hybrid-parallel`\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}