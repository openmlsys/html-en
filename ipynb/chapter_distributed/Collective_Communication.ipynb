{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6264064",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Collective Communication\n",
    "\n",
    "This section delves into the application of collective communication in\n",
    "the creation of distributed training systems within machine learning\n",
    "clusters. Collective communication, a fundamental aspect of parallel\n",
    "computing, is instrumental in developing high-performance Single Program\n",
    "Multiple Data (SPMD) programs. We will begin by discussing common\n",
    "operators within collective communication. Following this, we explore\n",
    "the use of the AllReduce algorithm to alleviate network bottlenecks in\n",
    "distributed training systems. Lastly, we will address the support\n",
    "available for different collective communication algorithms within\n",
    "existing machine learning systems.\n",
    "\n",
    "## Collective Communication Operators\n",
    "\n",
    "In this subsection, we will establish a simplified model of collective\n",
    "communication before introducing commonly used collective communication\n",
    "operators. These include Broadcast, Reduce, AllGather, Scatter, and\n",
    "AllReduce:\n",
    "\n",
    "![Examples of collective communicationoperators](../img/ch10/ch10-collective-operators.png)\n",
    ":label:`ch010/ch10-collective-operators`\n",
    "\n",
    "-   **Broadcast**: The Broadcast operator is often employed in a\n",
    "    distributed machine learning system to transmit model parameters or\n",
    "    configuration files from device $i$ to all other devices. The\n",
    "    starting and final states of this operation, initiated by device 1\n",
    "    in a three-device cluster, are depicted in Figure\n",
    "    :numref:`ch010/ch10-collective-operators`.\n",
    "\n",
    "-   **Reduce**: In a distributed machine learning system, the Reduce\n",
    "    operator plays a pivotal role by consolidating computation results\n",
    "    from different devices. It is commonly used to aggregate local\n",
    "    gradients from each device to compute the gradient summation. This\n",
    "    operator employs functions, represented as $f$, which often obey the\n",
    "    associative and commutative laws. Such functions, including sum,\n",
    "    prod, max, and min, are initiated by all devices, with the final\n",
    "    aggregate result stored in device $i$. The initial and final states\n",
    "    when device 1 executes the Reduce operator for summation are\n",
    "    depicted in Figure\n",
    "    :numref:`ch010/ch10-collective-operators`.\n",
    "\n",
    "-   **AllReduce**: The AllReduce operator, a part of collective\n",
    "    communication, stores the result of the Reduce function $f$ in all\n",
    "    devices. Figure\n",
    "    :numref:`ch010/ch10-collective-operators` shows the starting\n",
    "    and ending states when devices 1, 2, and 3 jointly execute AllReduce\n",
    "    to perform a summation.\n",
    "\n",
    "-   **Gather**: The Gather operator can gather data from all devices and\n",
    "    store it in device $i$. Figure\n",
    "    :numref:`ch010/ch10-collective-operators` shows the initial\n",
    "    and end states when device 1 invokes the Gather operator to gather\n",
    "    data from all devices.\n",
    "\n",
    "-   **AllGather**: The AllGather operator sends the gather result to all\n",
    "    devices. Figure\n",
    "    :numref:`ch010/ch10-collective-operators` shows the initial\n",
    "    and end states when devices 1, 2, and 3 invoke the AllGather\n",
    "    operator.\n",
    "\n",
    "-   **Scatter**: The Scatter operator is the inverse of the Gather\n",
    "    operator. Figure\n",
    "    :numref:`ch010/ch10-collective-operators` shows the initial\n",
    "    and end states when device 1 invokes the Scatter operator.\n",
    "\n",
    "It's important to note that other collective communication operators may\n",
    "also be deployed in distributed machine learning applications. Examples\n",
    "of these are ReduceScatter, Prefix Sum, Barrier, and All-to-All.\n",
    "However, this section will not delve into the specifics of these\n",
    "operators.\n",
    "\n",
    "## Gradient Averaging with AllReduce\n",
    "\n",
    "The following discusses how to utilize AllReduce operators to implement\n",
    "efficient gradient averaging in large clusters. We can implement a\n",
    "simple method for computing the average gradient, whereby a device in\n",
    "the cluster gathers local gradients from each device and then broadcasts\n",
    "the computed average gradient to all devices. Although this approach is\n",
    "easy to implement, it leads to two problems. 1) Network congestion may\n",
    "occur if multiple devices send data to the gather device simultaneously.\n",
    "2) It is not feasible to fit gradient averaging computation on a single\n",
    "device due to the computing power constraint.\n",
    "\n",
    "To solve the preceding problems, the Reduce-Broadcast implementation of\n",
    "the AllReduce operator can be used to optimize the algorithm. In this\n",
    "implementation, all nodes participate in network communication and\n",
    "averaging computation of gradients so that the huge amount of network\n",
    "and computing overheads is evenly shared across all nodes. This\n",
    "implementation can solve the two problems of a single gradient gather\n",
    "node. Assume that there are $M$ devices, and that each device stores a\n",
    "model replica consisting of $N$ parameters/gradients. According to the\n",
    "requirements of AllReduce, all parameters need to be partitioned into\n",
    "$M$ partitions based on the number of devices, with each partition\n",
    "containing $N/M$ parameters. The initial and end states of the algorithm\n",
    "are provided.\n",
    "\n",
    "In the AllReduce example shown in Figure\n",
    ":numref:`ch010/ch10-collective-operators`, there are three\n",
    "devices. Each device has a model replica, and each replica has 3\n",
    "parameters. According to the partitioning method of AllReduce,\n",
    "parameters are partitioned into three partitions (because there are 3\n",
    "devices), and each partition has 1 ($N/M$ = 3/3) parameter. In this\n",
    "example, assume that device 1 has parameters 2, 4, and 6; device 2 has\n",
    "parameters 1, 2, and 3; and device 3 has parameters 4, 8, and 12. After\n",
    "an AllReduce operator is used for computation, the gradient summation\n",
    "results 7, 14, and 21 are sent to all devices. The result 7 of partition\n",
    "1 is the sum of the initial results of partition 1 in the three devices\n",
    "(7 = 1 + 2 + 4). To compute the average gradient, the sum of gradients\n",
    "needs to be divided by the number of devices (e.g., to obtain the final\n",
    "result of partition 1, divide 7 by 3).\n",
    "\n",
    "The AllReduce operator splits the gradient computation into $M-1$ Reduce\n",
    "operators and $M-1$ Broadcast operators (where $M$ indicates the number\n",
    "of nodes). Reduce operators are used to compute the summation of\n",
    "gradients, and Broadcast operators are used to broadcast the summation\n",
    "of gradients to all nodes.\n",
    "\n",
    "Figure :numref:`ch010/ch10-allreduce-process` shows the execution\n",
    "process of an AllReduce operator. The AllReduce operator starts with a\n",
    "Reduce operator. In the first Reduce operator, the AllReduce operator\n",
    "performs pairing on all nodes and enables them to jointly complete\n",
    "gradient summation. In the first Reduce operator shown in Figure\n",
    ":numref:`ch010/ch10-allreduce-process`, devices 1 and 2 are\n",
    "paired to jointly complete the summation of data in partition 1. Device\n",
    "2 sends local gradient data 1 to device 1, which adds up the received\n",
    "gradient data 1 and gradient data 2 stored in local partition 1 to\n",
    "obtain the intermediate gradient summation result 3. At the same time,\n",
    "devices 1 and 3 are paired to jointly complete the summation of data in\n",
    "partition 3, and devices 3 and 2 are paired to jointly complete the\n",
    "summation of data in partition 2.\n",
    "\n",
    "![Process of the AllReducealgorithm](../img/ch10/ch10-allreduce-process.png)\n",
    ":label:`ch010/ch10-allreduce-process`\n",
    "\n",
    "Such distributed computing of gradients performed by Reduce operators\n",
    "realizes the following performance optimizations:\n",
    "\n",
    "1.  **Network optimization:** All devices receive and send data\n",
    "    simultaneously by utilizing their ingress and egress bandwidths.\n",
    "    Therefore, in the execution process of the AllReduce algorithm, the\n",
    "    available bandwidth is $M * B$, where $M$ indicates the number of\n",
    "    nodes and $B$ indicates the node bandwidth. This enables the system\n",
    "    to implement network bandwidth scalability.\n",
    "\n",
    "2.  **Computing power optimization:** Processors of all devices\n",
    "    participate in the gradient summation. Therefore, in the execution\n",
    "    process of the AllReduce algorithm, the total number of available\n",
    "    processors is $M * P$, where $M$ indicates the number of nodes and\n",
    "    $P$ indicates the number of processors for a single device. This\n",
    "    enables the system to implement computing scalability.\n",
    "\n",
    "3.  **Load balancing:** Data partitions are evenly partitioned.\n",
    "    Therefore, the communication and computing overheads allocated to\n",
    "    each device are the same.\n",
    "\n",
    "In the Reduce operators other than the first one, the AllReduce\n",
    "algorithm selects other pairing methods for different data partitions.\n",
    "For example, in the second Reduce operator shown in Figure\n",
    ":numref:`ch010/ch10-allreduce-process`, the AllReduce algorithm\n",
    "pairs devices 1 and 3 for data summation in partition 1. Devices 1 and 2\n",
    "are paired for data summation in partition 2, and devices 2 and 3 are\n",
    "paired for data summation in partition 3. In a three-node AllReduce\n",
    "cluster, after two Reduce operators complete execution, the data\n",
    "summation result of each partition is obtained. The data summation\n",
    "result (7) of partition 1 is stored on device 3, the data summation\n",
    "result (14) of partition 2 is stored on device 1, and the data summation\n",
    "result (21) of partition 3 is stored on device 2.\n",
    "\n",
    "The AllReduce algorithm then enters the broadcast phase. The process in\n",
    "this phase is similar to the execution process of Reduce operators. The\n",
    "core difference is that, after nodes are paired, they do not add up data\n",
    "--- instead, they broadcast the computation results of Reduce operators.\n",
    "In the first Broadcast operator shown in Figure\n",
    ":numref:`ch010/ch10-allreduce-process`, device 1 directly writes\n",
    "the result (14) of partition 2 to partition 2 of device 3. Device 2\n",
    "directly writes the result (21) of partition 3 to device 1, and device 3\n",
    "directly writes the result of partition 1 to device 2. In a three-node\n",
    "AllReduce cluster, the Broadcast operator is repeated twice in order to\n",
    "notify all nodes of the Reduce computation result of each partition.\n",
    "\n",
    "## Model Training with Collective Communication\n",
    "\n",
    "Typically, a machine learning system flexibly combines different\n",
    "collective communication operators for different clusters to maximize\n",
    "communication efficiency. The following describes two cases: ZeRO and\n",
    "DALL-E.\n",
    "\n",
    "ZeRO is a neural network optimizer proposed by Microsoft. In practice,\n",
    "ZeRO successfully trained the world's largest language model in 2020\n",
    "(with up to 17 billion parameters). In the training process of a neural\n",
    "network like this, parameters of the optimizer, gradients obtained\n",
    "during backward computation, and model parameters all impose significant\n",
    "pressure on the memory space of accelerators. If parameters are\n",
    "represented by 32-bit floating-point numbers, a model with 17 billion\n",
    "parameters requires at least 680 GB of memory, far exceeding the maximum\n",
    "memory capacity (80 GB) of NVIDIA A100 (an accelerator with the largest\n",
    "memory available today). Therefore, we need to explore how to\n",
    "efficiently split a model across different accelerators, and how to\n",
    "efficiently utilize collective communication operators for model\n",
    "training and inference. The following describes three optimization\n",
    "technologies regarding collective communication:\n",
    "\n",
    "1.  **Parameter storage on a single node:** The bandwidth of the\n",
    "    accelerators inside a node in a modern cluster is much greater than\n",
    "    the inter-node bandwidth. Therefore, we need to minimize inter-node\n",
    "    communication and ensure that communication mostly happens between\n",
    "    accelerators inside nodes. The model slicing process shows that the\n",
    "    amount of communication between different slices during the forward\n",
    "    and backward computation of the model is far less than the average\n",
    "    amount of communication required for gradient averaging of model\n",
    "    replicas. As such, ZeRO stores all slices of a single model in the\n",
    "    same node, greatly improving the training efficiency.\n",
    "\n",
    "2.  **Forward computation based on the AllGather operator:** Assuming\n",
    "    that the parameters in a model are linear by layer, we can assign\n",
    "    the parameters to different accelerators from front to back based on\n",
    "    the sequence of these parameters on the network. In forward\n",
    "    computation, the computation of a layer depends only on the\n",
    "    parameters of its adjacent layers. Given this, we can apply\n",
    "    AllGather computation once on all accelerators that contain model\n",
    "    parameters in order to extract the parameters of the next layer for\n",
    "    the current layer and to compute the activation value of the current\n",
    "    layer. To conserve memory resources, the parameters of layers other\n",
    "    than the current one need to be discarded immediately after the\n",
    "    AllGather operation is complete.\n",
    "\n",
    "3.  **Gradient averaging based on the ReduceScatter operator:**\n",
    "    Similarly, during backward computation, only the parameters of the\n",
    "    previous layer are needed to compute the activation value and\n",
    "    gradient of the current layer. Therefore, AllGather can be used\n",
    "    again to complete the gradient computation on each accelerator. At\n",
    "    the same time, after gradients are gathered, each accelerator needs\n",
    "    only the gradient corresponding to the layer with the same index as\n",
    "    the accelerator. In this case, the ReduceScatter operator, instead\n",
    "    of AllReduce, can be used to directly store the corresponding\n",
    "    gradient to accelerator $i$.\n",
    "\n",
    "DALL-E is a text-based image generation model proposed by OpenAI. This\n",
    "model has up to 12 billion parameters. In addition to the AllGather +\n",
    "ReduceScatter technique used by ZeRO during training, the OpenAI team\n",
    "made further optimizations. The following describes two optimization\n",
    "technologies regarding collective communication:\n",
    "\n",
    "1.  **Matrix factorization:** The operational speeds of collective\n",
    "    communication operators are positively correlated with the message\n",
    "    length. In model training, the message length indicates the number\n",
    "    of model parameters. DALL-E uses matrix factorization to convert a\n",
    "    high-dimensional tensor into a two-dimensional matrix, and then uses\n",
    "    collective communication operators for transmission after\n",
    "    factorization. In this way, DALL-E significantly reduces the amount\n",
    "    of communication.\n",
    "\n",
    "2.  **Custom data types:** Another way to reduce the amount of\n",
    "    communication is to modify data types. As expected, the 16-bit\n",
    "    half-precision floating-point number representation can reduce the\n",
    "    amount of communication by nearly half compared with the 32-bit\n",
    "    floating-point number representation. However, in practice,\n",
    "    low-precision data types cause unstable model convergence and\n",
    "    compromise the final training result. OpenAI analyzes the structure\n",
    "    of the DALL-E model and classifies the model parameters into three\n",
    "    categories based on their sensitivity to the precision of data\n",
    "    types. The most precision-sensitive parameters are represented by\n",
    "    32-bit floating-point numbers and synchronized only by the AllReduce\n",
    "    operator, whereas the most precision-insensitive parameters are\n",
    "    compressed and transmitted using matrix factorization. For the\n",
    "    remaining parameters, such as the moments and variance parameters\n",
    "    involved in Adam optimization, OpenAI implements two new data types\n",
    "    based on the IEEE 754 standard: 1-6-9 and 0-6-10. (The first digit\n",
    "    indicates the number of bits required for expressing positive and\n",
    "    negative, the second digit indicates the number of bits required for\n",
    "    expressing the exponent, and the third digit indicates the number of\n",
    "    bits required for expressing a valid number.) In addition to\n",
    "    conserving space, this also ensures training convergence.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}