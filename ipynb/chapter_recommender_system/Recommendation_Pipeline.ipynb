{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20084d20",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Recommendation Pipeline\n",
    "\n",
    "A recommendation pipeline, designed to suggest items of potential\n",
    "interest to users based on their requests, is an integral part of any\n",
    "recommender system. Specifically, a user seeking recommendations submits\n",
    "a request that includes their user ID and the current context features,\n",
    "such as recently browsed items and browsing duration, to the inference\n",
    "service. The recommendation pipeline uses these user features and those\n",
    "of potential items as input for computation. It then derives a score for\n",
    "each candidate item, selects the highest-scoring items (ranging from\n",
    "dozens to hundreds) to form the recommendation result, and delivers this\n",
    "result back to the user.\n",
    "\n",
    "Given that a recommender system generally contains billions of potential\n",
    "items, using just a single model to compute the score of each item\n",
    "necessitates a trade-off between model accuracy and speed. In other\n",
    "words, opting for a simpler model may boost speed but potentially result\n",
    "in recommendations that fail to pique the user's interest due to\n",
    "diminished accuracy. On the other hand, using a more complex model may\n",
    "provide more accurate results but deter users due to longer waiting\n",
    "times.\n",
    "\n",
    "![Example of a multi-stage recommendationpipeline](../img/ch_recommender/recommender_pipeline.png)\n",
    ":label:`recommender pipeline`\n",
    "\n",
    "To mitigate this, contemporary recommender systems typically deploy\n",
    "multiple recommendation models as part of a pipeline, as illustrated in\n",
    "Figure :numref:`recommender pipeline`. The pipeline begins with the\n",
    "retrieval stage, employing fast, simple models to filter the entire pool\n",
    "of candidate items, identifying thousands to tens of thousands of items\n",
    "that the user may find appealing. Following this, in the ranking stage,\n",
    "slower, more complex models score and order the retrieved items. The\n",
    "top-scoring items (the exact number may vary depending on the specific\n",
    "service scenario), numbering in the dozens or hundreds, are returned as\n",
    "the final recommendation. If the ranking models are too intricate and\n",
    "cannot process all retrieved items within the given time frame, the\n",
    "ranking stage may be further divided into three sub-stages: pre-ranking,\n",
    "ranking, and re-ranking.\n",
    "\n",
    "## Retrieval Stage\n",
    "\n",
    "The retrieval stage is the initial phase of the recommendation process.\n",
    "The model takes user features as input and performs a rough filter of\n",
    "all candidate items to identify those the user might be interested in.\n",
    "These selected items form the output. The main goal of the retrieval\n",
    "stage is to reduce the pool of candidate items, thereby lightening the\n",
    "computational load on the ranking model in the subsequent stage.\n",
    "\n",
    "### Two-Tower Model\n",
    "\n",
    "To illustrate the retrieval process, let's consider the two-tower model\n",
    "as an example, as shown in Figure\n",
    ":numref:`two tower model`. The two-tower model contains two\n",
    "multilayer perceptrons (MLPs) which encode user features and item\n",
    "features, referred to as the user tower[^1] and item tower,\n",
    "respectively.\n",
    "\n",
    "Continuous features can be input directly into the MLPs, while discrete\n",
    "features must first be mapped into a dense vector using embedding tables\n",
    "before being fed into the MLPs. The user tower and item tower process\n",
    "these features to generate user vectors and item vectors, respectively,\n",
    "each representing a unique user or item. The two-tower model employs a\n",
    "scoring function to evaluate the similarity between user vectors and\n",
    "item vectors.\n",
    "\n",
    "![Structure of the two-towermodel](../img/ch_recommender/two_tower_model.png)\n",
    ":label:`two tower model`\n",
    "\n",
    "### Training\n",
    "\n",
    "During training, the model input consists of the user's feedback data on\n",
    "historical recommendation results, represented by the tuple \\<user,\n",
    "item, label\\>. The label denotes whether the user has clicked the item,\n",
    "with 1 and 0 typically representing a click and non-click, respectively.\n",
    "The two-tower model uses positive samples (i.e., samples where the label\n",
    "is 1) for training. To obtain negative samples, an intra-batch sampler\n",
    "that corrects sampling bias performs sampling within the batch. The\n",
    "details of the algorithm, while not the focus here, can be found in the\n",
    "original paper.\n",
    "\n",
    "The model's output consists of the click probabilities for different\n",
    "items. During training, a suitable loss function is chosen to ensure\n",
    "that the predicted results for positive samples are as close to 1 as\n",
    "possible, and as close to 0 as possible for negative samples.\n",
    "\n",
    "### Inference\n",
    "\n",
    "Before inference, item vectors for all items are computed and saved\n",
    "using the trained model. Given that item features are relatively stable,\n",
    "this step can reduce computational overhead during inference and speed\n",
    "up the process. User features, which are related to user behavior, are\n",
    "processed when user requests arrive. The two-tower model uses the user\n",
    "tower to compute current user features and generate the user vector. The\n",
    "same scoring function used during training is then used to measure\n",
    "similarity. This enables similarity search based on the user vector\n",
    "across all candidate item vectors. The most similar items are output as\n",
    "the retrieval result.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "A common evaluation metric of the retrieval model is the recall metric\n",
    "when $k$ items are recalled (Recall@k). This metric essentially\n",
    "quantifies the ability of a model to successfully retrieve the top $k$\n",
    "items of interest.\n",
    "\n",
    "The mathematical definition of Recall@k is expressed as follows:\n",
    "\n",
    "$$\\text{Recall@k} = \\frac{\\text{TP}}{\\min(\\text{TP} + \\text{FN}, k)}$$\n",
    "\n",
    "In this equation, the term \\\"True Positive\\\" (TP) refers to the count of\n",
    "items correctly identified by the model as relevant (i.e., with a true\n",
    "label of 1) among the $k$ items retrieved. On the other hand, \\\"False\n",
    "Negative\\\" (FN) denotes the count of relevant items (again, with a true\n",
    "label of 1) that the model failed to include among the $k$ retrieved\n",
    "items.\n",
    "\n",
    "Thus, the Recall@k metric serves as a measure of the model's ability to\n",
    "correctly identify and retrieve positive samples. Importantly, it is\n",
    "crucial to understand that if the total number of positive samples\n",
    "surpasses $k$, the maximum possible count of correctly retrieved items\n",
    "is $k$. This is due to the fact that the model is limited to retrieving\n",
    "only $k$ items. Consequently, the denominator in the Recall@k equation\n",
    "is defined as the lesser of two quantities: the sum of true positives\n",
    "and false negatives, or $k$.\n",
    "\n",
    "## Ranking Stage\n",
    "\n",
    "During the ranking phase, the model appraises the items gathered in the\n",
    "retrieval stage, evaluating each individually in terms of user features\n",
    "and item features. Each item's score is indicative of the probability\n",
    "that the user might be interested in that item. As a result, the\n",
    "highest-scoring items based on these rankings are then suggested to the\n",
    "user.\n",
    "\n",
    "If the number of candidate items evaluated by the recommendation model\n",
    "continually increases, or if the recommendation logic and rules become\n",
    "more complex, the entire ranking stage can be efficiently divided into\n",
    "three sub-stages: pre-ranking, ranking, and re-ranking.\n",
    "\n",
    "### Pre-ranking\n",
    "\n",
    "Acting as an intermediary between the retrieval and ranking stages, the\n",
    "pre-ranking stage serves as an additional layer of filtering. This\n",
    "becomes particularly useful when there's a large influx of candidate\n",
    "items from the retrieval stage, or when multi-channel retrieval methods\n",
    "are used to boost retrieval result diversity. If every retrieved item\n",
    "was directly fed into the ranking model, the subsequent process could\n",
    "become overly lengthy due to the sheer volume of items. Thus,\n",
    "introducing a pre-ranking stage to the recommendation pipeline reduces\n",
    "the number of items proceeding to the ranking stage, enhancing overall\n",
    "system efficiency.\n",
    "\n",
    "### Ranking\n",
    "\n",
    "Ranking, the second stage, is pivotal in the pipeline. In this phase,\n",
    "it's essential that the model precisely represents the user's\n",
    "preferences across varying items. When referring to the \\\"ranking\n",
    "model\\\" in subsequent sections, we are specifically addressing the model\n",
    "used during this ranking sub-stage.\n",
    "\n",
    "### Re-ranking\n",
    "\n",
    "In the final re-ranking stage, the preliminary outcomes derived from the\n",
    "ranking stage are further refined according to specific business logic\n",
    "and rules. The goal of this stage is to improve the holistic quality of\n",
    "the recommendation service, shifting the focus from the click-through\n",
    "rate (CTR) of a single item to the broader user experience. For\n",
    "instance, the applied business logic might include efforts to increase\n",
    "the visibility of new items, filter out previously purchased items or\n",
    "watched videos, and create rules to diversify the order and variety of\n",
    "recommended items, thereby decreasing the frequency of similar item\n",
    "recommendations.\n",
    "\n",
    "## Ranking with Deep Learning\n",
    "\n",
    "The ranking stage in a recommender system has largely benefited from the\n",
    "use of deep learning models. These models are often referred to as the\n",
    "Deep Learning Recommendation Model (DLRM). As depicted in Figure\n",
    ":numref:`dlrm model`, a\n",
    "DLRM consists of embedding tables, multi-layer perceptrons (MLPs) that\n",
    "include two layers, and an interaction layer.[^2]\n",
    "\n",
    "![Structure of DLRM](../img/ch_recommender/dlrm_model.png)\n",
    ":label:`dlrm model`\n",
    "\n",
    "Similar to the two-tower model, the DLRM initially uses embedding tables\n",
    "to transform discrete features into corresponding embedding items, which\n",
    "are represented as dense vectors. The model then combines all continuous\n",
    "features into a single vector, which is introduced into the bottom MLP,\n",
    "generating an output vector with the same dimension as the embedding\n",
    "items. Both this output vector and all the embedding items are then\n",
    "forwarded to the interaction layer for further processing.\n",
    "\n",
    "As illustrated in Figure :numref:`interaction`, the interaction layer performs dot product\n",
    "operations on all features (encompassing all embedding items and the\n",
    "processed continuous features) to obtain second-order interactions. As\n",
    "the features interacted within the interaction layer are symmetric, the\n",
    "diagonal represents each feature's self-interaction result. In the\n",
    "non-diagonal section, every distinct pair of features interacts twice\n",
    "(e.g., for features $p$ and $q$, two results are acquired: $<p,q>$ and\n",
    "$<q,p>$). Therefore, only the lower triangular part of the result matrix\n",
    "is retained and flattened. This flattened interaction result is merged\n",
    "with the output from the bottom MLP, and the combined result is used as\n",
    "the input for the top MLP. After further processing by the top MLP, the\n",
    "final output score reflects the probability of a user clicking on the\n",
    "item.\n",
    "\n",
    "![Interaction principlediagram](../img/ch_recommender/interaction.png)\n",
    ":label:`interaction`\n",
    "\n",
    "### Training Process\n",
    "\n",
    "The DLRM bases its training on \\<user, item, label\\> tuples. It takes in\n",
    "user and item features as inputs and interacts with these features to\n",
    "predict the likelihood of a user clicking an item. For positive samples,\n",
    "the model aims to approximate this probability as closely to 1 as\n",
    "possible, while for negative samples, the goal is to get this\n",
    "probability as near to 0 as possible.\n",
    "\n",
    "The ranking process can be considered a binary classification problem;\n",
    "the (user, item) pair can be classified either as click (label: 1) or no\n",
    "click (label: 0). Therefore, the method used to evaluate a ranking model\n",
    "is analogous to that employed for assessing a binary classification\n",
    "model. However, it's crucial to consider that recommender system\n",
    "datasets tend to be extremely imbalanced, meaning the proportion of\n",
    "positive samples is drastically different from that of negative samples.\n",
    "To minimize the influence of this data imbalance on metrics, we use the\n",
    "Area Under the Curve (AUC) and F1 score to evaluate ranking models.\n",
    "\n",
    "The AUC is the area under the Receiver Operating Characteristic (ROC)\n",
    "curve, a graph used to define classification thresholds, plotted with\n",
    "the True Positive Rate (TPR) against the False Positive Rate (FPR) ---\n",
    "with the TPR on the y-axis and the FPR on the x-axis. An appropriate\n",
    "classification threshold can be determined by calculating the AUC and\n",
    "the ROC curves. If the predicted probability exceeds the classification\n",
    "threshold, the prediction result is 1 (click); otherwise, it is 0 (no\n",
    "click). From the prediction result, recall and precision can be\n",
    "computed, which in turn allows for the calculation of the F1 score using\n",
    "the formula :eqref:`f1`.\n",
    "\n",
    "$$F1 = 2 \\times \\frac{recall \\times precision}{recall + precision}$$ \n",
    ":eqlabel:`equ:f1`\n",
    "\n",
    "### Inference Process\n",
    "\n",
    "During the inference stage, the features of the retrieved items, along\n",
    "with their corresponding user features, are merged and inputted into the\n",
    "DLRM. The model then predicts scores, and the items with the highest\n",
    "probabilities are selected for output.\n",
    "\n",
    "[^1]: In the original paper, the user tower also uses the features of\n",
    "    videos watched by users as seed features.\n",
    "\n",
    "[^2]: DLRM is designed for structural customization. This section will\n",
    "    illustrate an example using the standard code implementation of\n",
    "    DLRM.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}