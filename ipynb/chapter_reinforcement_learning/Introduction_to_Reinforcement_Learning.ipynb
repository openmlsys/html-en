{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadf14cd",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Introduction to Reinforcement Learning\n",
    "\n",
    "## Background\n",
    "\n",
    "As a branch of machine learning, reinforcement learning has attracted\n",
    "more and more attention in recent years. DeepMind proposed deep\n",
    "Q-learning in 2013, enabling AI to learn how to play video games based\n",
    "on images. Since then, DeepMind-led scientific institutions have made\n",
    "remarkable achievements in reinforcement learning --- a representative\n",
    "example is AlphaGo, which defeated the world's top Go player Lee Sedol\n",
    "in 2016. Other significant achievements include AlphaStar (agent of\n",
    "StarCraft), OpenAI Five (agent of Dota 2), Pluribus (Texas hold'em\n",
    "poker, which is a multi-player zero-sum game), and robot dog motion\n",
    "control algorithms. These achievements have been made possible by the\n",
    "rapid iterations and progress of algorithms in the reinforcement\n",
    "learning field over the past few years. The data-hungry deep neural\n",
    "networks can demonstrate a good fitting effect based on the large\n",
    "amounts of data generated by simulators, thereby fully leveraging the\n",
    "capabilities of reinforcement learning algorithms and performing\n",
    "comparably or even better than human experts in terms of learning.\n",
    "Although originally utilized in the video gaming field, reinforcement\n",
    "learning has since been gradually applied in a wider range of realistic\n",
    "and meaningful fields, including robot control, dexterous manipulation,\n",
    "energy system scheduling, network load distribution, and automatic\n",
    "trading for stocks or futures. Such applications have impacted\n",
    "traditional control methods and heuristic decision-making theory.\n",
    "\n",
    "## Reinforcement Learning Components\n",
    "\n",
    "The core of reinforcement learning is the process of continuously\n",
    "interacting with the environment to optimize the policy with the\n",
    "intention of improving the reward. Such a process is manifested as the\n",
    "selection of an *action* based on a specific *state*. The object that\n",
    "makes the decision is called an *agent*, and the impact of the decision\n",
    "is reflected in the *environment*. More specifically, the *state\n",
    "transition* and *reward* in the environment vary depending on the\n",
    "decision. State transition, which can be either deterministic or\n",
    "stochastic, is a function that specifies the environment's transition\n",
    "from the current state to the next state. A reward, which is generally a\n",
    "scalar, is the feedback of the environment on the agent's action.\n",
    "FigureÂ :numref:`ch011/ch11-rl` shows the abstract process, which is the\n",
    "most common model description of reinforcement learning in the\n",
    "literature.\n",
    "\n",
    "![Framework of reinforcementlearning](../img/ch11/ch11-rl.pdf)\n",
    ":label:`ch011/ch11-rl`\n",
    "\n",
    "Take video gaming as an example. A gamer needs to gradually become\n",
    "familiar with the game operations in order to achieve better results.\n",
    "The process from getting started with the game to gradually mastering\n",
    "game skills is similar to the reinforcement learning process. At any\n",
    "given moment after the game starts, it is in a specific state. By\n",
    "viewing the state, the gamer can obtain an *observation* (e.g., images\n",
    "on the screen of the game console), based on which the gamer performs an\n",
    "action (e.g., firing bullets) that changes the game state and enables\n",
    "the game to enter the next state (e.g., defeating monsters).\n",
    "Furthermore, the gamer can know the effect of the current action (e.g.,\n",
    "defeating a monster generates a positive score, whereas losing to a\n",
    "monster generates a negative score). The gamer then selects a new action\n",
    "based on the observation of the next state, and repeats this process\n",
    "until the game ends. Through these repetitive operations and\n",
    "observations, the gamer can gradually master the skills of the game. A\n",
    "reinforcement learning agent learns to play the game in a similar way.\n",
    "\n",
    "However, there are several key issues to be noticed in this process. (1)\n",
    "The observation may not be equal to the state. Instead, it is generally\n",
    "a function of the state, and the mapping from the state to the\n",
    "observation may cause information loss. The environment is *fully\n",
    "observable* if the observation is equal to the state or if the state of\n",
    "the environment can be completely restored based on the observation; in\n",
    "all other cases, it is *partially observable*. (2) Each action performed\n",
    "by a gamer may not produce immediate feedback but may produce delayed\n",
    "effects after many steps. Reinforcement learning models allow such a\n",
    "delayed feedback. (3) The feedback may not be a scalar in the human\n",
    "learning process. To convert the feedback received by the reinforcement\n",
    "learning agent into a scalar, called the reward value, we can perform\n",
    "mathematical abstraction on it. The reward value can be a function of\n",
    "the state, or a function of the state and action. The existence of the\n",
    "reward value is a basic assumption for reinforcement learning, and is\n",
    "also a major difference between reinforcement learning and supervised\n",
    "learning.\n",
    "\n",
    "## Markov Decision Process\n",
    "\n",
    "In reinforcement learning, the decision-making process is generally\n",
    "described by a Markov decision process[^1], and can be represented by a\n",
    "tuple $(\\mathcal{S}, \\mathcal{A}, R, \\mathcal{T}, \\gamma)$.\n",
    "$\\mathcal{S}$ and $\\mathcal{A}$ indicate the state space and action\n",
    "space, respectively. $R$ indicates the reward function. $R(s,a)$:\n",
    "$\\mathcal{S}\\times \\mathcal{A}\\rightarrow \\mathbb{R}$ indicates the\n",
    "reward value regarding the current state $s\\in\\mathcal{S}$ and the\n",
    "current action $a\\in\\mathcal{A}$. The probability of transitioning from\n",
    "the current state and action to the next state is defined as\n",
    "$\\mathcal{T}(s^\\prime|s,a)$:\n",
    "$\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow \\mathbb{R}_+$.\n",
    "$\\gamma\\in(0,1)$ indicates the discount factor[^2] for the reward.\n",
    "Reinforcement learning aims to maximize the expected cumulative reward\n",
    "value ($\\mathbb{E}[\\sum_t \\gamma^t r_t]$) received by the agent.\n",
    "\n",
    "The Markov property in a Markov decision process is defined as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathcal{T}(s_{t+1}|s_t) = \\mathcal{T}(s_{t+1}|s_0, s_1, s_2, \\dots, s_t)\n",
    "\\end{aligned}$$ \n",
    "\n",
    "That is, the transition to the current state depends on\n",
    "the previous state only (it does not depend on historical states). We\n",
    "can omit action $a$ in the state transition function $\\mathcal{T}$\n",
    "because the Markov property is part of the environment transition\n",
    "process and is independent of the decision process.\n",
    "\n",
    "Based on the Markov property, we can further deduce that the optimal\n",
    "policy at any given moment depends only on the decision on the latest\n",
    "state --- it does not depend on the entire decision history. This\n",
    "conclusion is of great significance in the design of reinforcement\n",
    "learning algorithms because it simplifies the process of solving the\n",
    "optimal policy.\n",
    "\n",
    "[^1]: A Markov decision process is a function in which a subsequent\n",
    "    state depends only on the current state and action (it does not\n",
    "    depend on historical states).\n",
    "\n",
    "[^2]: Each subsequent reward value can be multiplied by the discount\n",
    "    factor so that an infinite sequence has a limited sum of reward\n",
    "    values.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}