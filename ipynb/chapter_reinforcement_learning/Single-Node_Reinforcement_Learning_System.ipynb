{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0389263",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Single-Node Reinforcement Learning System\n",
    "\n",
    "This section focuses on the single-node reinforcement learning system,\n",
    "which is a type of simple system for reinforcement learning with a\n",
    "single agent. Here, \"node\" refers to a computing unit used for model\n",
    "update. Reinforcement learning systems are classified into single-node\n",
    "ones and distributed ones based on whether parallel processing is\n",
    "performed during model update. In a single-node reinforcement learning\n",
    "system, only one class object is instantiated and used as the agent. The\n",
    "sampling process (based on interaction with the environment) and the\n",
    "update process (based on the collected samples) are considered as\n",
    "different private functions within this class. Unlike such a single-node\n",
    "system, a distributed one is more complex.\n",
    "\n",
    "## RL System\n",
    "\n",
    "There are many forms of distributed reinforcement learning systems, and\n",
    "the forms of the systems depend on the algorithms to implement. In the\n",
    "most basic distributed reinforcement learning framework, it is assumed\n",
    "that the reinforcement learning algorithm is implemented on only one\n",
    "computing unit, and that the sampling and update processes are\n",
    "implemented as two or more parallel processes in order to balance\n",
    "computing resources. To coordinate these processes, inter-process\n",
    "communication is required. In a more complex framework, the algorithm\n",
    "runs on multiple computing devices (e.g., in a multi-node computing\n",
    "cluster), and the functions of the agent may need to be implemented\n",
    "through cross-node and cross-process communication. A more complex\n",
    "computing system design is necessary for multiagent systems in which\n",
    "models of multiple agents need to be updated at the same time. We will\n",
    "describe the implementation mechanisms of these systems later.\n",
    "\n",
    "## RLzoo\n",
    "\n",
    "Taking RLzoo as an example, the following describes the basic modules\n",
    "required for establishing a single-node reinforcement learning system.\n",
    "Figure :numref:`ch011/ch11-rlzoo` shows a typical example of such a\n",
    "system used in the RLzoo algorithm library[^1]. The basic components of\n",
    "the system include the neural networks, adapters, policy networks, value\n",
    "networks, environment instances, model learner, and experience replay\n",
    "buffer.\n",
    "\n",
    "Let's first introduce three components: neural networks, adapters, and\n",
    "policy and value networks. Same as in deep learning, neural networks are\n",
    "used for function approximation based on data. Figure\n",
    ":numref:`ch011/ch11-rlzoo` involves three common types of neural\n",
    "networks: fully-connected, convolutional, and recurrent neural networks.\n",
    "A policy network is a parameterized policy representation by a deep\n",
    "neural network, whereas a value network is a neural network representing\n",
    "the state-value function or state-action value function. Policy networks\n",
    "and value networks are common components in deep reinforcement learning.\n",
    "The fully-connected, convolutional, and recurrent neural networks are\n",
    "general neural networks, which are usually important components for\n",
    "constituting the special networks in reinforcement learning --- policy\n",
    "networks and value networks. In RLzoo, an adapter is a functional module\n",
    "for transforming a general neural network into a special neural network.\n",
    "There are three types of adapters: observation-based adapter, policy\n",
    "adapter, and action adapter. They perform different tasks in order to\n",
    "implement a special network for a reinforcement learning agent. First,\n",
    "an observation-based adapter is used to select the network structure of\n",
    "the head for the neural network used by the reinforcement learning agent\n",
    "based on the observation type. Then, a policy adapter decides whether to\n",
    "use the deterministic policy or stochastic policy as the output policy\n",
    "for the tail of the policy network based on the type of the\n",
    "reinforcement learning algorithm. Finally, an action adapter is used to\n",
    "select the type of the output action distribution, which can be\n",
    "discrete, continuous, or categorical. The three types of adapters are\n",
    "collectively referred to as adapters in Figure\n",
    ":numref:`ch011/ch11-rlzoo`.\n",
    "\n",
    "![Reinforcement learning system used in the RLzoo algorithmlibrary](../img/ch11/ch11-rlzoo.pdf)\n",
    ":label:`ch011/ch11-rlzoo`\n",
    "\n",
    "The policy networks and value networks constitute the core learning\n",
    "modules of the reinforcement learning agent. A learner is required to\n",
    "update such modules based on rules (i.e., loss function provided by the\n",
    "reinforcement learning algorithm). One of the most important roles in\n",
    "the process of updating learning modules is the input learning data ---\n",
    "the samples collected during the interaction between the agent and\n",
    "environment. For *off-policy* reinforcement learning, the samples are\n",
    "stored in the experience replay buffer, from which the learner obtains\n",
    "certain samples to update the model. Reinforcement learning algorithms\n",
    "are classified as either *on-policy* ones if the updated model is the\n",
    "same as the model used for sampling or off-policy ones if the two models\n",
    "are different. Off-policy reinforcement learning allows samples that are\n",
    "collected through interaction with the environment to be extracted for\n",
    "model update after they are stored in a large buffer for a long time. In\n",
    "on-policy reinforcement learning, such a buffer may also exist, but it\n",
    "stores only very recently collected data. This means that we can\n",
    "consider the updated model and the model used for sampling to be\n",
    "approximately the same. The experience replay buffer shown in Figure\n",
    ":numref:`ch011/ch11-rlzoo`, along with the policy networks, value\n",
    "networks, adapters, and learner, form a single-node reinforcement\n",
    "learning agent in RLzoo. This agent interacts with environment instances\n",
    "and collects data to update the model. Environment instantiation allows\n",
    "parallel sampling in multiple environments.\n",
    "\n",
    "## Other Systems\n",
    "\n",
    "Recent research suggests that the development bottleneck in the\n",
    "reinforcement learning algorithm field may not lie in only the\n",
    "algorithm, but also in the simulation speed of the simulator from which\n",
    "the agent can collect data. Issac Gym --- a GPU-based simulation engine\n",
    "launched by NVIDIA in 2021 --- can run 2--3 times faster on a single GPU\n",
    "than in CPU-based simulators. The acceleration of GPU-based execution is\n",
    "described in Chapter 5. GPU-based simulation can significantly\n",
    "accelerate reinforcement learning tasks because GPUs are capable of\n",
    "multi-core parallel computing and eliminate the need for data\n",
    "transmission and communication between CPUs and GPUs. Traditional\n",
    "reinforcement learning environments such as OpenAI Gym (a common\n",
    "reinforcement learning benchmark test environment) perform simulated\n",
    "computing based on CPUs. In contrast, neural network training using deep\n",
    "learning is performed on GPUs or TPUs.\n",
    "\n",
    "Data samples collected when the agent interacts with the instantiated\n",
    "simulation environment on a CPU are temporarily stored in a format of\n",
    "CPU data. Such data is then transferred to the GPU and converted into a\n",
    "format of GPU data for model training when necessary. Take PyTorch as an\n",
    "example: a tensor of the torch.Tensor type can be transferred to the GPU\n",
    "by setting *device* in the tensor.to(device) function to 'cuda'. In\n",
    "addition, because model parameters are stored in the GPU's data type,\n",
    "input data also needs to be transferred from the CPU to GPU when the\n",
    "model is invoked to perform forward propagation. Furthermore, GPU data\n",
    "output by the model may need to be converted back into the CPU's data\n",
    "type. Such redundant conversions significantly increase the time\n",
    "required for model learning and the workload in using the algorithm. The\n",
    "design of the Isaac Gym simulator solves this problem of transformation\n",
    "between computational hardware from underlying frameworks of the\n",
    "simulators. Because both the simulator and models are implemented on the\n",
    "GPU, they can communicate with each other without involving the CPU,\n",
    "thereby eliminating the need for bidirectional data transmission between\n",
    "the CPU and GPU. In this way, the Isaac Gym simulator accelerates the\n",
    "simulation process in reinforcement learning tasks.\n",
    "\n",
    "[^1]: RLzoo code address: <https://github.com/tensorlayer/RLzoo>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}