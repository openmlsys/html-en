{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461a2e12",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Operator Compiler {#sec:operator-compiler}\n",
    "\n",
    "Operator compilers are used for compiling and optimizing operators,\n",
    "which may be part of a neural network or come from the code implemented\n",
    "in a domain-specific language (DSL). The compilation is the process of\n",
    "*transforming* the source code from one *representation* into another.\n",
    "\n",
    "The objective of an operator compiler is to improve the *execution\n",
    "performance* of operators. An operator compiler accepts tensor\n",
    "computation logic described in *dynamic languages* (e.g., Python) as the\n",
    "input and outputs executable files on *specific AI processors*.\n",
    "\n",
    "## Scheduling Strategy\n",
    "\n",
    "An operator compiler abstracts the execution of statements in an\n",
    "operator implementation into \\\"scheduling strategies\\\". Since an\n",
    "operator typically consists of multiple statements, the focus lies in\n",
    "determining the scheduling strategy for the statements within the\n",
    "operator. This strategy encompasses considerations such as the\n",
    "calculation order, data block movement, and other relevant factors.\n",
    "\n",
    "If ignoring the specific processor architecture, for the best\n",
    "performance, we only need to load all input tensors to the computation\n",
    "core based on the *computational logic* of the operator and access the\n",
    "result from the core for storage. *Computational logic* refers to basic\n",
    "arithmetic operations (e.g., addition, subtraction, multiplication, and\n",
    "division) and other function expressions (e.g., convolution,\n",
    "transposition, and loss functions).\n",
    "\n",
    "Modern computer memory hierarchy looks like a pyramid structure, as\n",
    "shown in Figure\n",
    ":numref:`ch05/ch05-memory_architecture`. As we move up the\n",
    "pyramid, the storage elements have a higher cost but a faster access\n",
    "time.\n",
    "\n",
    "![Modern computer memoryhierarchy](../img/ch05/memory_architecture.png)\n",
    ":label:`ch05/ch05-memory_architecture`\n",
    "\n",
    "Such hardware design leads to two basic types of locality:\n",
    "\n",
    "\\(1\\) Temporal locality: the tendency to access the same memory location\n",
    "several times in quick succession. As such, accessing the same location\n",
    "in the L1 cache several times is more efficient than accessing different\n",
    "locations in the L1 cache several times.\n",
    "\n",
    "\\(2\\) Spatial locality: the tendency to access nearby memory locations\n",
    "in quick succession. As such, accessing nearby locations in the L1 cache\n",
    "several times is more efficient than moving back and forth between the\n",
    "L1 cache and the main memory.\n",
    "\n",
    "Both types of locality help improve system performance. Specifically, in\n",
    "order to improve the data access speed, data to be repeatedly processed\n",
    "can be placed in fixed nearby memory locations when possible.\n",
    "\n",
    "For a serial computational task, it is also possible to decouple the\n",
    "data part from the logic part and generate a range of independent groups\n",
    "of data that can be executed in parallel, as shown in Figure\n",
    ":numref:`ch05/ch05-parallel_computing`.\n",
    "\n",
    "![Serial computing and parallelcomputing](../img/ch05/parallel_computing.png)\n",
    ":label:`ch05/ch05-parallel_computing`\n",
    "\n",
    "These specific data-oriented operations performed at program runtime are\n",
    "referred to as *schedules*. A schedule defines the following aspects:\n",
    "\n",
    "\\(1\\) When and where should each value in a function be calculated?\n",
    "\n",
    "\\(2\\) Where is data stored?\n",
    "\n",
    "\\(3\\) How long does it take to access each value between those\n",
    "calculated using preorder structure consumers? And when is independent\n",
    "recomputation performed by each such value?\n",
    "\n",
    "Simply put, a scheduling strategy is defined by a set of algorithms\n",
    "designed during compilation based on the characteristics of target\n",
    "hardware architecture to improve locality and parallelism. The purpose\n",
    "of this is to ensure that the resulting executable file delivers optimal\n",
    "performance at runtime. These algorithms have no effect on the\n",
    "computation result; instead, they only adjust the computation process in\n",
    "order to shorten the computation time.\n",
    "\n",
    "## Combining Scheduling Strategies\n",
    "\n",
    "In the realm of operator compilers, a common optimization technique\n",
    "involves combining multiple abstracted scheduling strategies into a\n",
    "comprehensive and efficient scheduling set through manual template\n",
    "matching. However, this approach may not be fine-tuned and can be\n",
    "labor-intensive when applied to achieve refined optimization across\n",
    "different operators. To illustrate this, let's consider an optimization\n",
    "algorithm implemented in the Tensor Virtual Machine (TVM). It\n",
    "accelerates and optimizes a multiply-accumulate code segment on the CPU\n",
    "by combining several fundamental scheduling strategies.\n",
    "\n",
    "In Code `lst:before_tvm`, the basic computational logic is as\n",
    "follows: Initialize tensor C, multiply tensor A by tensor B, and\n",
    "accumulate the results to tensor C.\n",
    "\n",
    "**lst:before_tvm**\n",
    "\n",
    "```\n",
    "for (m: int32, 0, 1024) {\n",
    "  for (n: int32, 0, 1024) {\n",
    "    C[((m*1024) + n)] = 0f32\n",
    "      for (k: int32, 0, 1024) {\n",
    "        let cse_var_2: int32 = (m*1024)\n",
    "          let cse_var_1: int32 = (cse_var_2 + n)\n",
    "            C[cse_var_1] = (C[cse_var_1] + (A[(cse_var_2 + k)]*B[((k*1024) + n)]))\n",
    "      }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Assuming that the data type is float and that tensors A, B, and C are of\n",
    "size 1024 $\\times$ 1024, then the total memory required by the tensors\n",
    "is 1024 $\\times$ 1024 $\\times$ 3 $\\times$ sizeof(float) = 12 MB. This\n",
    "far exceeds the capacity of common caches (e.g., the L1 cache is 32 KB).\n",
    "Therefore, if we want to compute on Tensor A, B, and C in a single\n",
    "operation, we must store them in the main memory. However, the main\n",
    "memory is distant from the compute core, resulting in significantly\n",
    "lower access efficiency compared to using the cache for storage.\n",
    "\n",
    "There are several scheduling strategies that can help improve\n",
    "performance: tile, reorder, and split. The size of the L1 cache is 32\n",
    "KB. To ensure that data used in every computation step is stored in the\n",
    "cache, tiling based on the factors of 32 is performed. In this way, only\n",
    "the tiny block formed by `m.inner `$\\times$` n.inner` needs to be taken\n",
    "into account, and memory access of the innermost tiny block is\n",
    "independent of the outer loops. A tiny block will occupy only 32\n",
    "$\\times$ 32 $\\times$ 3 $\\times$ sizeof(float), which is 12 KB in the\n",
    "cache. The optimized code is shown in Code\n",
    "`lst:after_tvm`. We perform tiling on loops m and n based on\n",
    "factor 32 as the previous analysis. Similarly, we tile the loop k based\n",
    "on factor 4, then reorder the k.outer and k.inner axis as the outermost\n",
    "axis.\n",
    "\n",
    "**lst:after_tvm**\n",
    "\n",
    "```\n",
    "// Obtain an outer loop by tiling for (m: int32, 0, 1024) based on factor 32.\n",
    "for (m.outer: int32, 0, 32) {\n",
    "  // Obtain an outer loop by tiling for (n: int32, 0, 1024) based on factor 32.\n",
    "  for (n.outer: \n",
    "    // Obtain an inner loop by tiling for (m: int32, 0, 1024) based on factor 32.\n",
    "    for (m.inner.init: int32, 0, 32) {\n",
    "      // Obtain an inner loop by tiling for (n: int32, 0, 1024) based on factor 32.\n",
    "      for (n.inner.init: int32, 0, 32) {\n",
    "        // Obtain the corresponding factors.\n",
    "        C[((((m.outer*32768) + (m.inner.init*1024)) + (n.outer*32)) + n.inner.init)] = 0f32\n",
    "      }\n",
    "    }\n",
    "    // Obtain an outer loop by splitting for (k: int32, 0, 1024) based on factor 4, with reorder.\n",
    "    for (k.outer: int32, 0, 256) {\n",
    "      // Obtain an outer loop by splitting for (k: int32, 0, 1024) based on factor 4, with reorder.\n",
    "      for (k.inner: int32, 0, 4) {\n",
    "        // Obtain an inner loop by tiling for (m: int32, 0, 1024) based on factor 32.\n",
    "        for (m.inner: int32, 0, 32) {\n",
    "          // Obtain an inner loop by tiling for (n: int32, 0, 1024) based on factor 32.\n",
    "          for (n.inner: int32, 0, 32) {\n",
    "            // Outer axis factor obtained by tiling along axis n\n",
    "            let cse_var_3: int32 = (n.outer*32)\n",
    "            // Outer axis & inner axis factors obtained by tiling along axis m\n",
    "            let cse_var_2: int32 = ((m.outer*32768) + (m.inner*1024))\n",
    "            // Outer axis & inner axis factors obtained by tiling along axes m & n\n",
    "            let cse_var_1: int32 = ((cse_var_2 + cse_var_3) + n.inner)\n",
    "            // Split the computational logic into different layers so that data involved every loop can be stored in the cache.\n",
    "            C[cse_var_1] = (C[cse_var_1] + (A[((cse_var_2 + (k.outer*4)) + n.inner)] * B[((((k.outer*4096) + (k.inner*1024)) + cse_var_3) + n.inner)]))\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## Finding Optimized Strategies with Polyhedral Models\n",
    "\n",
    "Another optimization approach is to automatically select an operator\n",
    "schedule from a schedule search space. A good example of this idea is\n",
    "the polyhedral compilation. They improve the generalization of operator\n",
    "compilation at the expense of prolonged compile time.\n",
    "\n",
    "Polyhedral compilation mainly optimizes the loops in user code by\n",
    "abstracting each loop into a multidimensional space, computing instances\n",
    "into points in the space, and dependencies between the instances into\n",
    "lines in the space. The main idea of this algorithm is to model the\n",
    "memory access characteristics in code and adjust the execution order of\n",
    "each instance within each loop. In this way, it aims to enable better\n",
    "locality and parallelism of the loop code under the new schedule.\n",
    "\n",
    "Code `lst:before_poly` is used as an example to describe the\n",
    "algorithm.\n",
    "\n",
    "**lst:before_poly**\n",
    "\n",
    "```\n",
    "for (int i = 0; i < N; i++)\n",
    "  for (int j = 1; j < N; j++)\n",
    "    a[i+1][j] = a[i][j+1] - a[i][j] + a[i][j-1];\n",
    "```\n",
    "\n",
    "As shown in Figure :numref:`ch05/ch05-poly_test`, a memory access structure is first\n",
    "modeled by using the polyhedral model algorithm, and then dependencies\n",
    "(denoted by arrows) between instances (denoted by nodes) are analyzed.\n",
    "\n",
    "![Polyhedral model of the samplecode](../img/ch05/poly_test.png)\n",
    ":label:`ch05/ch05-poly_test`\n",
    "\n",
    "Complex dependency analysis and schedule transformation are then\n",
    "performed to obtain an optimal solution that fits the memory model.\n",
    "Using the polyhedral model algorithm, the code is optimized to that\n",
    "shown in Code `lst:after_poly`.\n",
    "\n",
    "**lst:after_poly**\n",
    "\n",
    "```\n",
    "for (int i_new = 0; i_new < N; i_new++)\n",
    "  for (int j_new = i+1; j_new < i+N; j_new++)\n",
    "    a[i_new+1][j_new-i_new] = a[i_new][j_new-i_new+1] - a[i_new][j_new-i_new] + a[i_new][j_new-i_new-1];\n",
    "```\n",
    "\n",
    "The resulting code looks relatively complex. We can model the code (as\n",
    "shown in Figure :numref:`ch05/ch05-poly`) to determine its performance\n",
    "improvements. Through dependency analysis, we find that the loop\n",
    "dependencies present in the source code are removed in the optimized\n",
    "code, thereby increasing the opportunities for parallel computing.\n",
    "Specifically, parallel computing is possible when the loop dependencies\n",
    "are partitioned along the dashed lines based on the green blocks, as\n",
    "shown in Figure :numref:`ch05/ch05-poly`.\n",
    "\n",
    "![Optimization result with the polyhedralmodel](../img/ch05/poly.png)\n",
    ":label:`ch05/ch05-poly`\n",
    "\n",
    "We have only introduced the Polyhedral Compilation technique in this\n",
    "section. However, there are other optimization techniques available,\n",
    "such as Ansor, which is a heuristic searching method with pruning.\n",
    "\n",
    "## Adaptation to Instruction Sets\n",
    "\n",
    "We have previously explored the optimization techniques of operator\n",
    "compilers. In this section, we build on this foundation to examine how\n",
    "operator compilers adapt to instruction sets on different chips.\n",
    "Typically, a general-purpose compiler is designed to be compatible with\n",
    "as many backend architectures and instruction sets as possible. However,\n",
    "this can present challenges when the compiler must handle backends with\n",
    "different architectures and instruction sets.\n",
    "\n",
    "Two common programming models adopted by AI processors are single\n",
    "instruction, multiple data (SIMD) and single instruction, multiple\n",
    "threads (SIMT). As shown in Figures\n",
    ":numref:`ch05/ch05-SIMD` and\n",
    ":numref:`ch05/ch05-SIMT`, respectively, SIMD corresponds to chips\n",
    "with vector instructions, while SIMT corresponds to chips that support\n",
    "multiple threads. Recently, some chips have begun to combine both\n",
    "programming models in order to support both multithreaded parallel\n",
    "computing and vector instructions. When handling different programming\n",
    "models, an operator compiler adopts different optimization strategies,\n",
    "such as vectorization.\n",
    "\n",
    "![SIMD diagram](../img/ch05/SIMD.png)\n",
    ":label:`ch05/ch05-SIMD`\n",
    "\n",
    "![SIMT diagram](../img/ch05/SIMT.png)\n",
    ":label:`ch05/ch05-SIMT`\n",
    "\n",
    "Operator compilers place a strong emphasis on differentiated support in\n",
    "the frontend, midend, and backend. In the frontend, support for multiple\n",
    "backend instruction sets is added, allowing AI programmers to focus on\n",
    "algorithm logic without having to worry about chip differences. In the\n",
    "midend, the architectures of different chips are identified, which\n",
    "allows for specific optimization methods to be implemented for each\n",
    "chip. When generating backend code, the instruction sets of different\n",
    "chips are further identified to ensure efficient execution on target\n",
    "chips.\n",
    "\n",
    "## Expression Ability\n",
    "\n",
    "The representation capability of an operator compiler is important\n",
    "because it determines how well the frontend can express the input code\n",
    "in an IR without loss of syntax information. The frontend of an operator\n",
    "compiler is often fed with code programmed in flexible languages (e.g.,\n",
    "PyTorch code written in Python). However, flexible expressions (e.g.,\n",
    "indexing and view syntax in Python) pose high requirements on the\n",
    "frontend expression ability of operator compilers. From the model\n",
    "perspective, managing the inputs of an operatorn often contain many\n",
    "control flow statements. Also, some models allow for dynamic-shape\n",
    "operators whose shapes vary with control flow decisions across\n",
    "iterations.\n",
    "\n",
    "Additionally, there are a large number of operators that may not have\n",
    "optimized implementation provided by the accelerator libraries (e.g.,\n",
    "cuDNN) directly. This phenomenon is referred to as long tail operators.\n",
    "However, the long tail operators can have highly flexible syntax or\n",
    "abundant control flow statements and sometimes support dynamic shapes,\n",
    "making it extremely difficult for the frontend of existing operator\n",
    "compilers to express, optimize, or accelerate them. Consequently, such\n",
    "operators have to be executed by the Python interpreter or slow virtual\n",
    "machines, leading to a performance bottleneck in network execution. This\n",
    "is why it is imperative to improve the expression ability of the\n",
    "operator compiler frontend.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}