{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60798f1",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Operator Selection\n",
    "\n",
    "Following graph optimization, the compiler backend generates a sequence\n",
    "of operators that can be executed on hardware. This is achieved by\n",
    "selecting the most suitable operators from a set of candidate operators\n",
    "for each node in the IR. Since these candidate operators have diverse\n",
    "specifications, their execution efficiency varies depending on the\n",
    "scenario. Therefore, the primary objective of operator selection is to\n",
    "choose the operators that are most appropriate for the target device\n",
    "based on the information provided by the IR.\n",
    "\n",
    "## Basic Concepts of Operator Selection\n",
    "\n",
    "We can think of the nodes in a backend-optimized IR as being units of\n",
    "execution that are visible to the user, and each unit represents a\n",
    "hardware-agnostic operation in the user code. In essence, operator\n",
    "selection involves selecting appropriate hardware information, which is\n",
    "referred to as operator information. Such information defines the\n",
    "following:\n",
    "\n",
    "1.  The format of an operator, which is a determinant of the operator's\n",
    "    performance on the target platform. Machine learning systems\n",
    "    commonly use NCHW and NHWC formats.\n",
    "\n",
    "2.  The data type (such as float32, float16, or int32) of an operator on\n",
    "    the target platform. The operators selected are those with data\n",
    "    types close to (or the same as) user definitions.\n",
    "\n",
    "### Data Formats\n",
    "\n",
    "In machine learning systems, many operations are converted into matrix\n",
    "multiplication (e.g., convolution) for faster computation. Matrix\n",
    "multiplication in the form of\n",
    "$\\textit{\\textit{A}}\\times \\textit{\\textit{B}} = \\textit{\\textit{C}}$ is\n",
    "essentially a row-by-column multiplication. Specifically, the entry *ij*\n",
    "of **C** is obtained by multiplying the entries in the *i*th row of\n",
    "**A** and the corresponding entries in the *j*th column of **B** and\n",
    "then adding the results together. Consider the example shown in Figure\n",
    ":numref:`ch07/ch07-compiler-backend-06`. Matrix data is stored in\n",
    "row-major order by default, as shown at the top of the figure. However,\n",
    "matrix **B** is read in column-major order in the matrix multiplication\n",
    "process, as shown at the bottom.\n",
    "\n",
    "![Matrix data layouts in matrixmultiplication](../img/ch07/matmuldatalayout.png)\n",
    ":label:`ch07/ch07-compiler-backend-06`\n",
    "\n",
    "Storing matrix **B** in the reading order increases the computation\n",
    "efficiency because access to contiguous blocks of memory is faster. We\n",
    "can therefore see that data formats play an important role in\n",
    "performance improvement.\n",
    "\n",
    "There are two major formats in machine learning systems: NCHW and NHWC.\n",
    "For an image input, N denotes the batch size, C denotes the number of\n",
    "channels, and H and W denote the height and width respectively. Figure\n",
    ":numref:`ch07/ch07-compiler-backend-07` depicts the logical\n",
    "diagram of an input with batch size 2, channels 16, height 5, and width\n",
    "4.\n",
    "\n",
    "![Formatdiagram](../img/ch07/data_format.png)\n",
    ":label:`ch07/ch07-compiler-backend-07`\n",
    "\n",
    "A multidimensional matrix is flattened into 1D format before it is\n",
    "written to memory. This involves indexing, which maps logical data to\n",
    "physical memory.\n",
    "\n",
    "Access to machine learning data is performed in an axis-wise order from\n",
    "the last axis forward. For instance, data in NCHW format is read in the\n",
    "axis order of W, H, C, and N. Equation\n",
    ":eqref:`ch05/equation-01` denotes the mapping between\n",
    "logical memory and physical memory for this format of data.\n",
    "\n",
    "$$\n",
    "\\text{offsetnchw}(n,c,h,w) = n \\times \\textit{C} \\times \\textit{H} \\times \\textit{W} + c \\times \\textit{H} \\times \\textit{W} + h \\times \\textit{W} + w\n",
    "$$ \n",
    ":eqlabel:`equation:ch05/equation-01`\n",
    "\n",
    "As shown in Figure\n",
    ":numref:`ch07/ch07-compiler-backend-08`, matrix elements are\n",
    "flattened from the lowest dimension (i.e., W axis) forward, and\n",
    "neighboring elements of an axis reside next to each other in memory. To\n",
    "take the same element on the next image in the same location, the whole\n",
    "image size ($C*H*W$) has to be jumped. Assume we have a batch of eight\n",
    "RGB images of size 32$\\times$`<!-- -->`{=html}32, or a matrix with\n",
    "$N=8,C=3,H=32,W=32$. Memory storage of these images begins from the\n",
    "first channel of the first image by flattening the matrix along axis W\n",
    "and then arranging matrix elements along axis H. This is performed\n",
    "before the next channel is processed. The same procedure is repeated\n",
    "until the last channel of the last image is processed. NCHW is the\n",
    "default format on PyTorch and MindSpore.\n",
    "\n",
    "![RGB image data in NHWCformat](../img/ch07/nchw.png)\n",
    ":label:`ch07/ch07-compiler-backend-08`\n",
    "\n",
    "Access to data in NHWC format also begins at the lowest dimension (i.e.,\n",
    "C axis) forward. NHWC is the default format on TensorFlow (PyTorch\n",
    "refers to it as the channel-last format). Equation\n",
    ":eqref:`ch05/equation-02` denotes the mapping from logical\n",
    "memory to physical memory for this format of data.\n",
    "\n",
    "$$\n",
    "\\text{offsetnchw}(n,h,w,c) = n \\times \\textit{H} \\times \\textit{W} \\times \\textit{C} + h \\times  \\textit{W} \\times \\textit{C} + w \\times \\textit{C} + c\n",
    "$$ \n",
    ":eqlabel:`equation:ch05/equation-02`\n",
    "\n",
    "Figure\n",
    ":numref:`ch07/ch07-compiler-backend-nchwandnhwc` compares the\n",
    "logical indexing of the NCHW and NHWC formats. The \\[x:1\\] marks refer\n",
    "to the jumps from the innermost axis to the next. For example, \\[a:1\\]\n",
    "indicates the jump from axis W to axis H, and \\[b:1\\] indicates the jump\n",
    "from axis C (the innermost) to axis W.\n",
    "\n",
    "![NCHW and NHWCformats](../img/ch07/nchwandnhwc.png)\n",
    ":label:`ch07/ch07-compiler-backend-nchwandnhwc`\n",
    "\n",
    "These two formats offer a high degree of flexibility and are therefore\n",
    "used on many frameworks. However, to accelerate computing on hardware,\n",
    "further optimization is needed. In a machine learning system, if the\n",
    "size of the user input exceeds what the compute component can pass\n",
    "through the network at a time (which is often the case), the input will\n",
    "be batched before computation. For further optimization, many frameworks\n",
    "introduce blocked formats (which are more hardware-friendly), such as\n",
    "the nChw16c and nChw8c formats of the oneAPI Deep Neural Network Library\n",
    "(oneDNN) and the NC1HWC0 format on the Ascend platform. By leveraging\n",
    "hardware acceleration instructions to move and compute data, matrices\n",
    "can be quickly transformed into vectors, increasing the utilization of\n",
    "the on-chip cache.\n",
    "\n",
    "### Data Types\n",
    "\n",
    "Single-precision (float32), occupying 32 bits in memory, is the most\n",
    "commonly used data type in machine learning systems. In applications\n",
    "where higher precision is not essential, the half-precision (float16)\n",
    "data type may be used, occupying 16 bits in memory. When used on\n",
    "hardware, float16 offers up to 7 times more arithmetic throughput with\n",
    "less memory footprint compared with the single-precision data type ---\n",
    "this allows for larger batch sizes and consequently reduced training\n",
    "time. Next, we will look at the differences between half-precision\n",
    "floating-point numbers and single-precision floating-point numbers.\n",
    "\n",
    "In Figure :numref:`ch07/ch07-float32andfloat16`, *Sig* refers to the sign\n",
    "bit that indicates the sign of a number, *Exponent* refers to the\n",
    "exponent bits, and *Mantissa* refers to the mantissa bits.\n",
    "\n",
    "![Binary representation of floating-pointnumbers](../img/ch07/floatdtype.png)\n",
    ":label:`ch07/ch07-float32andfloat16`\n",
    "\n",
    "Applying Equation\n",
    ":eqref:`ch05/equation-03` will convert a float16 number in\n",
    "binary scientific notation to decimal format.\n",
    "\n",
    "$$\n",
    "(-1)^{\\text{Sig}}\\times 2^{\\text{Exponent}-15}\\times (\\frac{\\text{Mantissa}}{1024}+1)\n",
    "$$ \n",
    ":eqlabel:`equation:ch05/equation-03`\n",
    "\n",
    "If the exponent bits and mantissa bits are all 0s, the number is 0. If\n",
    "the exponent bits are all 0s but the mantissa bits are not, the number\n",
    "is very small. If the exponent bits are all 1s and the mantissa bits are\n",
    "all 0s, the number is an infinity, either positive or negative depending\n",
    "on the sign bit. Not a Number (NaN) is denoted by the exponent bits\n",
    "being all 1s while the mantissa bits are not all 0s. bfloat16 is a\n",
    "special data type developed by Google for machine learning on its tensor\n",
    "processing units (TPUs). Although bfloat16 is not an industry-standard\n",
    "IEEE 16-bit floating-point data type, it has the same exponent size as\n",
    "float32, meaning that it can be easily converted to and from float32.\n",
    "\n",
    "### Operator Information Library\n",
    "\n",
    "Hardware devices support different operators based on their data format\n",
    "and data type requirements. Each device maintains an operator\n",
    "information library that contains a comprehensive list of operators\n",
    "supported by that device. During the operator selection process, the\n",
    "most suitable operators are chosen from this library. The library serves\n",
    "as a reference for determining which operators are compatible and can be\n",
    "efficiently executed on a particular hardware device.\n",
    "\n",
    "## Process of Operator Selection\n",
    "\n",
    "Operator selection involves selecting the most appropriate operator for\n",
    "each operation node in an IR. Operator information contains the\n",
    "supported device type, data type, and data format. After the compiler\n",
    "frontend completes type inference and static analysis, the data type of\n",
    "user code is derived from the IR.\n",
    "\n",
    "Figure :numref:`ch07/ch07-compiler-backend-select` shows the operator\n",
    "selection process. First, the target hardware needs to be selected (or\n",
    "this step can be skipped in order to keep the default hardware selection\n",
    "defined in the compiler backend). The implementation, supported data\n",
    "types, and execution efficiency of a given operator vary depending on\n",
    "the target hardware. Then, the compiler backend selects an operator\n",
    "based on the data type and data format derived from the IR.\n",
    "\n",
    "![Operator selection process (using GPU as anexample)](../img/ch07/select_kernel.png)\n",
    ":label:`ch07/ch07-compiler-backend-select`\n",
    "\n",
    "The result of the operator selection process might not be as expected\n",
    "due to software or hardware specifications. Sometimes, we might need to\n",
    "adjust the precision of a particular node to find an operator with the\n",
    "right data type. For example, the Conv2D operator supported by Ascend\n",
    "(i.e., the backend of MindSpore) allows only the float16 data type. When\n",
    "used on a float32 network on Ascend, the Conv2D operator is executable\n",
    "only when its input precision is reduced from float32 to float16.\n",
    "\n",
    "Converting operators from one format to another can be time-consuming\n",
    "and incur memory movement overheads. To avoid this, data should be\n",
    "transferred between operators of the same format whenever possible. In\n",
    "addition, data type inconsistency may lead to reduced precision,\n",
    "potentially slowing down or even preventing network convergence. As\n",
    "such, thorough operator analysis is needed to ensure that the right data\n",
    "type is selected.\n",
    "\n",
    "Simply put, an operator selection algorithm is considered optimal if it\n",
    "keeps the data type as consistent as possible with user settings while\n",
    "also minimizing data format conversion.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}