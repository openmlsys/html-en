{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cca1a9b",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Chapter Summary\n",
    "\n",
    "1.  Model deployment is restricted by factors including the model size,\n",
    "    runtime memory usage, inference latency, and inference power\n",
    "    consumption.\n",
    "\n",
    "2.  Models can be compressed using techniques such as quantization,\n",
    "    pruning, and knowledge distillation in the offline phase. In\n",
    "    addition, some model optimization techniques, such as operator\n",
    "    fusion, can also reduce the model size, albeit to a lesser degree.\n",
    "\n",
    "3.  Runtime memory usage can be improved by optimizing the model size,\n",
    "    deployment framework size, and runtime temporary memory usage.\n",
    "    Methods for optimizing the model size have been summarized earlier.\n",
    "    Making the framework code simpler and more modular helps optimize\n",
    "    the deployment framework. Memory pooling can help implement memory\n",
    "    overcommitment to optimize the runtime temporary memory usage.\n",
    "\n",
    "4.  Model inference latency can be optimized from two aspects. In the\n",
    "    offline phase, the model computation workload can be reduced using\n",
    "    model optimization and compression methods. Furthermore, improving\n",
    "    the inference parallelism and optimizing operator implementation can\n",
    "    help maximize the utilization of the computing power. In addition to\n",
    "    the computation workload and computing power, consideration should\n",
    "    be given to the load/store overhead during inference.\n",
    "\n",
    "5.  Power consumption during inference can be reduced through offline\n",
    "    model optimization and compression technologies. By reducing the\n",
    "    computational workload, these technologies also facilitate power\n",
    "    consumption reduction, which coincides with the optimization method\n",
    "    for model inference latency.\n",
    "\n",
    "6.  In addition to the optimization of factors related to model\n",
    "    deployment, this chapter also discussed technologies regarding\n",
    "    deployment security, such as model obfuscation and model encryption.\n",
    "    Secure deployment protects the model assets of enterprises and\n",
    "    prevents hackers from attacking the deployment environment by\n",
    "    tampering with models.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}