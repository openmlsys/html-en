{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811a4f0f",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Model Inference\n",
    "\n",
    "After conversion and compression, a trained model needs to be deployed\n",
    "on the computation hardware in order to execute inference. Such\n",
    "execution involves the following steps:\n",
    "\n",
    "1.  Preprocessing: Process raw data to suit the network input.\n",
    "\n",
    "2.  Inference execution: Deploy the model resulting from offline\n",
    "    conversion on the device to execute inference and compute the output\n",
    "    based on the input.\n",
    "\n",
    "3.  Postprocessing: Further process the output of the model, for\n",
    "    example, by threshold filtering.\n",
    "\n",
    "## Preprocessing and Postprocessing\n",
    "\n",
    "**1. Preprocessing**\n",
    "\n",
    "Raw data, such as images, voices, and texts, is so disordered that\n",
    "machine learning models cannot identify or extract useful information\n",
    "from it. Preprocessing is intended to convert such into tensors that\n",
    "work for machine learning networks, eliminate irrelevant information,\n",
    "restore useful true information, enhance the detectability of relevant\n",
    "information, and simplify the data as much as possible. In this way,\n",
    "reliability indicators related to feature extraction, image\n",
    "segmentation, matching, and recognition of the models can be improved.\n",
    "\n",
    "The following techniques are often used in data preprocessing:\n",
    "\n",
    "1.  Feature encoding: Encode the raw data that describes features into\n",
    "    numbers and input them to machine learning models which can process\n",
    "    only numerical values. Common encoding approaches include\n",
    "    discretization, ordinal encoding, one-hot encoding, and binary\n",
    "    encoding.\n",
    "\n",
    "2.  Normalization: Modify features to be on the same scale without\n",
    "    changing the correlation between them, eliminating the impact of\n",
    "    dimensions between data indicators. Common approaches include\n",
    "    Min-Max normalization that normalizes the data range, and Z-score\n",
    "    normalization that normalizes data distribution.\n",
    "\n",
    "3.  Outliner processing: An outlier is a data point that is distant from\n",
    "    all others in distribution. Elimination of outliers can improve the\n",
    "    accuracy of a model.\n",
    "\n",
    "**2. Postprocessing**\n",
    "\n",
    "After model inference, the output data is transferred to users for\n",
    "postprocessing. Common postprocessing techniques include:\n",
    "\n",
    "1.  Discretization of contiguous data: Assume we expect to predict\n",
    "    discrete data, such as the quantity of a good, using a model, but a\n",
    "    regression model only provides contiguous prediction values, which\n",
    "    have to be rounded or bounded.\n",
    "\n",
    "2.  Data visualization: This technique uses graphics and tables to\n",
    "    represent data so that we can find relationships in the data in\n",
    "    order to support analysis strategy selection.\n",
    "\n",
    "3.  Prediction range widening: Most values predicted by a regression\n",
    "    model are concentrated in the center, and few are in the tails. For\n",
    "    example, abnormal values of hospital laboratory data are used to\n",
    "    diagnose diseases. To increase the accuracy of prediction, we can\n",
    "    enlarge the values in both tails by widening the prediction range\n",
    "    and multiplying the values that deviate from the normal range by a\n",
    "    coefficient to\n",
    "\n",
    "## Parallel Computing\n",
    ":label:`ch-deploy/parallel-inference`\n",
    "\n",
    "Most inference models have a multi-thread mechanism that leverages the\n",
    "capabilities of multiple cores in order to achieve performance\n",
    "improvements. In this mechanism, the input data of operators is\n",
    "partitioned, and multiple threads are used to process different data\n",
    "partitions. This allows operators to be computed in parallel, thereby\n",
    "multiplying the operator performance.\n",
    "\n",
    "![Data partitioning for matrixmultiplication](../img/ch08/ch09-parallel.png)\n",
    ":label:`ch09_parallel`\n",
    "\n",
    "In Figure :numref:`ch09_parallel`, the matrix in the multiplication can be\n",
    "partitioned according to the rows of matrix A. Three threads can then be\n",
    "used to compute A1 \\* B, A2 \\* B, and A3 \\* B (one thread per\n",
    "computation), implementing multi-thread parallel execution of the matrix\n",
    "multiplication.\n",
    "\n",
    "To facilitate parallel computing of operators and avoid the overhead of\n",
    "frequent thread creation and destruction, inference frameworks usually\n",
    "have a thread pooling mechanism. There are two common practices:\n",
    "\n",
    "1.  Open Multi-Processing (OpenMP) API: OpenMP is an API that supports\n",
    "    concurrency through memory sharing across multiple platforms. It\n",
    "    provides interfaces that are commonly used to implement operator\n",
    "    parallelism. An example of such an interface is `parallel for`,\n",
    "    which allows `for` loops to be concurrently executed by multiple\n",
    "    threads.\n",
    "\n",
    "2.  Framework-provided thread pools: Such pools are more lightweight and\n",
    "    targeted at the AI domain compared with OpenMP interfaces, and can\n",
    "    deliver better performance.\n",
    "\n",
    "## Operator Optimization\n",
    ":label:`ch-deploy/kernel-optimization`\n",
    "\n",
    "When deploying an AI model, we want model training and inference to be\n",
    "performed as fast as possible in order to obtain better performance. For\n",
    "a deep learning network, the scheduling of the framework takes a short\n",
    "period of time, whereas operator execution is often a bottleneck for\n",
    "performance. This section introduces how to optimize operators from the\n",
    "perspectives of hardware instructions and algorithms.\n",
    "\n",
    "**1. Hardware instruction optimization**\n",
    "\n",
    "Given that most devices have CPUs, the time that CPUs spend processing\n",
    "operators has a direct impact on the performance. Here we look at the\n",
    "methods for optimizing hardware instructions on ARM CPUs.\n",
    "\n",
    "**1) Assembly language**\n",
    "\n",
    "High-level programming languages such as C++ and Java are compiled as\n",
    "machine instruction code sequences by compilers, which often have a\n",
    "direct influence on which capabilities these languages offer. Assembly\n",
    "languages are close to machine code and can implement any instruction\n",
    "code sequence in one-to-one mode. Programs written in assembly languages\n",
    "occupy less memory, and are faster and more efficient than those written\n",
    "in high-level languages.\n",
    "\n",
    "In order to exploit the advantages of both types of languages, we can\n",
    "write the parts of a program that require better performance in assembly\n",
    "languages and the other parts in high-level languages. Because\n",
    "convolution and matrix multiplication operators in deep learning involve\n",
    "a large amount of computation, using assembly languages for code\n",
    "necessary to perform such computation can improve model training and\n",
    "inference performance by dozens or even hundreds of times.\n",
    "\n",
    "Next, we use ARMv8 CPUs to illustrate the optimization related to\n",
    "hardware instructions.\n",
    "\n",
    "**2) Registers and NEON instructions**\n",
    "\n",
    "Each ARMv8 CPU has 32 NEON registers, that is, v0 to v31. As shown in\n",
    "Figure :numref:`ch-deploy/register`, NEON register v0 can store 128\n",
    "bits, which is equal to the capacity of 4 float32, 8 float16, or 16\n",
    "int8.\n",
    "\n",
    "![Structure of the NEON register v0 of an ARMv8CPU](../img/ch08/ch09-register.png)\n",
    ":label:`ch-deploy/register`\n",
    "\n",
    "The single instruction multiple data (SIMD) method can be used to\n",
    "improve the data access and computing speed on this CPU. Compared with\n",
    "single data single instruction (SISD), the NEON instruction can process\n",
    "multiple data values in the NEON register at a time. For example, the\n",
    "`fmla` instruction for floating-point data is used as\n",
    "`fmla v0.4s, v1.4s, v2.4s`. As depicted in Figure\n",
    ":numref:`ch-deploy/fmla`, the products of the corresponding\n",
    "floating-point values in registers v1 and v2 are added to the value in\n",
    "v0.\n",
    "\n",
    "![fmla instructioncomputing](../img/ch08/ch09-fmla.png)\n",
    ":label:`ch-deploy/fmla`\n",
    "\n",
    "**3) Assembly language optimization**\n",
    "\n",
    "For assembly language programs with known functions, computational\n",
    "instructions are usually fixed. In this case, non-computational\n",
    "instructions are the source the performance bottleneck. The structure of\n",
    "computer storage devices resembles a pyramid, as shown in Figure\n",
    ":numref:`ch-deploy/fusion-storage`. The top layer has the fastest\n",
    "speed but the smallest space; conversely, the bottom layer has the\n",
    "largest space but the slowest speed. L1 to L3 are referred to as caches.\n",
    "When accessing data, the CPU first attempts to access the data from one\n",
    "of its caches. If the data is not found, the CPU then accesses an\n",
    "external main memory. Cache hit rate is introduced to measure the\n",
    "proportion of data that is accessed from the cache. In this sense, the\n",
    "cache hit rate must be maximized to improve the program performance.\n",
    "\n",
    "There are some techniques to improve the cache hit rate and optimize the\n",
    "assembly performance:\n",
    "\n",
    "1.  Loop unrolling: Use as many registers as possible to achieve better\n",
    "    performance at the cost of increasing the code size.\n",
    "\n",
    "2.  Instruction reordering: Reorder the instructions of different\n",
    "    execution units to improve the pipeline utilization, thereby\n",
    "    allowing instructions that incur latency to be executed first. In\n",
    "    addition to reducing the latency, this method also reduces data\n",
    "    dependency before and after the instruction.\n",
    "\n",
    "3.  Register blocking: Block NEON registers appropriately to reduce the\n",
    "    number of idle registers and reuse more registers.\n",
    "\n",
    "4.  Data rearrangement: Rearrange the computational data to ensure\n",
    "    contiguous memory reads and writes and improve the cache hit rate.\n",
    "\n",
    "5.  Instruction prefetching: Load the required data from the main memory\n",
    "    to the cache in advance to reduce the access latency.\n",
    "\n",
    "**2. Algorithm optimization**\n",
    "\n",
    "For most AI models, 90% or more of the inference time of the entire\n",
    "network is spent on computing convolution and matrix multiplication\n",
    "operators. This section focuses on the optimization of convolution\n",
    "operator algorithms, which can be applied to various hardware devices.\n",
    "The computation of convolution can be converted into the multiplication\n",
    "of two matrices, and we have elaborated on the optimization of the GEMM\n",
    "algorithm in Section :ref:`ch-deploy/parallel-inference`. For different hardware,\n",
    "appropriate matrix blocking can optimize data load/store efficiency and\n",
    "instruction parallelism. This helps to maximize the utilization of the\n",
    "hardware's computing power, thereby improving the inference performance.\n",
    "\n",
    "**(1) Img2col**\n",
    "\n",
    "Img2col is often used to convert convolution into matrix multiplication.\n",
    "Convolutional layers typically operate on 4D inputs in NHWC format.\n",
    "Figure :numref:`ch-deploy/conv_nhwc` is a diagram of convolution. The\n",
    "input shape is (1, IH, IW, IC), the convolution kernel shape is (OC, KH,\n",
    "KW, IC), and the output shape is (1, OH, OW, OC).\n",
    "\n",
    "![Generalconvolution](../img/ch08/ch09-conv_nhwc.png)\n",
    ":label:`ch-deploy/conv_nhwc`\n",
    "\n",
    "As shown in Figure\n",
    ":numref:`ch-deploy/img2col_input`, the Img2col rules for\n",
    "convolution are as follows: The input is reordered to obtain the matrix\n",
    "on the right. The number of rows corresponds to the number of OH \\* OW\n",
    "outputs. For a row vector, Img2col processes KH \\* KW data points of\n",
    "each input channel in sequence, from the first channel to channel IC.\n",
    "\n",
    "![Img2col on the convolutioninput](../img/ch08/ch09-img2col_input.png)\n",
    ":label:`ch-deploy/img2col_input`\n",
    "\n",
    "As shown in Figure\n",
    ":numref:`ch-deploy/img2col_weight`, the weights are rearranged.\n",
    "One convolution kernel is expanded into one column of the weight matrix.\n",
    "This means that there are OC columns in total. On each column vector, KH\n",
    "\\* KW data values on the first input channel are arranged first, and\n",
    "then on subsequent channels until the channel IC. In this manner, the\n",
    "convolution operation is converted into the multiplication of two\n",
    "matrices. In practice, the data rearrangement of Img2col and GEMM is\n",
    "performed simultaneously to save time.\n",
    "\n",
    "![Img2col on the convolutionkernel](../img/ch08/ch09-img2col_weight.png)\n",
    ":label:`ch-deploy/img2col_weight`\n",
    "\n",
    "**(2) Winograd**\n",
    "\n",
    "Convolution is essentially considered as matrix multiplication. The time\n",
    "complexity of multiplying two 2D matrices is $O(n^3)$. The Winograd\n",
    "algorithm can reduce the complexity of matrix multiplication.\n",
    "\n",
    "Assume that a 1D convolution operation is denoted as ***F***($m$, $r$),\n",
    "where $m$ indicates the number of outputs, and $r$ indicates the number\n",
    "of convolution kernels. The input is\n",
    "$\\textit{\\textbf{d}}=[d_0 \\ d_1 \\ d_2 \\ d_3]$, and the convolution\n",
    "kernel is $g=[g_0 \\ g_1 \\ g_2]^{\\rm T}$. The convolution operation may\n",
    "be written using matrices as Equation\n",
    ":eqref:`ch-deploy/conv-matmul-one-dimension`, which contains six\n",
    "multiplications and four additions.\n",
    "\n",
    "$$\n",
    "\\textit{\\textbf{F}}(2, 3)=\n",
    "\\left[ \\begin{matrix} d_0 & d_1 & d_2 \\\\ d_1 & d_2 & d_3 \\end{matrix} \\right] \\times \\left[ \\begin{matrix} g_0 \\\\ g_1 \\\\ g_2 \\end{matrix} \\right]=\n",
    "\\left[ \\begin{matrix} y_0 \\\\ y_1 \\end{matrix} \\right]\n",
    "$$ \n",
    ":eqlabel:`equ:ch-deploy/conv-matmul-one-dimension`\n",
    "\n",
    "In the preceding equation, there are repeated elements $d_1$ and $d_2$\n",
    "in the input matrix. As such, there is space for optimization for matrix\n",
    "multiplication converted from convolution compared with general matrix\n",
    "multiplication. The matrix multiplication result may be obtained by\n",
    "computing an intermediate variable $m_0-m_3$, as shown in Equation\n",
    ":eqref:`ch-deploy/conv-2-winograd`:\n",
    "\n",
    "$$\n",
    "\\textit{\\textbf{F}}(2, 3)=\n",
    "\\left[ \\begin{matrix} d_0 & d_1 & d_2 \\\\ d_1 & d_2 & d_3 \\end{matrix} \\right] \\times \\left[ \\begin{matrix} g_0 \\\\ g_1 \\\\ g_2 \\end{matrix} \\right]=\n",
    "\\left[ \\begin{matrix} m_0+m_1+m_2 \\\\ m_1-m_2+m_3 \\end{matrix} \\right]\n",
    "$$ \n",
    ":eqlabel:`equ:ch-deploy/conv-2-winograd`\n",
    "\n",
    "where $m_0-m_3$ are computed as Equation\n",
    ":eqref:`ch-deploy/winograd-param`:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_0=(d_0-d_2) \\times g_0 \\\\\n",
    "m_1=(d_1+d_2) \\times (\\frac{g_0+g_1+g_2}{2}) \\\\\n",
    "m_2=(d_0-d_2) \\times (\\frac{g_0-g_1+g_2}{2}) \\\\\n",
    "m_3=(d_1-d_3) \\times g_2\n",
    "\\end{aligned}\n",
    "$$ \n",
    ":eqlabel:`equ:ch-deploy/winograd-param`\n",
    "\n",
    "The indirect computation of r1 and r2 by computing $m_0-m_3$ involves\n",
    "four additions of the input $d$ and four multiplications and four\n",
    "additions of the output $m$. Because the weights are constant during\n",
    "inference, the operations on the convolution kernel can be performed\n",
    "during graph compilation, which is excluded from the online runtime. In\n",
    "total, there are four multiplications and eight additions --- fewer\n",
    "multiplications and more additions compared with direct computation\n",
    "(which has six multiplications and four additions). In computer systems,\n",
    "multiplications are generally more time-consuming than additions.\n",
    "Decreasing the number of multiplications while adding a small number of\n",
    "additions can accelerate computation.\n",
    "\n",
    "In a matrix form, the computation can be written as Equation\n",
    ":eqref:`ch-deploy/winograd-matrix`, where $\\odot$ indicates the\n",
    "multiplication of corresponding locations, and ***A***, ***B***, and\n",
    "***G*** are all constant matrices. The matrix here is used to facilitate\n",
    "clarity --- in real-world use, faster computation can be achieved if the\n",
    "matrix computation is performed based on the handwritten form, as\n",
    "provided in Equation\n",
    ":eqref:`ch-deploy/winograd-param`.\n",
    "\n",
    "$$\\textit{\\textbf{Y}}=\\textit{\\textbf{A}}^{\\rm T}(\\textit{\\textbf{G}}g) \\odot (\\textit{\\textbf{B}}^{\\rm T}d)$$ \n",
    ":eqlabel:`equ:ch-deploy/winograd-matrix`\n",
    "\n",
    "$$\\textit{\\textbf{B}}^{\\rm T}=\n",
    "\\left[ \\begin{matrix} 1 & 0 & -1 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 0 & -1 & 1 & 0 \\\\ 0 & 1 & 0 & -1 \\end{matrix} \\right]$$ \n",
    ":eqlabel:`equ:ch-deploy/winograd-matrix-bt`\n",
    "\n",
    "$$\\textit{\\textbf{G}}=\n",
    "\\left[ \\begin{matrix} 1 & 0 & 0 \\\\ 0.5 & 0.5 & 0.5 \\\\ 0.5 & -0.5 & 0.5 \\\\ 0 & 0 & 1 \\end{matrix} \\right]$$ \n",
    ":eqlabel:`equ:ch-deploy/winograd-matrix-g`\n",
    "\n",
    "$$\\textit{\\textbf{A}}^{\\rm T}=\n",
    "\\left[ \\begin{matrix} 1 & 1 & -1 & 0 \\\\ 0 & 1 & -1 & -1  \\end{matrix} \\right] \\\\$$ \n",
    ":eqlabel:`equ:ch-deploy/winograd-matrix-at`\n",
    "\n",
    "In deep learning, 2D convolution is typically used. When ***F***(2, 3)\n",
    "is extended to ***F***(2$\\times$`<!-- -->`{=html}2,\n",
    "3$\\times$`<!-- -->`{=html}3), it can be written in a matrix form, as\n",
    "shown in Equation\n",
    ":eqref:`ch-deploy/winograd-two-dimension-matrix`. In this case,\n",
    "Winograd has 16 multiplications, reducing the computation complexity by\n",
    "2.25 times compared with 36 multiplications of the original convolution.\n",
    "\n",
    "$$\\textit{\\textbf{Y}}=\\textit{\\textbf{A}}^{\\rm T}(\\textit{\\textbf{G}}g\\textit{\\textbf{G}}^{\\rm T}) \\odot (\\textit{\\textbf{B}}^{\\rm T}d\\textit{\\textbf{B}})\\textit{\\textbf{A}}$$ \n",
    ":eqlabel:`equ:ch-deploy/winograd-two-dimension-matrix`\n",
    "\n",
    "The logical process of Winograd can be divided into four steps, as shown\n",
    "in Figure :numref:`ch-deploy/winograd`.\n",
    "\n",
    "![Winogradsteps](../img/ch08/ch09-winograd.png)\n",
    ":label:`ch-deploy/winograd`\n",
    "\n",
    "To use Winograd of ***F***(2$\\times$`<!-- -->`{=html}2,\n",
    "3$\\times$`<!-- -->`{=html}3) for any output size, we need to divide the\n",
    "output into 2$\\times$`<!-- -->`{=html}2 blocks. We can then perform the\n",
    "preceding four steps using the corresponding input to obtain the\n",
    "corresponding output. Winograd is not limited to solving\n",
    "***F***(2$\\times$`<!-- -->`{=html}2, 3$\\times$`<!-- -->`{=html}3). For\n",
    "any ***F***($m \\times m$, $r \\times r$), appropriate constant matrices\n",
    "***A***, ***B***, and ***G*** can be found to reduce the number of\n",
    "multiplications through indirect computation. However, as $m$ and $r$\n",
    "increase, the number of additions involved in input and output and the\n",
    "number of multiplications of constant weights increase. In this case,\n",
    "the decrease in the computation workload brought by fewer\n",
    "multiplications is offset by additions and constant multiplications.\n",
    "Therefore, we need to evaluate the benefits of Winograd before using it.\n",
    "\n",
    "This section describes methods for processing data and optimizing\n",
    "performance during model inference. An appropriate data processing\n",
    "method can facilitate the input feature extraction and output\n",
    "processing. And to fully leverage the computing power of hardware, we\n",
    "can use parallel computing and operator-level hardware instruction and\n",
    "algorithm optimization. In addition, the memory usage and load/store\n",
    "rate are also important for the inference performance. Therefore, it is\n",
    "essential to design an appropriate memory overcommitment strategy for\n",
    "inference. Related methods have been discussed in the section about the\n",
    "compiler backend.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}