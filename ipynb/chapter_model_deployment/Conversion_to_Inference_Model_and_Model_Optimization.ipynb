{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24265c9",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Conversion to Inference Model and Model Optimization\n",
    ":label:`ch-deploy/model-optimization`\n",
    "\n",
    "## Model Conversion\n",
    "\n",
    "As mentioned earlier, TensorFlow, PyTorch, MindSpore, MXNet, and CNTK\n",
    "define their own model data structures. This means that the inference\n",
    "system needs to convert these structures to a unified one. Open Neural\n",
    "Network Exchange (ONNX) is designed to implement such conversion. It\n",
    "supports an extensive range of machine learning operators and converts\n",
    "models from various frameworks (e.g., TensorFlow and PyTorch) into ONNX\n",
    "models. Because models are structured data, the conversion process\n",
    "involves converting the data structure. It starts by analyzing the\n",
    "similarities and differences between two data structures. If they are\n",
    "the same, data is transferred; if the structures are similar but with\n",
    "slight differences, data is mapped; if the structures differ\n",
    "significantly, extra semantics conversion might be required; and if they\n",
    "are totally incompatible, the conversion will fail. ONNX features strong\n",
    "expressive power, meaning that it can convert models from most\n",
    "frameworks in the industry to compatible ONNX models. If a model is\n",
    "abstracted as a graph, its data structure can be defined as follows:\n",
    "\n",
    "1.  **Topological expression of model:** The topological connections of\n",
    "    a model are represented as edges in a graph. From the perspective of\n",
    "    a model, these edges define the data flows and control flows in the\n",
    "    model. Based on such definitions, we can extend to the expressions\n",
    "    of the subgraphs, model inputs and outputs, and control flow\n",
    "    structures. For example, the control flow on TensorFlow 1.x is\n",
    "    expressed as a cyclic graph. To prevent the formation of cycles,\n",
    "    TensorFlow 1.x uses operators such as Enter, Exit, Switch, LoopCond,\n",
    "    and NextIteration, whereas ONNX uses operators such as Loop and If.\n",
    "    As such, when converting a TensorFlow1.x control flow model into an\n",
    "    ONNX model, the control flow graph structure in the TensorFlow model\n",
    "    must be merged into a While or If operator on ONNX.\n",
    "\n",
    "2.  **Operator prototype definition:** Operators can be regarded as data\n",
    "    processing or control flow nodes in a model or as vertices in a\n",
    "    graph. An operator prototype defines the type, inputs, outputs, and\n",
    "    attributes of an operator. For instance, Slice has different\n",
    "    semantics on Caffe and ONNX. To convert a Caffe model into an ONNX\n",
    "    model, we need to map Slice on Caffe to Split on ONNX.\n",
    "    FusedBatchnorm on TensorFlow does not have a mapping operator on\n",
    "    Caffe. Rather, Batchnorm and Scale on Caffe need to be combined to\n",
    "    express the same semantics of FusedBatchnorm on TensorFlow.\n",
    "    Generally, the model conversion process involves converting the\n",
    "    topological relationships and mapping the operator prototypes\n",
    "    between models.\n",
    "\n",
    "Following model conversion, some input-agnostic operations are conducted\n",
    "for optimization purposes prior to model deployment, including constant\n",
    "folding, operator fusion, operator replacement, and operator reordering\n",
    "--- optimization methods discussed earlier in this book. For instance,\n",
    "constant folding is usually performed during the compilation executed on\n",
    "the compiler frontend, whereas, operator fusion and partition are often\n",
    "performed (depending on the backend hardware support) once the\n",
    "compilation is complete. However, some optimization operations can only\n",
    "be performed in their entirety during the deployment phase.\n",
    "\n",
    "![Layered computer storagearchitecture](../img/ch08/ch09-storage.png)\n",
    ":label:`ch-deploy/fusion-storage`\n",
    "\n",
    "## Operator Fusion\n",
    ":label:`ch-deploy/kernel-fusion`\n",
    "\n",
    "Operator fusion involves combining multiple operators in a deep neural\n",
    "network (DNN) model into a new operator based on certain rules, reducing\n",
    "the inference latency and power consumption by lowering the computation\n",
    "workload and load/store overhead during online inference.\n",
    "\n",
    "The two main performance benefits brought by operator fusion are as\n",
    "follows: First, it maximizes the utilization of registers and caches.\n",
    "And second, because it combines operators, the load/store time between\n",
    "the CPU and memory is reduced. Figure\n",
    ":numref:`ch-deploy/fusion-storage` shows the architecture of a\n",
    "computer's storage system. While the storage capacity increases from the\n",
    "level-1 cache (L1) to hard disk, so too does the time for reading data.\n",
    "After operator fusion is performed, the previous computation result can\n",
    "be temporarily stored in the CPU's register or cache where the next\n",
    "computation can directly read the result, reducing the number of I/O\n",
    "operations on the memory. Furthermore, operator fusion allows some\n",
    "computation to be completed in advance, eliminating redundant or even\n",
    "cyclic redundant computing during forward computation.\n",
    "\n",
    "![Convolution + Batchnorm operatorfusion](../img/ch08/ch09-conv-bn-fusion.png)\n",
    ":label:`ch-deploy/conv-bn-fusion`\n",
    "\n",
    "To describe the principle of operator fusion, we will use two operators,\n",
    "Convolution and Batchnorm, as shown in Figure\n",
    ":numref:`ch-deploy/conv-bn-fusion`. In the figure, the\n",
    "solid-colored boxes indicate operators, the resulting operators after\n",
    "fusion is performed are represented by hatched boxes, and the weights or\n",
    "constant tensors of operators are outlined in white. The fusion can be\n",
    "understood as the simplification of an equation. The computation of\n",
    "Convolution is expressed as Equation\n",
    ":eqref:`ch-deploy/conv-equation`.\n",
    "\n",
    "$$\\bf{Y_{\\rm conv}}=\\bf{W_{\\rm conv}}\\cdot\\bf{X_{\\rm conv}}+\\bf{B_{\\rm conv}}$$ \n",
    ":eqlabel:`equ:ch-deploy/conv-equation`\n",
    "\n",
    "Here, we do not need to understand what each variable means. Instead, we\n",
    "only need to keep in mind that Equation\n",
    ":eqref:`ch-deploy/conv-equation` is an equation for\n",
    "$\\bf{Y_{\\rm conv}}$ with respect to $\\bf{X_{\\rm conv}}$, and other\n",
    "symbols are constants.\n",
    "\n",
    "Equation\n",
    ":eqref:`ch-deploy/bn-equation` is about the computation of\n",
    "Batchnorm:\n",
    "\n",
    "$$\\bf{Y_{\\rm bn}}=\\gamma\\frac{\\bf{X_{\\rm bn}}-\\mu_{\\mathcal{B}}}{\\sqrt{{\\sigma_{\\mathcal{B}}}^{2}+\\epsilon}}+\\beta$$ \n",
    ":eqlabel:`equ:ch-deploy/bn-equation`\n",
    "\n",
    "Similarly, it is an equation for $\\bf{Y_{\\rm bn}}$ with respect to\n",
    "$\\bf{X_{\\rm bn}}$. Other symbols in the equation represent constants.\n",
    "\n",
    "As shown in Figure\n",
    ":numref:`ch-deploy/conv-bn-fusion`, when the output of\n",
    "Convolution is used as the input of Batchnorm, the formula of Batchnorm\n",
    "is a function for $\\bf{Y_{\\rm bn}}$ with respect to $\\bf{X_{\\rm conv}}$.\n",
    "After substituting $\\bf{Y_{\\rm conv}}$ into $\\bf{X_{\\rm bn}}$ and\n",
    "uniting and extracting the constants, we obtain Equation\n",
    ":eqref:`ch-deploy/conv-bn-equation-3`.\n",
    "\n",
    "$$\\bf{Y_{\\rm bn}}=\\bf{A}\\cdot\\bf{X_{\\rm conv}}+\\bf{B}$$ \n",
    ":eqlabel:`equ:ch-deploy/conv-bn-equation-3`\n",
    "\n",
    "Here, $\\bf{A}$ and $\\bf{B}$ are two matrices. It can be noticed that\n",
    "Equation\n",
    ":eqref:`ch-deploy/conv-bn-equation-3` is a formula for computing\n",
    "Convolution. The preceding example shows that the computation of\n",
    "Convolution and Batchnorm can be fused into an equivalent Convolution\n",
    "operator. Such fusion is referred to as formula fusion.\n",
    "\n",
    "The fusion of Convolution and Batchnorm eliminates a Batchnorm\n",
    "operation, thereby reducing the quantity of parameters and computation\n",
    "workload are reduced, and thereby the load/store operations are also\n",
    "reduced. In general, this fusion not only optimizes the power\n",
    "consumption and performance during model deployment, but also brings\n",
    "certain benefits in compressing the model size.\n",
    "\n",
    "Symbols that are considered as constants in the Convolution and\n",
    "Batchnorm formulas during fusion are considered as parameters during\n",
    "training. Performing fusion during the training process will result in\n",
    "missing model parameters. Because the fusion eliminates a Batchnorm\n",
    "operator and corresponding parameters from the network, the algorithm of\n",
    "the DNN is changed, degrading the accuracy to unacceptable levels.\n",
    "Therefore, the fusion of Convolution and Batchnorm is an optimization\n",
    "method typically used during deployment. To evaluate the optimization\n",
    "effect, we constructed a sample network with Convolution and Batchnorm\n",
    "using MindSpore Lite. We ran the sample network and mobilenet-v2 network\n",
    "for inference in dual threads on a Huawei Mate 30 smartphone to compare\n",
    "the time of running 3,000 inference epochs before and after the fusion.\n",
    "As shown in Table\n",
    "`ch09-conv-bn-fusion`, the inference performance of\n",
    "the sample network and mobilenet-v2 network is improved considerably\n",
    "after the fusion --- by 8.5% and 11.7% respectively. Such improvements\n",
    "are achieved without bringing side effects and without requiring\n",
    "additional hardware or operator libraries.\n",
    "\n",
    ":Convolution + Batchnorm inference performance before and after fusion (unit: ms)\n",
    "\n",
    "|Fusion         |  Sample |  Mobilenet-v2 |\n",
    "|---------------| --------| -------------- |\n",
    "|Before fusion  |  0.035  |     15.415 |\n",
    "|After fusion   |  0.031  |     13.606 |\n",
    ":label:`ch09/ch09-conv-bn-fusion`\n",
    "\n",
    "## Operator Replacement\n",
    "\n",
    "The principle of operator replacement is to simplify an operator formula\n",
    "by uniting like terms, extracting common factors, and employing other\n",
    "mathematical methods, and then map the simplified formula to a certain\n",
    "type of operators that have the same computational logic but are more\n",
    "suitable for online deployment. In this way, we can reduce the\n",
    "computation workload and compress the model.\n",
    "\n",
    "![Replacement ofBatchnorm](../img/ch08/ch09-bn-replace.png)\n",
    ":label:`ch-deploy/bn-replace`\n",
    "\n",
    "Figure :numref:`ch-deploy/bn-replace` depicts the replacement of\n",
    "Batchnorm with Scale, which is used as an example to describe the\n",
    "principle of operator replacement. After decomposing Equation\n",
    ":eqref:`ch-deploy/bn-equation` (the Batchnorm formula) and\n",
    "folding the constants, Batchnorm is defined as Equation\n",
    ":eqref:`ch-deploy/replace-scale`\n",
    "\n",
    "$$\\bf{Y_{bn}}=scale\\cdot\\bf{X_{bn}}+offset$$ \n",
    ":eqlabel:`equ:ch-deploy/replace-scale`\n",
    "\n",
    "where **scale** and **offsets** are scalars. This simplified formula can\n",
    "be mapped to a Scale operator.\n",
    "\n",
    "Compared with the original Batchnorm formula, the simplified formula has\n",
    "fewer parameters and involves less computation workload. This indicates\n",
    "that operator replacement is an effective approach to optimizing the\n",
    "power consumption and performance of a model during deployment. Symbols\n",
    "that are considered as constants in Batchnorm during deployment are not\n",
    "considered as constants during training, meaning that the replacement\n",
    "can be performed only during deployment. Operator replacement reduces\n",
    "the quantity of parameters and changes the structure of the model,\n",
    "weakening the expressive power and reducing the accuracy of the model\n",
    "during convergence.\n",
    "\n",
    "## Operator Reordering\n",
    "\n",
    "Another way of reducing the computation workload of an inference model\n",
    "is to adjust the topological order of its operators according to certain\n",
    "rules, on the condition that the inference accuracy is not degraded.\n",
    "Common methods of operator reordering include moving cropping operators\n",
    "(e.g., Slice, StrideSlice, and Crop) forward, and reordering Reshape,\n",
    "Transpose, and BinaryOp.\n",
    "\n",
    "![Reordering ofCrop](../img/ch08/ch09-crop-reorder.png)\n",
    ":label:`ch-deploy/crop-reorder`\n",
    "\n",
    "Crop is used to cut a part out of the input feature map as the output.\n",
    "After Crop is executed, the size of the feature map is reduced. As shown\n",
    "in Figure :numref:`ch-deploy/crop-reorder`, moving Crop forward to cut the\n",
    "feature map before other operators reduces the computation workload of\n",
    "subsequent operators, thereby improving the inference performance in the\n",
    "deployment phase. Such improvement is related to the operator\n",
    "parameters. Note, however, that Crop can be moved forward only along\n",
    "element-wise operators.\n",
    "\n",
    "The experiment result above proves that optimizing models before\n",
    "inference makes it possible to significantly reduce the latency, power\n",
    "consumption, and memory usage.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}